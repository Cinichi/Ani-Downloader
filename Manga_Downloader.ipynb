{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhsrJGZpaZG/GkVOnIoMHP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cinichi/Ani-Downloader/blob/main/Manga_Downloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# üé¨ AnimeKai Episode Downloader & Merger\n",
        "# Enhanced with chunk downloads, yt-dlp support, and better error handling\n",
        "\n",
        "# @title üîß **Install Dependencies** { display-mode: \"form\" }\n",
        "print(\"üì¶ Installing required packages...\")\n",
        "!pip install -q requests beautifulsoup4 cloudscraper m3u8 pycryptodome tqdm yt-dlp\n",
        "!apt-get -qq install -y ffmpeg aria2 > /dev/null 2>&1\n",
        "print(\"‚úÖ All dependencies installed!\\n\")\n",
        "\n",
        "# @title ‚öôÔ∏è **Configuration** { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### üîó Anime URL\n",
        "#@markdown Enter the AnimeKai watch URL:\n",
        "anime_url = \"https://anikai.to/watch/attack-on-titan-season-3-oxgk\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üì∫ Episode Selection\n",
        "download_mode = \"All Episodes\" #@param [\"All Episodes\", \"Episode Range\", \"Single Episode\"]\n",
        "\n",
        "#@markdown Single episode number:\n",
        "single_episode = 1 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Episode range (Start and End):\n",
        "start_episode = 1 #@param {type:\"integer\"}\n",
        "end_episode = 3 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üé• Quality & Audio Settings\n",
        "video_quality = \"1080p\" #@param [\"1080p\", \"720p\", \"480p\", \"360p\"]\n",
        "prefer_type = \"Dub & S-Sub\" #@param [\"Hard Sub\", \"Soft Sub\", \"Dub & S-Sub\"]\n",
        "prefer_server = \"Server 1\" #@param [\"Server 1\", \"Server 2\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üì• Download Settings\n",
        "download_method = \"yt-dlp\" #@param [\"yt-dlp\", \"aria2\", \"chunks\", \"ffmpeg\"]\n",
        "\n",
        "#@markdown Chunk size in MB (for chunked downloads):\n",
        "chunk_size_mb = 14 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "\n",
        "#@markdown Max parallel workers/connections:\n",
        "max_workers = 15 #@param {type:\"slider\", min:1, max:32, step:1}\n",
        "\n",
        "#@markdown Max retry attempts:\n",
        "max_retries = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "\n",
        "#@markdown Timeout in seconds:\n",
        "timeout = 300 #@param {type:\"slider\", min:60, max:600, step:30}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üîó Merge Settings\n",
        "merge_episodes = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üì§ Upload Settings\n",
        "upload_to = \"GoFile.io Only\" #@param [\"Google Drive Only\", \"GoFile.io Only\", \"Both\", \"None (Keep Local)\"]\n",
        "\n",
        "print(\"‚úÖ Configuration loaded!\")\n",
        "\n",
        "# @title üåê **Core Functions** { display-mode: \"form\" }\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import threading\n",
        "from bs4 import BeautifulSoup\n",
        "import cloudscraper\n",
        "from urllib.parse import urljoin, urlparse, quote, unquote\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Create cloudscraper session\n",
        "scraper = cloudscraper.create_scraper(\n",
        "    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}\n",
        ")\n",
        "\n",
        "BASE_URL = \"https://animekai.to\"\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "    'Referer': BASE_URL,\n",
        "    'Accept': '*/*',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "    'Connection': 'keep-alive'\n",
        "}\n",
        "\n",
        "def enc_dec_request(endpoint, text):\n",
        "    \"\"\"Make request to enc-dec API\"\"\"\n",
        "    try:\n",
        "        url = f\"https://enc-dec.app/api/{endpoint}?text={text}\"\n",
        "        response = scraper.get(url, headers=headers, timeout=30)\n",
        "        data = response.json()\n",
        "        return data.get('result', '')\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Enc-dec error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_anime_details(url):\n",
        "    \"\"\"Get anime ID and title\"\"\"\n",
        "    try:\n",
        "        response = scraper.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        anime_div = soup.select_one('div[data-id]')\n",
        "        anime_id = anime_div.get('data-id') if anime_div else None\n",
        "\n",
        "        title_elem = soup.select_one('div.title-wrapper h1.title span')\n",
        "        title = title_elem.get('title', '') if title_elem else \"Unknown\"\n",
        "        title = re.sub(r'[<>:\"/\\\\|?*]', '', title)\n",
        "\n",
        "        return anime_id, title\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error getting anime details: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def get_episode_list(anime_id):\n",
        "    \"\"\"Get list of all episodes\"\"\"\n",
        "    try:\n",
        "        enc = enc_dec_request('enc-kai', anime_id)\n",
        "        if not enc:\n",
        "            return []\n",
        "\n",
        "        ep_url = f\"{BASE_URL}/ajax/episodes/list?ani_id={anime_id}&_={enc}\"\n",
        "        response = scraper.get(ep_url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        if 'result' not in data:\n",
        "            return []\n",
        "\n",
        "        html = data['result']\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        episodes = []\n",
        "        for ep in soup.select('div.eplist a'):\n",
        "            token = ep.get('token', '')\n",
        "            ep_num = ep.get('num', '0')\n",
        "            langs = ep.get('langs', '0')\n",
        "\n",
        "            langs_int = int(langs) if langs.isdigit() else 0\n",
        "            if langs_int == 1:\n",
        "                subdub = \"Sub\"\n",
        "            elif langs_int == 3:\n",
        "                subdub = \"Dub & Sub\"\n",
        "            else:\n",
        "                subdub = \"\"\n",
        "\n",
        "            episodes.append({\n",
        "                'number': float(ep_num),\n",
        "                'token': token,\n",
        "                'subdub': subdub,\n",
        "                'title': f\"Episode {ep_num}\"\n",
        "            })\n",
        "\n",
        "        return sorted(episodes, key=lambda x: x['number'])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error getting episodes: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_video_servers(token):\n",
        "    \"\"\"Get available video servers\"\"\"\n",
        "    try:\n",
        "        enc = enc_dec_request('enc-kai', token)\n",
        "        if not enc:\n",
        "            return []\n",
        "\n",
        "        url = f\"{BASE_URL}/ajax/links/list?token={token}&_={enc}\"\n",
        "        response = scraper.get(url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        if 'result' not in data:\n",
        "            return []\n",
        "\n",
        "        html = data['result']\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        servers = []\n",
        "        for type_div in soup.select('div.server-items[data-id]'):\n",
        "            type_id = type_div.get('data-id', '')\n",
        "\n",
        "            for server in type_div.select('span.server[data-lid]'):\n",
        "                server_id = server.get('data-lid', '')\n",
        "                server_name = server.text.strip()\n",
        "\n",
        "                servers.append({\n",
        "                    'type': type_id,\n",
        "                    'server_id': server_id,\n",
        "                    'server_name': server_name\n",
        "                })\n",
        "\n",
        "        return servers\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error getting servers: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_video_url(server_id, server_name):\n",
        "    \"\"\"Get direct video URL\"\"\"\n",
        "    try:\n",
        "        enc = enc_dec_request('enc-kai', server_id)\n",
        "        if not enc:\n",
        "            return None\n",
        "\n",
        "        url = f\"{BASE_URL}/ajax/links/view?id={server_id}&_={enc}\"\n",
        "        response = scraper.get(url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        encoded_link = data.get('result', '')\n",
        "        if not encoded_link:\n",
        "            return None\n",
        "\n",
        "        dec_body = json.dumps({\"text\": encoded_link})\n",
        "        dec_response = scraper.post(\n",
        "            \"https://enc-dec.app/api/dec-kai\",\n",
        "            data=dec_body,\n",
        "            headers={'Content-Type': 'application/json'}\n",
        "        )\n",
        "        dec_data = dec_response.json()\n",
        "        iframe_url = dec_data.get('result', {}).get('url', '')\n",
        "\n",
        "        if not iframe_url:\n",
        "            return None\n",
        "\n",
        "        return extract_megaup_url(iframe_url)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error getting video URL: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_megaup_url(iframe_url):\n",
        "    \"\"\"Extract video URL from MegaUp\"\"\"\n",
        "    try:\n",
        "        parsed = urlparse(iframe_url)\n",
        "        token = parsed.path.split('/')[-1]\n",
        "\n",
        "        media_url = f\"{parsed.scheme}://{parsed.netloc}/media/{token}\"\n",
        "        response = scraper.get(media_url, headers=headers)\n",
        "        data = response.json()\n",
        "        mega_token = data.get('result', '')\n",
        "\n",
        "        if not mega_token:\n",
        "            return None\n",
        "\n",
        "        dec_body = json.dumps({\"text\": mega_token, \"agent\": headers['User-Agent']})\n",
        "        dec_response = scraper.post(\n",
        "            \"https://enc-dec.app/api/dec-mega\",\n",
        "            data=dec_body,\n",
        "            headers={'Content-Type': 'application/json'}\n",
        "        )\n",
        "\n",
        "        mega_data = dec_response.json()\n",
        "        sources = mega_data.get('result', {}).get('sources', [])\n",
        "\n",
        "        if not sources:\n",
        "            return None\n",
        "\n",
        "        return sources[0].get('file', '')\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è MegaUp extraction error: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Core functions loaded\")\n",
        "\n",
        "# @title üì• **Download Methods** { display-mode: \"form\" }\n",
        "\n",
        "def download_with_ytdlp(url, output_file, episode_num):\n",
        "    \"\"\"Download using yt-dlp (BEST for m3u8)\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüì• Downloading Episode {episode_num} with yt-dlp...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        cmd = [\n",
        "            'yt-dlp', url, '-o', output_file,\n",
        "            '--no-warnings', '--no-check-certificate',\n",
        "            '--concurrent-fragments', str(max_workers),\n",
        "            '--retries', str(max_retries),\n",
        "            '--fragment-retries', str(max_retries),\n",
        "            '--socket-timeout', str(timeout),\n",
        "            '--progress', '--newline',\n",
        "            '--user-agent', headers['User-Agent'],\n",
        "            '--referer', BASE_URL\n",
        "        ]\n",
        "\n",
        "        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n",
        "\n",
        "        for line in process.stdout:\n",
        "            if '[download]' in line and '%' in line:\n",
        "                match = re.search(r'(\\d+\\.\\d+)%', line)\n",
        "                if match:\n",
        "                    percent = float(match.group(1))\n",
        "                    print(f\"\\r   ‚è≥ Progress: {percent:.1f}%\", end='', flush=True)\n",
        "\n",
        "        process.wait()\n",
        "\n",
        "        if process.returncode == 0 and os.path.exists(output_file):\n",
        "            file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "            print(f\"\\n   ‚úÖ Complete! ({file_size:.2f} MB)\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"\\n   ‚ùå Download failed\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   ‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_with_aria2(url, output_file, episode_num):\n",
        "    \"\"\"Download using aria2c (FAST multi-connection)\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüì• Downloading Episode {episode_num} with aria2...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        cmd = [\n",
        "            'aria2c', url,\n",
        "            '-o', os.path.basename(output_file),\n",
        "            '-d', os.path.dirname(output_file),\n",
        "            '-x', str(max_workers),\n",
        "            '-s', str(max_workers),\n",
        "            '--max-tries=5', '--retry-wait=3',\n",
        "            '--user-agent=' + headers['User-Agent'],\n",
        "            '--referer=' + BASE_URL\n",
        "        ]\n",
        "\n",
        "        subprocess.run(cmd, check=False)\n",
        "\n",
        "        if os.path.exists(output_file):\n",
        "            file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "            print(f\"\\n   ‚úÖ Complete! ({file_size:.2f} MB)\")\n",
        "            return True\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   ‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_chunk(url, start, end, chunk_file, pbar):\n",
        "    \"\"\"Download a single chunk\"\"\"\n",
        "    try:\n",
        "        chunk_headers = headers.copy()\n",
        "        chunk_headers['Range'] = f'bytes={start}-{end}'\n",
        "        response = scraper.get(url, headers=chunk_headers, stream=True, timeout=30)\n",
        "\n",
        "        if response.status_code not in [200, 206]:\n",
        "            return False\n",
        "\n",
        "        with open(chunk_file, 'wb') as f:\n",
        "            for data in response.iter_content(chunk_size=8192):\n",
        "                if data:\n",
        "                    f.write(data)\n",
        "                    pbar.update(len(data))\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def download_with_chunks(url, output_file, episode_num):\n",
        "    \"\"\"Download with chunked/parallel downloading\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüì• Downloading Episode {episode_num} with chunks...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        head_response = scraper.head(url, headers=headers, timeout=10)\n",
        "\n",
        "        if 'Content-Length' not in head_response.headers:\n",
        "            return download_direct(url, output_file, episode_num)\n",
        "\n",
        "        total_size = int(head_response.headers['Content-Length'])\n",
        "        chunk_size = chunk_size_mb * 1024 * 1024\n",
        "\n",
        "        chunks = []\n",
        "        for i in range(0, total_size, chunk_size):\n",
        "            start = i\n",
        "            end = min(i + chunk_size - 1, total_size - 1)\n",
        "            chunks.append((start, end))\n",
        "\n",
        "        chunk_dir = f\"{output_file}.chunks\"\n",
        "        os.makedirs(chunk_dir, exist_ok=True)\n",
        "        chunk_files = []\n",
        "\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"   ‚è≥ Downloading\", ncols=80) as pbar:\n",
        "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "                futures = {}\n",
        "                for idx, (start, end) in enumerate(chunks):\n",
        "                    chunk_file = f\"{chunk_dir}/chunk_{idx:04d}\"\n",
        "                    chunk_files.append(chunk_file)\n",
        "                    future = executor.submit(download_chunk, url, start, end, chunk_file, pbar)\n",
        "                    futures[future] = idx\n",
        "\n",
        "                for future in as_completed(futures):\n",
        "                    if not future.result():\n",
        "                        shutil.rmtree(chunk_dir, ignore_errors=True)\n",
        "                        return False\n",
        "\n",
        "        print(\"   üîó Merging chunks...\")\n",
        "        with open(output_file, 'wb') as outfile:\n",
        "            for chunk_file in chunk_files:\n",
        "                with open(chunk_file, 'rb') as infile:\n",
        "                    shutil.copyfileobj(infile, outfile)\n",
        "\n",
        "        shutil.rmtree(chunk_dir, ignore_errors=True)\n",
        "\n",
        "        file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "        print(f\"   ‚úÖ Complete! ({file_size:.2f} MB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   ‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_direct(url, output_file, episode_num):\n",
        "    \"\"\"Direct download with progress\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüì• Downloading Episode {episode_num}...\")\n",
        "        response = scraper.get(url, headers=headers, stream=True, timeout=30)\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "        with open(output_file, 'wb') as f:\n",
        "            with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"   ‚è≥\", ncols=80) as pbar:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "                        pbar.update(len(chunk))\n",
        "\n",
        "        file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "        print(f\"   ‚úÖ Complete! ({file_size:.2f} MB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   ‚ùå Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_episode(url, output_file, episode_num):\n",
        "    \"\"\"Main download with retry\"\"\"\n",
        "    is_m3u8 = '.m3u8' in url\n",
        "\n",
        "    if download_method == \"yt-dlp\":\n",
        "        download_func = download_with_ytdlp\n",
        "    elif download_method == \"aria2\" and not is_m3u8:\n",
        "        download_func = download_with_aria2\n",
        "    elif download_method == \"chunks\" and not is_m3u8:\n",
        "        download_func = download_with_chunks\n",
        "    else:\n",
        "        download_func = download_with_ytdlp if is_m3u8 else download_with_chunks\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        if attempt > 1:\n",
        "            print(f\"\\n   üîÑ Retry {attempt}/{max_retries}...\")\n",
        "            time.sleep(3)\n",
        "\n",
        "        if os.path.exists(output_file):\n",
        "            os.remove(output_file)\n",
        "\n",
        "        if download_func(url, output_file, episode_num):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "print(\"‚úÖ Download methods loaded\")\n",
        "\n",
        "# @title üì∫ **Fetch Anime Info & Download** { display-mode: \"form\" }\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üé¨ ANIMEKAI EPISODE DOWNLOADER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüîç Processing: {anime_url}\")\n",
        "\n",
        "anime_id, anime_title = get_anime_details(anime_url)\n",
        "\n",
        "if not anime_id:\n",
        "    raise Exception(\"‚ùå Could not extract anime ID\")\n",
        "\n",
        "print(f\"‚úÖ Anime ID: {anime_id}\")\n",
        "print(f\"üì∫ Title: {anime_title}\")\n",
        "\n",
        "episode_list = get_episode_list(anime_id)\n",
        "\n",
        "if not episode_list:\n",
        "    raise Exception(\"‚ùå No episodes found!\")\n",
        "\n",
        "print(f\"üìã Found {len(episode_list)} episode(s)\")\n",
        "\n",
        "# Determine episodes\n",
        "if download_mode == \"Single Episode\":\n",
        "    episodes_to_download = [ep for ep in episode_list if ep['number'] == single_episode]\n",
        "elif download_mode == \"Episode Range\":\n",
        "    episodes_to_download = [ep for ep in episode_list if start_episode <= ep['number'] <= end_episode]\n",
        "else:\n",
        "    episodes_to_download = episode_list\n",
        "\n",
        "if not episodes_to_download:\n",
        "    raise Exception(\"‚ùå No episodes match selection!\")\n",
        "\n",
        "print(f\"üì• Will download {len(episodes_to_download)} episode(s)\")\n",
        "\n",
        "download_dir = f\"downloads/{anime_title}\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "type_map = {\"Hard Sub\": \"sub\", \"Soft Sub\": \"softsub\", \"Dub & S-Sub\": \"dub\"}\n",
        "prefer_type_id = type_map.get(prefer_type, \"softsub\")\n",
        "\n",
        "downloaded_files = []\n",
        "failed_episodes = []\n",
        "\n",
        "for idx, episode in enumerate(episodes_to_download, 1):\n",
        "    print(f\"\\n[{idx}/{len(episodes_to_download)}] Episode {episode['number']}\")\n",
        "\n",
        "    try:\n",
        "        servers = get_video_servers(episode['token'])\n",
        "        if not servers:\n",
        "            failed_episodes.append(episode['number'])\n",
        "            continue\n",
        "\n",
        "        # Map preference types\n",
        "        # Note: \"dub\" type on AnimeKai means Dub & S-Sub (dual audio with soft subs)\n",
        "        type_map_search = {\n",
        "            \"Hard Sub\": \"sub\",\n",
        "            \"Soft Sub\": \"softsub\",\n",
        "            \"Dub & S-Sub\": \"dub\"  # This is the dual audio option\n",
        "        }\n",
        "        prefer_type_id = type_map_search.get(prefer_type, \"softsub\")\n",
        "\n",
        "        # Filter servers by preference\n",
        "        matching_servers = [s for s in servers if s['type'] == prefer_type_id and s['server_name'] == prefer_server]\n",
        "\n",
        "        # Fallback 1: Try any server with matching type\n",
        "        if not matching_servers:\n",
        "            matching_servers = [s for s in servers if s['type'] == prefer_type_id]\n",
        "\n",
        "        # Fallback 2: Try any server with preferred server name\n",
        "        if not matching_servers:\n",
        "            matching_servers = [s for s in servers if s['server_name'] == prefer_server]\n",
        "\n",
        "        # Fallback 3: Use first available server\n",
        "        if not matching_servers:\n",
        "            matching_servers = servers[:1]\n",
        "\n",
        "        server = matching_servers[0]\n",
        "\n",
        "        # Show what type we're actually downloading\n",
        "        type_display = {\n",
        "            \"sub\": \"Hard Sub\",\n",
        "            \"softsub\": \"Soft Sub\",\n",
        "            \"dub\": \"Dub & S-Sub (Dual Audio)\"\n",
        "        }.get(server['type'], server['type'])\n",
        "\n",
        "        print(f\"   üé• Server: {server['server_name']} | Type: {type_display}\")\n",
        "\n",
        "        video_url = get_video_url(server['server_id'], server['server_name'])\n",
        "        if not video_url:\n",
        "            failed_episodes.append(episode['number'])\n",
        "            continue\n",
        "\n",
        "        ep_num_str = f\"{int(episode['number']):03d}\"\n",
        "\n",
        "        # Add type to filename for clarity\n",
        "        type_suffix = {\n",
        "            \"sub\": \"HardSub\",\n",
        "            \"softsub\": \"SoftSub\",\n",
        "            \"dub\": \"DualAudio\"\n",
        "        }.get(server['type'], \"\")\n",
        "\n",
        "        filename = f\"{download_dir}/Episode_{ep_num_str}_{type_suffix}.mp4\"\n",
        "\n",
        "        if download_episode(video_url, filename, episode['number']):\n",
        "            downloaded_files.append(filename)\n",
        "        else:\n",
        "            failed_episodes.append(episode['number'])\n",
        "\n",
        "        time.sleep(2)\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Error: {e}\")\n",
        "        failed_episodes.append(episode['number'])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä DOWNLOAD SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n‚úÖ Downloaded: {len(downloaded_files)} episode(s)\")\n",
        "if failed_episodes:\n",
        "    print(f\"‚ùå Failed: {', '.join(map(str, failed_episodes))}\")\n",
        "if downloaded_files:\n",
        "    total_size = sum(os.path.getsize(f) for f in downloaded_files) / (1024*1024)\n",
        "    print(f\"üíæ Total: {total_size:.2f} MB\")\n",
        "print(\"\\nüéâ COMPLETE!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBOEj0HxBuGE",
        "outputId": "8ad4943a-b617-4938-cb1d-5aa56d0e2d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing required packages...\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ All dependencies installed!\n",
            "\n",
            "‚úÖ Configuration loaded!\n",
            "‚úÖ Core functions loaded\n",
            "‚úÖ Download methods loaded\n",
            "\n",
            "======================================================================\n",
            "üé¨ ANIMEKAI EPISODE DOWNLOADER\n",
            "======================================================================\n",
            "\n",
            "üîç Processing: https://anikai.to/watch/attack-on-titan-season-3-oxgk\n",
            "‚úÖ Anime ID: eoI\n",
            "üì∫ Title: Unknown\n",
            "üìã Found 12 episode(s)\n",
            "üì• Will download 12 episode(s)\n",
            "\n",
            "[1/12] Episode 1.0\n",
            "   üé• Server: Server 1 | Type: Dub & S-Sub (Dual Audio)\n",
            "\n",
            "üì• Downloading Episode 1.0 with yt-dlp...\n",
            "   File: Episode_001_DualAudio.mp4\n",
            "   ‚è≥ Progress: 99.5%\n",
            "   ‚úÖ Complete! (262.46 MB)\n",
            "\n",
            "[2/12] Episode 2.0\n",
            "   üé• Server: Server 1 | Type: Dub & S-Sub (Dual Audio)\n",
            "\n",
            "üì• Downloading Episode 2.0 with yt-dlp...\n",
            "   File: Episode_002_DualAudio.mp4\n",
            "   ‚è≥ Progress: 99.5%\n",
            "   ‚úÖ Complete! (310.39 MB)\n",
            "\n",
            "[3/12] Episode 3.0\n",
            "   üé• Server: Server 1 | Type: Dub & S-Sub (Dual Audio)\n",
            "\n",
            "üì• Downloading Episode 3.0 with yt-dlp...\n",
            "   File: Episode_003_DualAudio.mp4\n",
            "   ‚è≥ Progress: 99.7%\n",
            "   ‚úÖ Complete! (243.49 MB)\n",
            "\n",
            "[4/12] Episode 4.0\n",
            "   üé• Server: Server 1 | Type: Dub & S-Sub (Dual Audio)\n",
            "\n",
            "üì• Downloading Episode 4.0 with yt-dlp...\n",
            "   File: Episode_004_DualAudio.mp4\n",
            "   ‚è≥ Progress: 99.7%\n",
            "   ‚úÖ Complete! (309.38 MB)\n",
            "\n",
            "[5/12] Episode 5.0\n",
            "   üé• Server: Server 1 | Type: Dub & S-Sub (Dual Audio)\n",
            "\n",
            "üì• Downloading Episode 5.0 with yt-dlp...\n",
            "   File: Episode_005_DualAudio.mp4\n",
            "   ‚è≥ Progress: 99.6%\n",
            "   ‚úÖ Complete! (279.74 MB)\n",
            "\n",
            "[6/12] Episode 6.0\n",
            "   üé• Server: Server 1 | Type: Dub & S-Sub (Dual Audio)\n",
            "\n",
            "üì• Downloading Episode 6.0 with yt-dlp...\n",
            "   File: Episode_006_DualAudio.mp4\n",
            "   ‚è≥ Progress: 8.0%"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Manhwa Scraper, Merger & Uploader (Batch Mode) - ADVANCED PRIORITY MODE\n",
        "# Scrapes manhwa chapters from Comick.art, merges into PDF/CBZ in batches, uploads to GDrive/Gofile\n",
        "\n",
        "#@title **Install Dependencies** { display-mode: \"form\" }\n",
        "!pip install -q pillow beautifulsoup4 requests pyrogram tgcrypto langdetect img2pdf PyPDF2\n",
        "!apt-get install -y fonts-noto-cjk\n",
        "\n",
        "#@title **Import Libraries** { display-mode: \"form\" }\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import shutil\n",
        "import zipfile\n",
        "import requests\n",
        "import traceback\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageDraw\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive, files\n",
        "import img2pdf\n",
        "from PyPDF2 import PdfMerger\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from threading import Lock\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "\n",
        "#@title **Configuration** { display-mode: \"form\" }\n",
        "#@markdown Enter the Comick.art manga URL\n",
        "manga_url = \"https://comick.art/comic/03-omniscient-reader-s-viewpoint\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Scanlation Group Filter (leave empty to use priority mode)\n",
        "preferred_group = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Priority Mode (if group filter is empty)\n",
        "priority_mode = \"Mix Best Available\" #@param [\"Most Upvoted\", \"Latest Upload\", \"Mix Best Available\"]\n",
        "\n",
        "#@markdown Exclude groups (comma-separated, e.g., \"Official,WEBTOON,LINE Webtoon,Colored Manga\")\n",
        "exclude_groups = \"Official,WEBTOON,LINE Webtoon\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Only include numbered chapters (exclude side stories, prologues, etc.)\n",
        "only_numbered_chapters = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Chapters per file (creates separate files for every N chapters)\n",
        "chapters_per_file = 100 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Starting chapter (1 = from beginning)\n",
        "start_chapter = 1 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Total chapters to download (0 = all available)\n",
        "total_chapters = 0 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Upload destination\n",
        "upload_to = \"Gofile\" #@param [\"GDrive\", \"Gofile\", \"Both\", \"None\"]\n",
        "\n",
        "#@markdown Create PDF\n",
        "create_pdf = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Create CBZ\n",
        "create_cbz = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Number of parallel downloads (higher = faster, but may cause issues)\n",
        "max_workers = 10 #@param {type:\"integer\"}\n",
        "\n",
        "#@title **Core Functions** { display-mode: \"form\" }\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "    'Referer': 'https://comick.art/',\n",
        "}\n",
        "\n",
        "def fetch_content(url, headers=None):\n",
        "    \"\"\"Fetch content from URL\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_manga_info(url):\n",
        "    \"\"\"Extract manga information\"\"\"\n",
        "    try:\n",
        "        content = fetch_content(url, headers=headers)\n",
        "        soup = BeautifulSoup(content, \"html.parser\")\n",
        "        script_tag = soup.find(\"script\", {\"id\": \"comic-data\"})\n",
        "\n",
        "        if not script_tag:\n",
        "            return None\n",
        "\n",
        "        comic = json.loads(script_tag.string)\n",
        "        slug = comic['slug']\n",
        "        title = comic['title'].strip().split(\"[\")[0] if \"[\" in comic['title'] else comic['title']\n",
        "\n",
        "        comic_url = f'https://comick.art/api/comics/{slug}/chapter-list?lang=en'\n",
        "\n",
        "        return {\n",
        "            'title': title,\n",
        "            'slug': slug,\n",
        "            'comic_url': comic_url\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting manga info: {e}\")\n",
        "        return None\n",
        "\n",
        "def is_numeric_chapter(ch_num):\n",
        "    \"\"\"Check if chapter number is numeric (not prologue/special)\"\"\"\n",
        "    try:\n",
        "        float(ch_num)\n",
        "        return True\n",
        "    except (ValueError, TypeError):\n",
        "        return False\n",
        "\n",
        "def parse_date(date_str):\n",
        "    \"\"\"Parse date string to datetime object\"\"\"\n",
        "    try:\n",
        "        return datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "    except:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "        except:\n",
        "            return datetime.min\n",
        "\n",
        "def get_chapters(comic_url, preferred_group=None, exclude_groups_str=\"\", priority_mode_str=\"Mix Best Available\", only_numbered=False):\n",
        "    \"\"\"Get chapter URLs with advanced priority modes\"\"\"\n",
        "    try:\n",
        "        content = fetch_content(comic_url, headers=headers)\n",
        "        first_json = json.loads(content)\n",
        "\n",
        "        last_page = int(first_json['pagination']['last_page'])\n",
        "        manga_hid = comic_url.split(\"/\")[-2]\n",
        "\n",
        "        # Parse exclude groups\n",
        "        excluded = set()\n",
        "        if exclude_groups_str:\n",
        "            excluded = set(g.strip().lower() for g in exclude_groups_str.split(',') if g.strip())\n",
        "\n",
        "        # Store all chapters per chapter number\n",
        "        chapters_dict = defaultdict(list)\n",
        "\n",
        "        print(f\"  üìÑ Scanning {last_page} pages...\")\n",
        "\n",
        "        for page in range(1, last_page + 1):\n",
        "            if page % 5 == 0:\n",
        "                print(f\"  ‚è≥ Progress: {page}/{last_page} pages...\", end='\\r')\n",
        "\n",
        "            page_url = f\"{comic_url}&page={page}\"\n",
        "            content = fetch_content(page_url, headers=headers)\n",
        "            data = json.loads(content).get(\"data\", [])\n",
        "\n",
        "            for chapter in data:\n",
        "                # Only English\n",
        "                if chapter.get(\"lang\") != \"en\":\n",
        "                    continue\n",
        "\n",
        "                # Get chapter number\n",
        "                ch_num = chapter.get(\"chap\")\n",
        "                if ch_num is None or ch_num == \"\":\n",
        "                    ch_num = chapter.get(\"title\", \"0\")\n",
        "\n",
        "                ch_num_str = str(ch_num)\n",
        "\n",
        "                # Skip non-numeric if only_numbered is True\n",
        "                if only_numbered and not is_numeric_chapter(ch_num):\n",
        "                    continue\n",
        "\n",
        "                groups = chapter.get(\"group_name\", [])\n",
        "\n",
        "                # Skip excluded groups\n",
        "                if excluded and any(g.lower() in excluded for g in groups):\n",
        "                    continue\n",
        "\n",
        "                # If preferred group specified, skip if not from that group\n",
        "                if preferred_group and preferred_group not in groups:\n",
        "                    continue\n",
        "\n",
        "                # Store chapter with metadata\n",
        "                up_count = chapter.get(\"up_count\", 0)\n",
        "                down_count = chapter.get(\"down_count\", 0)\n",
        "                group_str = ', '.join(groups) if groups else 'Unknown'\n",
        "                created_at = chapter.get(\"created_at\", \"\")\n",
        "\n",
        "                chapters_dict[ch_num_str].append({\n",
        "                    'hid': chapter[\"hid\"],\n",
        "                    'chap': chapter.get(\"chap\", \"0\"),\n",
        "                    'ch_num': ch_num,\n",
        "                    'group': group_str,\n",
        "                    'up_count': up_count,\n",
        "                    'down_count': down_count,\n",
        "                    'created_at': created_at,\n",
        "                    'lang': chapter.get(\"lang\", \"en\"),\n",
        "                    'groups_raw': groups\n",
        "                })\n",
        "\n",
        "        print(f\"\\n  ‚úÖ Found {len(chapters_dict)} unique chapter numbers\")\n",
        "\n",
        "        # Preferred groups for Mix Best Available (in priority order)\n",
        "        preferred_groups_priority = [\n",
        "            'flame comics',\n",
        "            'asura',\n",
        "            'reaper scans',\n",
        "            'luminous scans',\n",
        "            'void scans'\n",
        "        ]\n",
        "\n",
        "        # Now select best chapter for each number\n",
        "        urls = []\n",
        "        selection_stats = defaultdict(int)\n",
        "\n",
        "        for ch_num_str, chapter_list in chapters_dict.items():\n",
        "            selected_chapter = None\n",
        "\n",
        "            if preferred_group:\n",
        "                # Use specific group\n",
        "                selected_chapter = chapter_list[0]\n",
        "            elif priority_mode_str == \"Most Upvoted\":\n",
        "                # Highest upvotes\n",
        "                selected_chapter = max(chapter_list, key=lambda x: (x['up_count'], -x['down_count']))\n",
        "            elif priority_mode_str == \"Latest Upload\":\n",
        "                # Most recent upload\n",
        "                selected_chapter = max(chapter_list, key=lambda x: parse_date(x['created_at']))\n",
        "            else:  # Mix Best Available\n",
        "                # Smart selection: prefer quality groups, then upvotes\n",
        "\n",
        "                # First, try to find from preferred groups\n",
        "                for pref_group in preferred_groups_priority:\n",
        "                    candidates = [ch for ch in chapter_list\n",
        "                                if any(pref_group in g.lower() for g in ch['groups_raw'])]\n",
        "                    if candidates:\n",
        "                        # Among preferred group, choose highest upvoted\n",
        "                        selected_chapter = max(candidates, key=lambda x: x['up_count'])\n",
        "                        break\n",
        "\n",
        "                # If no preferred group found, fallback to highest upvoted\n",
        "                if not selected_chapter:\n",
        "                    selected_chapter = max(chapter_list, key=lambda x: (x['up_count'], -x['down_count']))\n",
        "\n",
        "            if selected_chapter:\n",
        "                link = f'https://comick.art/comic/{manga_hid}/{selected_chapter[\"hid\"]}-chapter-{selected_chapter[\"chap\"]}-{selected_chapter[\"lang\"]}'\n",
        "                urls.append((link, selected_chapter['ch_num'], selected_chapter['group'], selected_chapter['up_count']))\n",
        "                selection_stats[selected_chapter['group']] += 1\n",
        "\n",
        "        # Sort by chapter number\n",
        "        def sort_key(item):\n",
        "            try:\n",
        "                return (0, float(item[1]))  # Numeric chapters\n",
        "            except (ValueError, TypeError):\n",
        "                return (1, str(item[1]))  # Non-numeric chapters\n",
        "\n",
        "        urls.sort(key=sort_key)\n",
        "\n",
        "        print(f\"\\n  üìä Selected chapters by group:\")\n",
        "        for group, count in sorted(selection_stats.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "            print(f\"    ‚Ä¢ {group}: {count} chapters\")\n",
        "\n",
        "        return urls\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting chapters: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return []\n",
        "\n",
        "def get_chapter_images(chapter_url):\n",
        "    \"\"\"Get image URLs from chapter\"\"\"\n",
        "    try:\n",
        "        content = fetch_content(chapter_url, headers=headers)\n",
        "        soup = BeautifulSoup(content, \"html.parser\")\n",
        "        script_tag = soup.find(\"script\", {\"id\": \"sv-data\"})\n",
        "\n",
        "        if not script_tag:\n",
        "            return []\n",
        "\n",
        "        data = json.loads(script_tag.string)\n",
        "\n",
        "        if \"chap\" in data:\n",
        "            return []\n",
        "\n",
        "        images = [img[\"url\"] for img in data[\"chapter\"][\"images\"] if img.get(\"url\")]\n",
        "        return images\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting chapter images: {e}\")\n",
        "        return []\n",
        "\n",
        "def download_image(url, filepath, headers=None, retry_count=3):\n",
        "    \"\"\"Download single image with retries\"\"\"\n",
        "    for attempt in range(retry_count):\n",
        "        try:\n",
        "            download_headers = headers.copy() if headers else {}\n",
        "            download_headers.update({\n",
        "                'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',\n",
        "                'Accept-Language': 'en-US,en;q=0.9',\n",
        "                'Connection': 'keep-alive',\n",
        "            })\n",
        "\n",
        "            response = requests.get(url, headers=download_headers, timeout=30, stream=True)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                with open(filepath, 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "\n",
        "                if os.path.exists(filepath) and os.path.getsize(filepath) > 0:\n",
        "                    return True\n",
        "\n",
        "            if attempt < retry_count - 1:\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt < retry_count - 1:\n",
        "                continue\n",
        "\n",
        "    return False\n",
        "\n",
        "def download_image_wrapper(args):\n",
        "    \"\"\"Wrapper for parallel downloading\"\"\"\n",
        "    url, filepath, headers = args\n",
        "    return download_image(url, filepath, headers)\n",
        "\n",
        "def download_chapter_parallel(images, batch_dir, ch_formatted, headers, max_workers=10):\n",
        "    \"\"\"Download all images of a chapter in parallel\"\"\"\n",
        "    download_tasks = []\n",
        "\n",
        "    for img_idx, img_url in enumerate(images, 1):\n",
        "        img_path = os.path.join(batch_dir, f\"ch{ch_formatted}_p{img_idx:03d}.jpg\")\n",
        "        download_tasks.append((img_url, img_path, headers))\n",
        "\n",
        "    downloaded = 0\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(download_image_wrapper, task): task for task in download_tasks}\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            if future.result():\n",
        "                downloaded += 1\n",
        "\n",
        "    return downloaded\n",
        "\n",
        "def create_pdf_from_images(image_folder, output_path, title):\n",
        "    \"\"\"Create PDF from images with progress tracking and automatic image fixing\"\"\"\n",
        "    try:\n",
        "        image_files = sorted([f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg', '.webp'))])\n",
        "\n",
        "        if not image_files:\n",
        "            print(\"  ‚ö†Ô∏è No images found\")\n",
        "            return False\n",
        "\n",
        "        total_images = len(image_files)\n",
        "        print(f\"  üìä Processing {total_images} images...\")\n",
        "\n",
        "        temp_folder = os.path.join(image_folder, 'temp_pdf')\n",
        "        os.makedirs(temp_folder, exist_ok=True)\n",
        "\n",
        "        valid_image_paths = []\n",
        "        fixed_count = 0\n",
        "\n",
        "        for idx, img_file in enumerate(image_files, 1):\n",
        "            img_path = os.path.join(image_folder, img_file)\n",
        "            temp_path = os.path.join(temp_folder, f\"fixed_{idx:05d}.jpg\")\n",
        "\n",
        "            try:\n",
        "                with Image.open(img_path) as img:\n",
        "                    width, height = img.size\n",
        "\n",
        "                    min_dimension = 100\n",
        "                    needs_resize = False\n",
        "\n",
        "                    if width < min_dimension or height < min_dimension:\n",
        "                        scale = max(min_dimension / width, min_dimension / height)\n",
        "                        new_width = max(int(width * scale), min_dimension)\n",
        "                        new_height = max(int(height * scale), min_dimension)\n",
        "\n",
        "                        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "                        needs_resize = True\n",
        "                        fixed_count += 1\n",
        "\n",
        "                    if img.mode != 'RGB':\n",
        "                        img = img.convert('RGB')\n",
        "                        needs_resize = True\n",
        "\n",
        "                    if needs_resize or img.mode != 'RGB':\n",
        "                        img.save(temp_path, 'JPEG', quality=95, optimize=True)\n",
        "                        valid_image_paths.append(temp_path)\n",
        "                    else:\n",
        "                        valid_image_paths.append(img_path)\n",
        "\n",
        "                if idx % 100 == 0 or idx == total_images:\n",
        "                    print(f\"  ‚è≥ Processed {idx}/{total_images} images...\", end='\\r')\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n  ‚ö†Ô∏è Corrupted image {img_file}, creating placeholder...\")\n",
        "\n",
        "                placeholder = Image.new('RGB', (800, 1200), color=(240, 240, 240))\n",
        "                draw = ImageDraw.Draw(placeholder)\n",
        "\n",
        "                text = f\"Image {idx}\\nCould not load\\n{img_file[:30]}\"\n",
        "                draw.text((400, 600), text, fill=(128, 128, 128), anchor=\"mm\")\n",
        "\n",
        "                placeholder.save(temp_path, 'JPEG', quality=95)\n",
        "                valid_image_paths.append(temp_path)\n",
        "                fixed_count += 1\n",
        "\n",
        "        if not valid_image_paths:\n",
        "            print(\"\\n  ‚ùå No valid images to create PDF\")\n",
        "            shutil.rmtree(temp_folder, ignore_errors=True)\n",
        "            return False\n",
        "\n",
        "        if fixed_count > 0:\n",
        "            print(f\"\\n  üîß Fixed/processed {fixed_count} images for PDF compatibility\")\n",
        "\n",
        "        print(f\"\\n  üîÑ Converting {len(valid_image_paths)} images to PDF in batches...\")\n",
        "\n",
        "        batch_size = 100\n",
        "        temp_pdfs = []\n",
        "        pdf_temp_folder = os.path.join(temp_folder, 'pdf_batches')\n",
        "        os.makedirs(pdf_temp_folder, exist_ok=True)\n",
        "\n",
        "        for i in range(0, len(valid_image_paths), batch_size):\n",
        "            batch_paths = valid_image_paths[i:i + batch_size]\n",
        "            progress = min(i + batch_size, len(valid_image_paths))\n",
        "            print(f\"  üìÑ Processing batch {progress}/{len(valid_image_paths)} images...\", end='\\r')\n",
        "\n",
        "            batch_pdf_path = os.path.join(pdf_temp_folder, f\"batch_{i:05d}.pdf\")\n",
        "            with open(batch_pdf_path, \"wb\") as f:\n",
        "                f.write(img2pdf.convert(batch_paths))\n",
        "            temp_pdfs.append(batch_pdf_path)\n",
        "\n",
        "        print(f\"\\n  üîó Merging {len(temp_pdfs)} PDF batches into final file...\")\n",
        "\n",
        "        merger = PdfMerger()\n",
        "        for idx, pdf_path in enumerate(temp_pdfs, 1):\n",
        "            print(f\"  üìö Merging batch {idx}/{len(temp_pdfs)}...\", end='\\r')\n",
        "            merger.append(pdf_path)\n",
        "\n",
        "        print(f\"\\n  üíæ Writing final PDF...\")\n",
        "        merger.write(output_path)\n",
        "        merger.close()\n",
        "\n",
        "        shutil.rmtree(temp_folder, ignore_errors=True)\n",
        "\n",
        "        file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "        print(f\"  ‚úÖ PDF created: {os.path.basename(output_path)} ({file_size:.2f} MB)\")\n",
        "        print(f\"  üìÑ Total pages: {len(valid_image_paths)}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error creating PDF: {e}\")\n",
        "        print(f\"  üîç Error details: {traceback.format_exc()}\")\n",
        "        try:\n",
        "            temp_folder = os.path.join(image_folder, 'temp_pdf')\n",
        "            shutil.rmtree(temp_folder, ignore_errors=True)\n",
        "        except:\n",
        "            pass\n",
        "        return False\n",
        "\n",
        "def create_cbz_from_images(image_folder, output_path, title):\n",
        "    \"\"\"Create CBZ from images with progress tracking\"\"\"\n",
        "    try:\n",
        "        image_files = []\n",
        "        for root, dirs, files in os.walk(image_folder):\n",
        "            for file in sorted(files):\n",
        "                if file.endswith(('.jpg', '.png', '.jpeg', '.webp')):\n",
        "                    image_files.append(os.path.join(root, file))\n",
        "\n",
        "        if not image_files:\n",
        "            print(\"  ‚ö†Ô∏è No images found\")\n",
        "            return False\n",
        "\n",
        "        total_images = len(image_files)\n",
        "        print(f\"  üìä Compressing {total_images} images...\")\n",
        "\n",
        "        with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:\n",
        "            for idx, file_path in enumerate(image_files, 1):\n",
        "                arcname = os.path.relpath(file_path, image_folder)\n",
        "                zipf.write(file_path, arcname)\n",
        "\n",
        "                if idx % 50 == 0 or idx == total_images:\n",
        "                    print(f\"  ‚è≥ Compressed {idx}/{total_images} images...\", end='\\r')\n",
        "\n",
        "        file_size = os.path.getsize(output_path) / (1024 * 1024)\n",
        "        print(f\"\\n  ‚úÖ CBZ created: {os.path.basename(output_path)} ({file_size:.2f} MB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error creating CBZ: {e}\")\n",
        "        return False\n",
        "\n",
        "def upload_to_gofile(filepath, retry_count=3):\n",
        "    \"\"\"Upload file to Gofile.io with retries\"\"\"\n",
        "    for attempt in range(retry_count):\n",
        "        try:\n",
        "            server_response = requests.get(\"https://api.gofile.io/servers\", timeout=30)\n",
        "            server_data = server_response.json()\n",
        "\n",
        "            if server_data['status'] != 'ok':\n",
        "                print(f\"  ‚ö†Ô∏è Attempt {attempt + 1}/{retry_count}: Failed to get Gofile server\")\n",
        "                if attempt < retry_count - 1:\n",
        "                    print(f\"  ‚è≥ Retrying in 5 seconds...\")\n",
        "                    import time\n",
        "                    time.sleep(5)\n",
        "                    continue\n",
        "                return None\n",
        "\n",
        "            servers = server_data['data']['servers']\n",
        "            if not servers:\n",
        "                print(f\"  ‚ö†Ô∏è Attempt {attempt + 1}/{retry_count}: No servers available\")\n",
        "                if attempt < retry_count - 1:\n",
        "                    print(f\"  ‚è≥ Retrying in 5 seconds...\")\n",
        "                    import time\n",
        "                    time.sleep(5)\n",
        "                    continue\n",
        "                return None\n",
        "\n",
        "            server = servers[0]['name']\n",
        "            upload_url = f\"https://{server}.gofile.io/contents/uploadfile\"\n",
        "\n",
        "            print(f\"  üì§ Uploading {os.path.basename(filepath)} (Attempt {attempt + 1}/{retry_count})...\")\n",
        "\n",
        "            with open(filepath, 'rb') as f:\n",
        "                files_data = {'file': f}\n",
        "                response = requests.post(upload_url, files=files_data, timeout=600)\n",
        "\n",
        "            result = response.json()\n",
        "\n",
        "            if result['status'] == 'ok':\n",
        "                download_page = result['data']['downloadPage']\n",
        "                print(f\"  ‚úÖ Gofile: {download_page}\")\n",
        "                return download_page\n",
        "            else:\n",
        "                print(f\"  ‚ö†Ô∏è Attempt {attempt + 1}/{retry_count}: Upload failed - {result.get('status', 'unknown error')}\")\n",
        "                if attempt < retry_count - 1:\n",
        "                    print(f\"  ‚è≥ Retrying in 5 seconds...\")\n",
        "                    import time\n",
        "                    time.sleep(5)\n",
        "                    continue\n",
        "                return None\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            print(f\"  ‚ö†Ô∏è Attempt {attempt + 1}/{retry_count}: Upload timeout\")\n",
        "            if attempt < retry_count - 1:\n",
        "                print(f\"  ‚è≥ Retrying in 10 seconds...\")\n",
        "                import time\n",
        "                time.sleep(10)\n",
        "                continue\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö†Ô∏è Attempt {attempt + 1}/{retry_count}: Error - {e}\")\n",
        "            if attempt < retry_count - 1:\n",
        "                print(f\"  ‚è≥ Retrying in 5 seconds...\")\n",
        "                import time\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "            return None\n",
        "\n",
        "    print(f\"  ‚ùå Failed to upload after {retry_count} attempts\")\n",
        "    return None\n",
        "\n",
        "#@title **Main Scraping Process** { display-mode: \"form\" }\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üéØ MANHWA BATCH SCRAPER & UPLOADER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nüìñ Fetching manga info from: {manga_url}\")\n",
        "manga_info = get_manga_info(manga_url)\n",
        "\n",
        "if not manga_info:\n",
        "    print(\"‚ùå Failed to get manga info\")\n",
        "else:\n",
        "    title = manga_info['title']\n",
        "    print(f\"‚úÖ Title: {title}\")\n",
        "\n",
        "    if preferred_group:\n",
        "        print(f\"\\nüéØ Using specific group: {preferred_group}\")\n",
        "    else:\n",
        "        print(f\"\\nüéØ Priority mode: {priority_mode}\")\n",
        "\n",
        "    if exclude_groups:\n",
        "        print(f\"üö´ Excluding groups: {exclude_groups}\")\n",
        "\n",
        "    if only_numbered_chapters:\n",
        "        print(f\"üî¢ Filter: Only numbered chapters (skipping prologue/side stories)\")\n",
        "\n",
        "    print(f\"üåê Language: English only\")\n",
        "\n",
        "    print(f\"\\nüìö Fetching and analyzing chapters...\")\n",
        "    all_chapters = get_chapters(manga_info['comic_url'], preferred_group, exclude_groups, priority_mode, only_numbered_chapters)\n",
        "    print(f\"‚úÖ Total unique chapters selected: {len(all_chapters)}\")\n",
        "\n",
        "    # Apply chapter range\n",
        "    if start_chapter > 1 or total_chapters > 0:\n",
        "        # Find chapters in range\n",
        "        filtered_chapters = []\n",
        "        for url, ch_num, group, upvotes in all_chapters:\n",
        "            try:\n",
        "                ch_float = float(ch_num)\n",
        "                if ch_float >= start_chapter:\n",
        "                    if total_chapters == 0 or ch_float < start_chapter + total_chapters:\n",
        "                        filtered_chapters.append((url, ch_num, group, upvotes))\n",
        "            except:\n",
        "                # Non-numeric chapters - include if no filtering\n",
        "                if start_chapter <= 1:\n",
        "                    filtered_chapters.append((url, ch_num, group, upvotes))\n",
        "\n",
        "        selected_chapters = filtered_chapters\n",
        "    else:\n",
        "        selected_chapters = all_chapters\n",
        "\n",
        "    print(f\"\\nüì• Will download {len(selected_chapters)} chapters (starting from chapter {start_chapter})\")\n",
        "    print(f\"üì¶ Creating files with {chapters_per_file} chapters each\")\n",
        "\n",
        "    # Show sample\n",
        "    print(f\"\\nüìã Sample chapters (first 5):\")\n",
        "    for i, (url, ch_num, group, upvotes) in enumerate(selected_chapters[:5], 1):\n",
        "        print(f\"  {i}. Chapter {ch_num} - [{group}] (üëç {upvotes})\")\n",
        "\n",
        "    if len(selected_chapters) > 5:\n",
        "        print(f\"  ...\")\n",
        "        for i, (url, ch_num, group, upvotes) in enumerate(selected_chapters[-2:], len(selected_chapters)-1):\n",
        "            print(f\"  {i}. Chapter {ch_num} - [{group}] (üëç {upvotes})\")\n",
        "\n",
        "    base_download_dir = f\"/content/{title}\"\n",
        "    output_dir = \"/content/output\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    batches = []\n",
        "    for i in range(0, len(selected_chapters), chapters_per_file):\n",
        "        batch = selected_chapters[i:i + chapters_per_file]\n",
        "        batches.append(batch)\n",
        "\n",
        "    print(f\"\\nüìä Total batches: {len(batches)}\")\n",
        "\n",
        "    gdrive_dir = None\n",
        "    if upload_to in [\"GDrive\", \"Both\"]:\n",
        "        print(\"\\n‚òÅÔ∏è Mounting Google Drive...\")\n",
        "        drive.mount('/content/gdrive')\n",
        "        gdrive_dir = \"/content/gdrive/MyDrive/Manhwa\"\n",
        "        os.makedirs(gdrive_dir, exist_ok=True)\n",
        "        print(f\"‚úÖ Google Drive mounted at: {gdrive_dir}\")\n",
        "\n",
        "    safe_title = re.sub(r'[<>:\"/\\\\|?*]', '', title)\n",
        "\n",
        "    # Store all upload links\n",
        "    all_upload_links = {}\n",
        "\n",
        "    for batch_num, batch in enumerate(batches, 1):\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(f\"üì¶ BATCH {batch_num}/{len(batches)}\")\n",
        "\n",
        "        first_ch = batch[0][1]\n",
        "        last_ch = batch[-1][1]\n",
        "\n",
        "        try:\n",
        "            first_ch_display = f\"{float(first_ch):.1f}\".rstrip('0').rstrip('.')\n",
        "        except:\n",
        "            first_ch_display = str(first_ch)\n",
        "\n",
        "        try:\n",
        "            last_ch_display = f\"{float(last_ch):.1f}\".rstrip('0').rstrip('.')\n",
        "        except:\n",
        "            last_ch_display = str(last_ch)\n",
        "\n",
        "        batch_name = f\"{safe_title} Chapters {first_ch_display}-{last_ch_display}\"\n",
        "        print(f\"üìñ Chapters: {first_ch_display} to {last_ch_display} ({len(batch)} chapters)\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        batch_dir = os.path.join(base_download_dir, f\"batch_{batch_num}\")\n",
        "        os.makedirs(batch_dir, exist_ok=True)\n",
        "\n",
        "        total_images = 0\n",
        "        for idx, (ch_url, ch_num, group, upvotes) in enumerate(batch, 1):\n",
        "            print(f\"\\n[{idx}/{len(batch)}] Chapter {ch_num} [{group}] (üëç {upvotes})\")\n",
        "\n",
        "            images = get_chapter_images(ch_url)\n",
        "            if not images:\n",
        "                print(f\"  ‚ö†Ô∏è No images found\")\n",
        "                continue\n",
        "\n",
        "            print(f\"  üì∑ Found {len(images)} images - downloading in parallel...\")\n",
        "\n",
        "            try:\n",
        "                ch_formatted = f\"{float(ch_num):06.1f}\"\n",
        "            except (ValueError, TypeError):\n",
        "                ch_formatted = re.sub(r'[<>:\"/\\\\|?*]', '', str(ch_num)).zfill(6)\n",
        "\n",
        "            downloaded = download_chapter_parallel(images, batch_dir, ch_formatted, headers, max_workers)\n",
        "            total_images += downloaded\n",
        "\n",
        "            print(f\"  ‚úÖ Downloaded {downloaded}/{len(images)} images\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Batch total: {total_images} images\")\n",
        "\n",
        "        uploaded_files = []\n",
        "        batch_links = []\n",
        "\n",
        "        if create_pdf:\n",
        "            print(f\"\\nüìÑ Creating PDF for batch {batch_num}...\")\n",
        "            pdf_path = os.path.join(output_dir, f\"{batch_name}.pdf\")\n",
        "            if create_pdf_from_images(batch_dir, pdf_path, batch_name):\n",
        "                uploaded_files.append(('PDF', pdf_path))\n",
        "\n",
        "        if create_cbz:\n",
        "            print(f\"\\nüì¶ Creating CBZ for batch {batch_num}...\")\n",
        "            cbz_path = os.path.join(output_dir, f\"{batch_name}.cbz\")\n",
        "            if create_cbz_from_images(batch_dir, cbz_path, batch_name):\n",
        "                uploaded_files.append(('CBZ', cbz_path))\n",
        "\n",
        "        if upload_to in [\"GDrive\", \"Both\"] and gdrive_dir:\n",
        "            print(f\"\\n‚òÅÔ∏è Uploading batch {batch_num} to Google Drive...\")\n",
        "            for file_type, filepath in uploaded_files:\n",
        "                dest = os.path.join(gdrive_dir, os.path.basename(filepath))\n",
        "                shutil.copy(filepath, dest)\n",
        "                gdrive_link = dest\n",
        "                batch_links.append((f\"{file_type} (GDrive)\", gdrive_link))\n",
        "                print(f\"  ‚úÖ {file_type}: {dest}\")\n",
        "\n",
        "        if upload_to in [\"Gofile\", \"Both\"]:\n",
        "            print(f\"\\nüåê Uploading batch {batch_num} to Gofile.io...\")\n",
        "            for file_type, filepath in uploaded_files:\n",
        "                gofile_link = upload_to_gofile(filepath)\n",
        "                if gofile_link:\n",
        "                    batch_links.append((f\"{file_type} (Gofile)\", gofile_link))\n",
        "\n",
        "        if batch_links:\n",
        "            all_upload_links[batch_num] = batch_links\n",
        "\n",
        "        if upload_to == \"None\":\n",
        "            print(f\"\\nüíæ Batch {batch_num} files saved locally in /content/output/\")\n",
        "\n",
        "        print(f\"\\nüßπ Cleaning up batch {batch_num} temporary files...\")\n",
        "        shutil.rmtree(batch_dir, ignore_errors=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚úÖ ALL BATCHES COMPLETED!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nüìä Summary:\")\n",
        "    print(f\"  ‚Ä¢ Total batches: {len(batches)}\")\n",
        "    print(f\"  ‚Ä¢ Chapters processed: {len(selected_chapters)}\")\n",
        "    print(f\"  ‚Ä¢ Files created: {len(batches) * len([x for x in [create_pdf, create_cbz] if x])}\")\n",
        "\n",
        "    if all_upload_links:\n",
        "        print(\"\\nüîó UPLOAD LINKS:\")\n",
        "        print(\"=\" * 70)\n",
        "        for batch_num, links in all_upload_links.items():\n",
        "            print(f\"\\nüì¶ Batch {batch_num}:\")\n",
        "            for file_type, link in links:\n",
        "                print(f\"  ‚Ä¢ {file_type}: {link}\")\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    if upload_to == \"None\":\n",
        "        print(\"\\nüì• Downloading all files to your computer...\")\n",
        "        for file in os.listdir(output_dir):\n",
        "            filepath = os.path.join(output_dir, file)\n",
        "            if os.path.isfile(filepath):\n",
        "                files.download(filepath)\n",
        "\n",
        "    print(\"\\nüßπ Final cleanup...\")\n",
        "    shutil.rmtree(base_download_dir, ignore_errors=True)\n",
        "\n",
        "    print(\"\\n‚ú® Done! Enjoy your manhwa collection!\")\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FugZMsZsZcRO",
        "outputId": "2a8395f2-85b1-4261-cb15-6585751d53da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "fonts-noto-cjk is already the newest version (1:20220127+repack1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
            "======================================================================\n",
            "üéØ MANHWA BATCH SCRAPER & UPLOADER\n",
            "======================================================================\n",
            "\n",
            "üìñ Fetching manga info from: https://comick.art/comic/03-omniscient-reader-s-viewpoint\n",
            "‚úÖ Title: Omniscient Reader's Viewpoint\n",
            "\n",
            "üéØ Priority mode: Mix Best Available\n",
            "üö´ Excluding groups: Official,WEBTOON,LINE Webtoon\n",
            "üåê Language: English only\n",
            "\n",
            "üìö Fetching and analyzing chapters...\n",
            "  üìÑ Scanning 36 pages...\n",
            "  ‚è≥ Progress: 35/36 pages...\n",
            "  ‚úÖ Found 394 unique chapter numbers\n",
            "\n",
            "  üìä Selected chapters by group:\n",
            "    ‚Ä¢ Flame Comics: 204 chapters\n",
            "    ‚Ä¢ Mangakakalot: 99 chapters\n",
            "    ‚Ä¢ Asura: 91 chapters\n",
            "‚úÖ Total unique chapters selected: 394\n",
            "\n",
            "üì• Will download 394 chapters (starting from chapter 1)\n",
            "üì¶ Creating files with 100 chapters each\n",
            "\n",
            "üìã Sample chapters (first 5):\n",
            "  1. Chapter 0 - [Flame Comics] (üëç 0)\n",
            "  2. Chapter 0.5 - [Flame Comics] (üëç 0)\n",
            "  3. Chapter 1 - [Flame Comics] (üëç 0)\n",
            "  4. Chapter 2 - [Flame Comics] (üëç 0)\n",
            "  5. Chapter 3 - [Asura] (üëç 371)\n",
            "  ...\n",
            "  393. Chapter 291 - [Flame Comics] (üëç 23)\n",
            "  394. Chapter 292 - [Flame Comics] (üëç 10)\n",
            "\n",
            "üìä Total batches: 4\n",
            "\n",
            "======================================================================\n",
            "üì¶ BATCH 1/4\n",
            "üìñ Chapters: 0 to 97 (100 chapters)\n",
            "======================================================================\n",
            "\n",
            "[1/100] Chapter 0 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 72 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 72/72 images\n",
            "\n",
            "[2/100] Chapter 0.5 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 4 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 4/4 images\n",
            "\n",
            "[3/100] Chapter 1 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 37 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 37/37 images\n",
            "\n",
            "[4/100] Chapter 2 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 120 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 120/120 images\n",
            "\n",
            "[5/100] Chapter 3 [Asura] (üëç 371)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[6/100] Chapter 4 [Asura] (üëç 332)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[7/100] Chapter 5 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 110 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 110/110 images\n",
            "\n",
            "[8/100] Chapter 6 [Asura] (üëç 355)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[9/100] Chapter 7 [Asura] (üëç 356)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[10/100] Chapter 8 [Asura] (üëç 342)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[11/100] Chapter 9 [Asura] (üëç 335)\n",
            "  üì∑ Found 9 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 9/9 images\n",
            "\n",
            "[12/100] Chapter 10 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 34 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 34/34 images\n",
            "\n",
            "[13/100] Chapter 11 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 135 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 135/135 images\n",
            "\n",
            "[14/100] Chapter 12 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 118 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 118/118 images\n",
            "\n",
            "[15/100] Chapter 13 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 110 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 110/110 images\n",
            "\n",
            "[16/100] Chapter 14 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 131 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 131/131 images\n",
            "\n",
            "[17/100] Chapter 15 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 163 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 163/163 images\n",
            "\n",
            "[18/100] Chapter 16 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 30 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 30/30 images\n",
            "\n",
            "[19/100] Chapter 17 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 136 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 136/136 images\n",
            "\n",
            "[20/100] Chapter 18 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 26 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 26/26 images\n",
            "\n",
            "[21/100] Chapter 19 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 32 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 32/32 images\n",
            "\n",
            "[22/100] Chapter 20 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 45 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 45/45 images\n",
            "\n",
            "[23/100] Chapter 21 [Asura] (üëç 274)\n",
            "  üì∑ Found 32 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 32/32 images\n",
            "\n",
            "[24/100] Chapter 22 [Asura] (üëç 283)\n",
            "  üì∑ Found 52 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 52/52 images\n",
            "\n",
            "[25/100] Chapter 23 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 141 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 141/141 images\n",
            "\n",
            "[26/100] Chapter 24 [Asura] (üëç 272)\n",
            "  üì∑ Found 8 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 8/8 images\n",
            "\n",
            "[27/100] Chapter 25 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 139 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 139/139 images\n",
            "\n",
            "[28/100] Chapter 26 [Asura] (üëç 267)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[29/100] Chapter 27 [Asura] (üëç 231)\n",
            "  üì∑ Found 34 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 34/34 images\n",
            "\n",
            "[30/100] Chapter 28 [Asura] (üëç 264)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[31/100] Chapter 29 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 139 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 139/139 images\n",
            "\n",
            "[32/100] Chapter 30 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 165 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 165/165 images\n",
            "\n",
            "[33/100] Chapter 31 [Asura] (üëç 217)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[34/100] Chapter 32 [Asura] (üëç 192)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[35/100] Chapter 33 [Asura] (üëç 174)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[36/100] Chapter 34 [Asura] (üëç 195)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[37/100] Chapter 35 [Asura] (üëç 193)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[38/100] Chapter 36 [Asura] (üëç 185)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[39/100] Chapter 37 [Asura] (üëç 180)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[40/100] Chapter 38 [Asura] (üëç 163)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[41/100] Chapter 39 [Asura] (üëç 171)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[42/100] Chapter 40 [Asura] (üëç 197)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[43/100] Chapter 41 [Asura] (üëç 176)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[44/100] Chapter 42 [Asura] (üëç 218)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[45/100] Chapter 43 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 109 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 109/109 images\n",
            "\n",
            "[46/100] Chapter 44 [Asura] (üëç 176)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[47/100] Chapter 45 [Asura] (üëç 180)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[48/100] Chapter 46 [Asura] (üëç 191)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[49/100] Chapter 47 [Asura] (üëç 192)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[50/100] Chapter 48 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 33 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 33/33 images\n",
            "\n",
            "[51/100] Chapter 49 [Asura] (üëç 199)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[52/100] Chapter 50 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[53/100] Chapter 51 [Asura] (üëç 183)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[54/100] Chapter 52 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[55/100] Chapter 53 [Asura] (üëç 222)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[56/100] Chapter 54 [Asura] (üëç 202)\n",
            "  üì∑ Found 14 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 14/14 images\n",
            "\n",
            "[57/100] Chapter 55 [Asura] (üëç 229)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[58/100] Chapter 56 [Asura] (üëç 199)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[59/100] Chapter 57 [Asura] (üëç 207)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[60/100] Chapter 58 [Asura] (üëç 196)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[61/100] Chapter 59 [Asura] (üëç 194)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[62/100] Chapter 60 [Asura] (üëç 218)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[63/100] Chapter 61 [Asura] (üëç 197)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[64/100] Chapter 62 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 26 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 26/26 images\n",
            "\n",
            "[65/100] Chapter 63 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 31 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 31/31 images\n",
            "\n",
            "[66/100] Chapter 64 [Asura] (üëç 234)\n",
            "  üì∑ Found 14 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 14/14 images\n",
            "\n",
            "[67/100] Chapter 65 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 25 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 25/25 images\n",
            "\n",
            "[68/100] Chapter 66 [Asura] (üëç 307)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[69/100] Chapter 67 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 83 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 83/83 images\n",
            "\n",
            "[70/100] Chapter 68 [Asura] (üëç 223)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[71/100] Chapter 69 [Asura] (üëç 221)\n",
            "  üì∑ Found 14 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 14/14 images\n",
            "\n",
            "[72/100] Chapter 70 [Asura] (üëç 217)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[73/100] Chapter 71 [Asura] (üëç 231)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[74/100] Chapter 72 [Asura] (üëç 231)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[75/100] Chapter 73 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 93 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 93/93 images\n",
            "\n",
            "[76/100] Chapter 74 [Asura] (üëç 215)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[77/100] Chapter 75 [Asura] (üëç 208)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[78/100] Chapter 76 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 93 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 93/93 images\n",
            "\n",
            "[79/100] Chapter 77 [Asura] (üëç 209)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[80/100] Chapter 78 [Asura] (üëç 201)\n",
            "  üì∑ Found 14 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 14/14 images\n",
            "\n",
            "[81/100] Chapter 79 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 94 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 94/94 images\n",
            "\n",
            "[82/100] Chapter 80 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 170 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 170/170 images\n",
            "\n",
            "[83/100] Chapter 81 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 159 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 159/159 images\n",
            "\n",
            "[84/100] Chapter 82 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 136 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 136/136 images\n",
            "\n",
            "[85/100] Chapter 83 [Asura] (üëç 238)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[86/100] Chapter 84 [Asura] (üëç 238)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[87/100] Chapter 85 [Asura] (üëç 231)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[88/100] Chapter 86 [Asura] (üëç 300)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[89/100] Chapter 87 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 130 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 130/130 images\n",
            "\n",
            "[90/100] Chapter 88 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 114 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 114/114 images\n",
            "\n",
            "[91/100] Chapter 89 [Asura] (üëç 222)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[92/100] Chapter 90 [Asura] (üëç 232)\n",
            "  üì∑ Found 8 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 8/8 images\n",
            "\n",
            "[93/100] Chapter 91 [Asura] (üëç 241)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[94/100] Chapter 92 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 106 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 106/106 images\n",
            "\n",
            "[95/100] Chapter 92.1 [Mangakakalot] (üëç 2)\n",
            "  üì∑ Found 85 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 85/85 images\n",
            "\n",
            "[96/100] Chapter 93 [Asura] (üëç 206)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[97/100] Chapter 94 [Asura] (üëç 251)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[98/100] Chapter 95 [Asura] (üëç 239)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[99/100] Chapter 96 [Asura] (üëç 227)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[100/100] Chapter 97 [Asura] (üëç 219)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "‚úÖ Batch total: 4227 images\n",
            "\n",
            "üì¶ Creating CBZ for batch 1...\n",
            "  üìä Compressing 4227 images...\n",
            "  ‚è≥ Compressed 4227/4227 images...\n",
            "  ‚úÖ CBZ created: Omniscient Reader's Viewpoint Chapters 0-97.cbz (690.18 MB)\n",
            "\n",
            "üåê Uploading batch 1 to Gofile.io...\n",
            "  üì§ Uploading Omniscient Reader's Viewpoint Chapters 0-97.cbz (Attempt 1/3)...\n",
            "  ‚úÖ Gofile: https://gofile.io/d/ZrOVVB\n",
            "\n",
            "üßπ Cleaning up batch 1 temporary files...\n",
            "\n",
            "======================================================================\n",
            "üì¶ BATCH 2/4\n",
            "üìñ Chapters: 98 to 169 (100 chapters)\n",
            "======================================================================\n",
            "\n",
            "[1/100] Chapter 98 [Asura] (üëç 254)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[2/100] Chapter 99 [Asura] (üëç 223)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[3/100] Chapter 100 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[4/100] Chapter 101 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[5/100] Chapter 102 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[6/100] Chapter 103 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[7/100] Chapter 104 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[8/100] Chapter 105 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[9/100] Chapter 106 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[10/100] Chapter 107 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 17 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 17/17 images\n",
            "\n",
            "[11/100] Chapter 108 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[12/100] Chapter 109 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 17 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 17/17 images\n",
            "\n",
            "[13/100] Chapter 110 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 17 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 17/17 images\n",
            "\n",
            "[14/100] Chapter 111 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[15/100] Chapter 112 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[16/100] Chapter 113 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 17 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 17/17 images\n",
            "\n",
            "[17/100] Chapter 114 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[18/100] Chapter 115 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[19/100] Chapter 116 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[20/100] Chapter 117 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[21/100] Chapter 118 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[22/100] Chapter 119 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[23/100] Chapter 120 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[24/100] Chapter 121 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[25/100] Chapter 122 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[26/100] Chapter 123 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[27/100] Chapter 124 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[28/100] Chapter 125 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[29/100] Chapter 126 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[30/100] Chapter 127 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 25 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 25/25 images\n",
            "\n",
            "[31/100] Chapter 128 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 24 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 24/24 images\n",
            "\n",
            "[32/100] Chapter 129 [Flame Comics] (üëç 1)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[33/100] Chapter 130 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[34/100] Chapter 131 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[35/100] Chapter 132 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[36/100] Chapter 133 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[37/100] Chapter 134 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 24 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 24/24 images\n",
            "\n",
            "[38/100] Chapter 135 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[39/100] Chapter 136 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[40/100] Chapter 137 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[41/100] Chapter 138 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[42/100] Chapter 139 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[43/100] Chapter 140 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[44/100] Chapter 141 [Flame Comics] (üëç 1)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[45/100] Chapter 141.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 2 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 2/2 images\n",
            "\n",
            "[46/100] Chapter 142 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[47/100] Chapter 142.5 [Asura] (üëç 187)\n",
            "  üì∑ Found 1 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 1/1 images\n",
            "\n",
            "[48/100] Chapter 143 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 17 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 17/17 images\n",
            "\n",
            "[49/100] Chapter 143.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 2 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 2/2 images\n",
            "\n",
            "[50/100] Chapter 144 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[51/100] Chapter 144.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 74 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 74/74 images\n",
            "\n",
            "[52/100] Chapter 145 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[53/100] Chapter 145.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 88 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 88/88 images\n",
            "\n",
            "[54/100] Chapter 146 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[55/100] Chapter 146.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 98 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 98/98 images\n",
            "\n",
            "[56/100] Chapter 147 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[57/100] Chapter 147.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 74 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 74/74 images\n",
            "\n",
            "[58/100] Chapter 148 [Flame Comics] (üëç 1)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[59/100] Chapter 148.1 [Mangakakalot] (üëç 1)\n",
            "  üì∑ Found 91 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 91/91 images\n",
            "\n",
            "[60/100] Chapter 149 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[61/100] Chapter 149.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 97 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 97/97 images\n",
            "\n",
            "[62/100] Chapter 150 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[63/100] Chapter 150.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 91 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 91/91 images\n",
            "\n",
            "[64/100] Chapter 151 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[65/100] Chapter 151.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 87 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 87/87 images\n",
            "\n",
            "[66/100] Chapter 152 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[67/100] Chapter 152.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 90 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 90/90 images\n",
            "\n",
            "[68/100] Chapter 153 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[69/100] Chapter 153.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 96 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 96/96 images\n",
            "\n",
            "[70/100] Chapter 154 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 24 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 24/24 images\n",
            "\n",
            "[71/100] Chapter 154.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 102 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 102/102 images\n",
            "\n",
            "[72/100] Chapter 155 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[73/100] Chapter 155.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 94 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 94/94 images\n",
            "\n",
            "[74/100] Chapter 156 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 24 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 24/24 images\n",
            "\n",
            "[75/100] Chapter 156.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 109 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 109/109 images\n",
            "\n",
            "[76/100] Chapter 157 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 24 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 24/24 images\n",
            "\n",
            "[77/100] Chapter 157.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 103 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 103/103 images\n",
            "\n",
            "[78/100] Chapter 158 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 17 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 17/17 images\n",
            "\n",
            "[79/100] Chapter 158.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 81 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 81/81 images\n",
            "\n",
            "[80/100] Chapter 159 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[81/100] Chapter 159.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 90 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 90/90 images\n",
            "\n",
            "[82/100] Chapter 160 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[83/100] Chapter 160.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 83 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 83/83 images\n",
            "\n",
            "[84/100] Chapter 161 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 25 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 25/25 images\n",
            "\n",
            "[85/100] Chapter 161.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 108 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 108/108 images\n",
            "\n",
            "[86/100] Chapter 162 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 27 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 27/27 images\n",
            "\n",
            "[87/100] Chapter 162.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 122 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 122/122 images\n",
            "\n",
            "[88/100] Chapter 163 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[89/100] Chapter 163.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 96 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 96/96 images\n",
            "\n",
            "[90/100] Chapter 164 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[91/100] Chapter 164.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 105 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 105/105 images\n",
            "\n",
            "[92/100] Chapter 165 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[93/100] Chapter 165.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 103 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 103/103 images\n",
            "\n",
            "[94/100] Chapter 166 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[95/100] Chapter 166.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 91 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 91/91 images\n",
            "\n",
            "[96/100] Chapter 167 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[97/100] Chapter 167.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 82 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 82/82 images\n",
            "\n",
            "[98/100] Chapter 168 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[99/100] Chapter 168.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 93 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 93/93 images\n",
            "\n",
            "[100/100] Chapter 169 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "‚úÖ Batch total: 3834 images\n",
            "\n",
            "üì¶ Creating CBZ for batch 2...\n",
            "  üìä Compressing 3834 images...\n",
            "  ‚è≥ Compressed 3834/3834 images...\n",
            "  ‚úÖ CBZ created: Omniscient Reader's Viewpoint Chapters 98-169.cbz (589.99 MB)\n",
            "\n",
            "üåê Uploading batch 2 to Gofile.io...\n",
            "  üì§ Uploading Omniscient Reader's Viewpoint Chapters 98-169.cbz (Attempt 1/3)...\n",
            "  ‚úÖ Gofile: https://gofile.io/d/vFHvrL\n",
            "\n",
            "üßπ Cleaning up batch 2 temporary files...\n",
            "\n",
            "======================================================================\n",
            "üì¶ BATCH 3/4\n",
            "üìñ Chapters: 169.1 to 231 (100 chapters)\n",
            "======================================================================\n",
            "\n",
            "[1/100] Chapter 169.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 102 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 102/102 images\n",
            "\n",
            "[2/100] Chapter 170 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[3/100] Chapter 170.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 84 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 84/84 images\n",
            "\n",
            "[4/100] Chapter 171 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[5/100] Chapter 171.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 97 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 97/97 images\n",
            "\n",
            "[6/100] Chapter 172 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 25 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 25/25 images\n",
            "\n",
            "[7/100] Chapter 172.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 110 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 110/110 images\n",
            "\n",
            "[8/100] Chapter 173 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[9/100] Chapter 173.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 117 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 117/117 images\n",
            "\n",
            "[10/100] Chapter 174 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 24 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 24/24 images\n",
            "\n",
            "[11/100] Chapter 174.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 104 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 104/104 images\n",
            "\n",
            "[12/100] Chapter 175 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[13/100] Chapter 175.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 108 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 108/108 images\n",
            "\n",
            "[14/100] Chapter 176 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 25 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 25/25 images\n",
            "\n",
            "[15/100] Chapter 176.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 111 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 111/111 images\n",
            "\n",
            "[16/100] Chapter 177 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 29 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 29/29 images\n",
            "\n",
            "[17/100] Chapter 177.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 124 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 124/124 images\n",
            "\n",
            "[18/100] Chapter 178 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 25 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 25/25 images\n",
            "\n",
            "[19/100] Chapter 178.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 113 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 113/113 images\n",
            "\n",
            "[20/100] Chapter 179 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 29 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 29/29 images\n",
            "\n",
            "[21/100] Chapter 179.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 130 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 130/130 images\n",
            "\n",
            "[22/100] Chapter 180 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[23/100] Chapter 180.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 108 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 108/108 images\n",
            "\n",
            "[24/100] Chapter 181 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[25/100] Chapter 181.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 92 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 92/92 images\n",
            "\n",
            "[26/100] Chapter 182 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 24 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 24/24 images\n",
            "\n",
            "[27/100] Chapter 182.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 122 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 122/122 images\n",
            "\n",
            "[28/100] Chapter 183 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[29/100] Chapter 183.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 91 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 91/91 images\n",
            "\n",
            "[30/100] Chapter 184 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[31/100] Chapter 184.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 95 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 95/95 images\n",
            "\n",
            "[32/100] Chapter 185 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[33/100] Chapter 185.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 93 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 93/93 images\n",
            "\n",
            "[34/100] Chapter 186 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[35/100] Chapter 186.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 79 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 79/79 images\n",
            "\n",
            "[36/100] Chapter 187 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[37/100] Chapter 187.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 109 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 109/109 images\n",
            "\n",
            "[38/100] Chapter 188 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[39/100] Chapter 188.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 94 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 94/94 images\n",
            "\n",
            "[40/100] Chapter 189 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[41/100] Chapter 189.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 97 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 97/97 images\n",
            "\n",
            "[42/100] Chapter 190 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[43/100] Chapter 190.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 101 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 101/101 images\n",
            "\n",
            "[44/100] Chapter 191 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[45/100] Chapter 191.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 108 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 108/108 images\n",
            "\n",
            "[46/100] Chapter 192 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[47/100] Chapter 192.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 93 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 93/93 images\n",
            "\n",
            "[48/100] Chapter 193 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[49/100] Chapter 193.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 98 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 98/98 images\n",
            "\n",
            "[50/100] Chapter 194 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 24 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 24/24 images\n",
            "\n",
            "[51/100] Chapter 194.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 140 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 140/140 images\n",
            "\n",
            "[52/100] Chapter 195 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[53/100] Chapter 195.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 87 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 87/87 images\n",
            "\n",
            "[54/100] Chapter 196 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[55/100] Chapter 197 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[56/100] Chapter 198 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[57/100] Chapter 198.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 108 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 108/108 images\n",
            "\n",
            "[58/100] Chapter 199 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[59/100] Chapter 200 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[60/100] Chapter 200.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 103 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 103/103 images\n",
            "\n",
            "[61/100] Chapter 201 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[62/100] Chapter 201.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 112 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 112/112 images\n",
            "\n",
            "[63/100] Chapter 202 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[64/100] Chapter 202.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 103 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 103/103 images\n",
            "\n",
            "[65/100] Chapter 203 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[66/100] Chapter 203.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 105 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 105/105 images\n",
            "\n",
            "[67/100] Chapter 204 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[68/100] Chapter 205 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[69/100] Chapter 206 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 16 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 16/16 images\n",
            "\n",
            "[70/100] Chapter 207 [Asura] (üëç 1286)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[71/100] Chapter 208 [Asura] (üëç 1665)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[72/100] Chapter 209 [Asura] (üëç 1356)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[73/100] Chapter 209.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 86 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 86/86 images\n",
            "\n",
            "[74/100] Chapter 210 [Asura] (üëç 1255)\n",
            "  üì∑ Found 9 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 9/9 images\n",
            "\n",
            "[75/100] Chapter 210.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 79 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 79/79 images\n",
            "\n",
            "[76/100] Chapter 211 [Asura] (üëç 1934)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[77/100] Chapter 211.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 84 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 84/84 images\n",
            "\n",
            "[78/100] Chapter 212 [Asura] (üëç 1295)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[79/100] Chapter 213 [Asura] (üëç 1214)\n",
            "  üì∑ Found 9 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 9/9 images\n",
            "\n",
            "[80/100] Chapter 214 [Asura] (üëç 1266)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[81/100] Chapter 215 [Asura] (üëç 1332)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[82/100] Chapter 216 [Asura] (üëç 1430)\n",
            "  üì∑ Found 8 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 8/8 images\n",
            "\n",
            "[83/100] Chapter 217 [Asura] (üëç 1642)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[84/100] Chapter 218 [Asura] (üëç 1426)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[85/100] Chapter 219 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 17 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 17/17 images\n",
            "\n",
            "[86/100] Chapter 219.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[87/100] Chapter 220 [Asura] (üëç 1997)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[88/100] Chapter 221 [Asura] (üëç 1351)\n",
            "  üì∑ Found 9 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 9/9 images\n",
            "\n",
            "[89/100] Chapter 221.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 4 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 4/4 images\n",
            "\n",
            "[90/100] Chapter 222 [Flame Comics] (üëç 1)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[91/100] Chapter 223 [Asura] (üëç 1389)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[92/100] Chapter 224 [Asura] (üëç 1609)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[93/100] Chapter 225 [Asura] (üëç 1186)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[94/100] Chapter 226 [Asura] (üëç 1317)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[95/100] Chapter 227 [Asura] (üëç 1300)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[96/100] Chapter 228 [Asura] (üëç 1192)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[97/100] Chapter 229 [Asura] (üëç 1369)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[98/100] Chapter 230 [Asura] (üëç 1212)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[99/100] Chapter 230.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 99 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 99/99 images\n",
            "\n",
            "[100/100] Chapter 231 [Asura] (üëç 1134)\n",
            "  üì∑ Found 17 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 17/17 images\n",
            "\n",
            "‚úÖ Batch total: 4763 images\n",
            "\n",
            "üì¶ Creating CBZ for batch 3...\n",
            "  üìä Compressing 4763 images...\n",
            "  ‚è≥ Compressed 4763/4763 images...\n",
            "  ‚úÖ CBZ created: Omniscient Reader's Viewpoint Chapters 169.1-231.cbz (577.00 MB)\n",
            "\n",
            "üåê Uploading batch 3 to Gofile.io...\n",
            "  üì§ Uploading Omniscient Reader's Viewpoint Chapters 169.1-231.cbz (Attempt 1/3)...\n",
            "  ‚úÖ Gofile: https://gofile.io/d/xaVDLe\n",
            "\n",
            "üßπ Cleaning up batch 3 temporary files...\n",
            "\n",
            "======================================================================\n",
            "üì¶ BATCH 4/4\n",
            "üìñ Chapters: 231.1 to 292 (94 chapters)\n",
            "======================================================================\n",
            "\n",
            "[1/94] Chapter 231.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 119 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 119/119 images\n",
            "\n",
            "[2/94] Chapter 232 [Asura] (üëç 1052)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[3/94] Chapter 233 [Flame Comics] (üëç 1240)\n",
            "  üì∑ Found 56 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 56/56 images\n",
            "\n",
            "[4/94] Chapter 233.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 84 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 84/84 images\n",
            "\n",
            "[5/94] Chapter 234 [Flame Comics] (üëç 0)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[6/94] Chapter 234.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 90 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 90/90 images\n",
            "\n",
            "[7/94] Chapter 235 [Asura] (üëç 1078)\n",
            "  üì∑ Found 11 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 11/11 images\n",
            "\n",
            "[8/94] Chapter 235.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 93 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 93/93 images\n",
            "\n",
            "[9/94] Chapter 236 [Flame Comics] (üëç 302)\n",
            "  üì∑ Found 57 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 57/57 images\n",
            "\n",
            "[10/94] Chapter 236.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 92 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 92/92 images\n",
            "\n",
            "[11/94] Chapter 237 [Asura] (üëç 1412)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[12/94] Chapter 237.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 96 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 96/96 images\n",
            "\n",
            "[13/94] Chapter 238 [Flame Comics] (üëç 1004)\n",
            "  üì∑ Found 62 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 62/62 images\n",
            "\n",
            "[14/94] Chapter 238.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 94 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 94/94 images\n",
            "\n",
            "[15/94] Chapter 239 [Flame Comics] (üëç 1048)\n",
            "  üì∑ Found 54 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 54/54 images\n",
            "\n",
            "[16/94] Chapter 239.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 95 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 95/95 images\n",
            "\n",
            "[17/94] Chapter 240 [Flame Comics] (üëç 252)\n",
            "  üì∑ Found 14 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 14/14 images\n",
            "\n",
            "[18/94] Chapter 240.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 79 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 79/79 images\n",
            "\n",
            "[19/94] Chapter 241 [Flame Comics] (üëç 1118)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[20/94] Chapter 241.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 80 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 80/80 images\n",
            "\n",
            "[21/94] Chapter 242 [Flame Comics] (üëç 1125)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[22/94] Chapter 242.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 93 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 93/93 images\n",
            "\n",
            "[23/94] Chapter 243 [Flame Comics] (üëç 1326)\n",
            "  üì∑ Found 14 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 14/14 images\n",
            "\n",
            "[24/94] Chapter 243.1 [Mangakakalot] (üëç 2)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[25/94] Chapter 244 [Flame Comics] (üëç 370)\n",
            "  üì∑ Found 71 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 71/71 images\n",
            "\n",
            "[26/94] Chapter 244.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[27/94] Chapter 245 [Flame Comics] (üëç 985)\n",
            "  üì∑ Found 14 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 14/14 images\n",
            "\n",
            "[28/94] Chapter 245.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[29/94] Chapter 246 [Flame Comics] (üëç 1024)\n",
            "  üì∑ Found 63 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 63/63 images\n",
            "\n",
            "[30/94] Chapter 246.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[31/94] Chapter 247 [Flame Comics] (üëç 1213)\n",
            "  üì∑ Found 74 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 74/74 images\n",
            "\n",
            "[32/94] Chapter 247.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[33/94] Chapter 248 [Flame Comics] (üëç 1100)\n",
            "  üì∑ Found 63 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 63/63 images\n",
            "\n",
            "[34/94] Chapter 248.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[35/94] Chapter 249 [Flame Comics] (üëç 1307)\n",
            "  üì∑ Found 69 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 69/69 images\n",
            "\n",
            "[36/94] Chapter 249.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[37/94] Chapter 250 [Flame Comics] (üëç 1394)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[38/94] Chapter 250.1 [Mangakakalot] (üëç 1)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[39/94] Chapter 251 [Flame Comics] (üëç 1880)\n",
            "  üì∑ Found 60 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 60/60 images\n",
            "\n",
            "[40/94] Chapter 251.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[41/94] Chapter 252 [Flame Comics] (üëç 1701)\n",
            "  üì∑ Found 56 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 56/56 images\n",
            "\n",
            "[42/94] Chapter 252.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 90 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 90/90 images\n",
            "\n",
            "[43/94] Chapter 253 [Flame Comics] (üëç 1533)\n",
            "  üì∑ Found 48 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 48/48 images\n",
            "\n",
            "[44/94] Chapter 253.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 83 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 83/83 images\n",
            "\n",
            "[45/94] Chapter 254 [Flame Comics] (üëç 1489)\n",
            "  üì∑ Found 51 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 51/51 images\n",
            "\n",
            "[46/94] Chapter 254.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 77 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 77/77 images\n",
            "\n",
            "[47/94] Chapter 255 [Flame Comics] (üëç 1458)\n",
            "  üì∑ Found 56 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 56/56 images\n",
            "\n",
            "[48/94] Chapter 255.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 85 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 85/85 images\n",
            "\n",
            "[49/94] Chapter 256 [Flame Comics] (üëç 1701)\n",
            "  üì∑ Found 14 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 14/14 images\n",
            "\n",
            "[50/94] Chapter 256.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 75 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 75/75 images\n",
            "\n",
            "[51/94] Chapter 257 [Flame Comics] (üëç 1647)\n",
            "  üì∑ Found 63 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 63/63 images\n",
            "\n",
            "[52/94] Chapter 257.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 113 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 113/113 images\n",
            "\n",
            "[53/94] Chapter 258 [Flame Comics] (üëç 1592)\n",
            "  üì∑ Found 56 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 56/56 images\n",
            "\n",
            "[54/94] Chapter 258.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 80 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 80/80 images\n",
            "\n",
            "[55/94] Chapter 259 [Flame Comics] (üëç 732)\n",
            "  üì∑ Found 44 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 44/44 images\n",
            "\n",
            "[56/94] Chapter 259.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 70 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 70/70 images\n",
            "\n",
            "[57/94] Chapter 260 [Flame Comics] (üëç 1700)\n",
            "  üì∑ Found 74 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 74/74 images\n",
            "\n",
            "[58/94] Chapter 260.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 105 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 105/105 images\n",
            "\n",
            "[59/94] Chapter 261 [Flame Comics] (üëç 1609)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[60/94] Chapter 261.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 107 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 107/107 images\n",
            "\n",
            "[61/94] Chapter 262 [Flame Comics] (üëç 1213)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[62/94] Chapter 262.1 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 93 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 93/93 images\n",
            "\n",
            "[63/94] Chapter 263 [Flame Comics] (üëç 1579)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[64/94] Chapter 264 [Flame Comics] (üëç 1639)\n",
            "  üì∑ Found 10 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 10/10 images\n",
            "\n",
            "[65/94] Chapter 264.1 [Mangakakalot] (üëç 1)\n",
            "  üì∑ Found 7 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 7/7 images\n",
            "\n",
            "[66/94] Chapter 264.2 [Mangakakalot] (üëç 0)\n",
            "  üì∑ Found 4 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 4/4 images\n",
            "\n",
            "[67/94] Chapter 265 [Flame Comics] (üëç 1601)\n",
            "  üì∑ Found 14 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 14/14 images\n",
            "\n",
            "[68/94] Chapter 266 [Flame Comics] (üëç 1581)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "[69/94] Chapter 267 [Flame Comics] (üëç 1520)\n",
            "  üì∑ Found 12 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 12/12 images\n",
            "\n",
            "[70/94] Chapter 268 [Flame Comics] (üëç 652)\n",
            "  üì∑ Found 17 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 17/17 images\n",
            "\n",
            "[71/94] Chapter 269 [Flame Comics] (üëç 1908)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[72/94] Chapter 270 [Flame Comics] (üëç 2292)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[73/94] Chapter 271 [Flame Comics] (üëç 1814)\n",
            "  üì∑ Found 16 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 16/16 images\n",
            "\n",
            "[74/94] Chapter 272 [Flame Comics] (üëç 1766)\n",
            "  üì∑ Found 14 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 14/14 images\n",
            "\n",
            "[75/94] Chapter 273 [Flame Comics] (üëç 1625)\n",
            "  üì∑ Found 26 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 26/26 images\n",
            "\n",
            "[76/94] Chapter 274 [Flame Comics] (üëç 1663)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[77/94] Chapter 275 [Flame Comics] (üëç 58)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[78/94] Chapter 276 [Flame Comics] (üëç 145)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[79/94] Chapter 277 [Flame Comics] (üëç 20)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[80/94] Chapter 278 [Flame Comics] (üëç 10)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[81/94] Chapter 279 [Flame Comics] (üëç 10)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[82/94] Chapter 280 [Flame Comics] (üëç 19)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[83/94] Chapter 281 [Flame Comics] (üëç 20)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[84/94] Chapter 282 [Flame Comics] (üëç 34)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[85/94] Chapter 283 [Flame Comics] (üëç 40)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[86/94] Chapter 284 [Flame Comics] (üëç 42)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[87/94] Chapter 285 [Flame Comics] (üëç 46)\n",
            "  üì∑ Found 22 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 22/22 images\n",
            "\n",
            "[88/94] Chapter 286 [Flame Comics] (üëç 48)\n",
            "  üì∑ Found 19 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 19/19 images\n",
            "\n",
            "[89/94] Chapter 287 [Flame Comics] (üëç 35)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[90/94] Chapter 288 [Flame Comics] (üëç 32)\n",
            "  üì∑ Found 23 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 23/23 images\n",
            "\n",
            "[91/94] Chapter 289 [Flame Comics] (üëç 40)\n",
            "  üì∑ Found 18 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 18/18 images\n",
            "\n",
            "[92/94] Chapter 290 [Flame Comics] (üëç 31)\n",
            "  üì∑ Found 21 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 21/21 images\n",
            "\n",
            "[93/94] Chapter 291 [Flame Comics] (üëç 23)\n",
            "  üì∑ Found 20 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 20/20 images\n",
            "\n",
            "[94/94] Chapter 292 [Flame Comics] (üëç 10)\n",
            "  üì∑ Found 13 images - downloading in parallel...\n",
            "  ‚úÖ Downloaded 13/13 images\n",
            "\n",
            "‚úÖ Batch total: 3885 images\n",
            "\n",
            "üì¶ Creating CBZ for batch 4...\n",
            "  üìä Compressing 3885 images...\n",
            "  ‚è≥ Compressed 3885/3885 images...\n",
            "  ‚úÖ CBZ created: Omniscient Reader's Viewpoint Chapters 231.1-292.cbz (502.85 MB)\n",
            "\n",
            "üåê Uploading batch 4 to Gofile.io...\n",
            "  üì§ Uploading Omniscient Reader's Viewpoint Chapters 231.1-292.cbz (Attempt 1/3)...\n",
            "  ‚úÖ Gofile: https://gofile.io/d/G0XE1b\n",
            "\n",
            "üßπ Cleaning up batch 4 temporary files...\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL BATCHES COMPLETED!\n",
            "======================================================================\n",
            "\n",
            "üìä Summary:\n",
            "  ‚Ä¢ Total batches: 4\n",
            "  ‚Ä¢ Chapters processed: 394\n",
            "  ‚Ä¢ Files created: 4\n",
            "\n",
            "üîó UPLOAD LINKS:\n",
            "======================================================================\n",
            "\n",
            "üì¶ Batch 1:\n",
            "  ‚Ä¢ CBZ (Gofile): https://gofile.io/d/ZrOVVB\n",
            "\n",
            "üì¶ Batch 2:\n",
            "  ‚Ä¢ CBZ (Gofile): https://gofile.io/d/vFHvrL\n",
            "\n",
            "üì¶ Batch 3:\n",
            "  ‚Ä¢ CBZ (Gofile): https://gofile.io/d/xaVDLe\n",
            "\n",
            "üì¶ Batch 4:\n",
            "  ‚Ä¢ CBZ (Gofile): https://gofile.io/d/G0XE1b\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üßπ Final cleanup...\n",
            "\n",
            "‚ú® Done! Enjoy your manhwa collection!\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}