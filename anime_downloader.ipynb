{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTs35/mIKE9GOxPYebgW25",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cinichi/Ani-Downloader/blob/main/anime_downloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxDjh24kTlGl",
        "outputId": "f2458720-21d6-450b-dda8-a14faf7215bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing required packages...\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… All dependencies installed!\n",
            "\n",
            "âœ… Configuration loaded!\n",
            "ğŸ“¥ Download method: yt-dlp\n",
            "âš™ï¸ Max workers: 8\n",
            "ğŸ”„ Max retries: 5\n",
            "\n",
            "======================================================================\n",
            "ğŸ¬ ANIMEKAI EPISODE DOWNLOADER\n",
            "======================================================================\n",
            "\n",
            "ğŸ” Processing: https://animekai.to/watch/jujutsu-kaisen-4gm6\n",
            "âœ… Anime ID: e4u-9g\n",
            "ğŸ“º Title: Unknown\n",
            "ğŸ“‹ Found 24 episode(s)\n",
            "ğŸ“¥ Will download 3 episode(s)\n",
            "\n",
            "ğŸ“ Download directory: downloads/Unknown\n",
            "======================================================================\n",
            "\n",
            "[1/3] Episode 1.0\n",
            "   ğŸ¥ Server: Server 1 (softsub)\n",
            "\n",
            "ğŸ“¥ Downloading Episode 1.0 with yt-dlp...\n",
            "   File: Episode_001.mp4\n",
            "   â³ Progress: 99.5%\n",
            "   âœ… Complete! (255.96 MB)\n",
            "\n",
            "[2/3] Episode 2.0\n",
            "   ğŸ¥ Server: Server 1 (softsub)\n",
            "\n",
            "ğŸ“¥ Downloading Episode 2.0 with yt-dlp...\n",
            "   File: Episode_002.mp4\n",
            "   â³ Progress: 100.0%\n",
            "   âœ… Complete! (239.18 MB)\n",
            "\n",
            "[3/3] Episode 3.0\n",
            "   ğŸ¥ Server: Server 1 (softsub)\n",
            "\n",
            "ğŸ“¥ Downloading Episode 3.0 with yt-dlp...\n",
            "   File: Episode_003.mp4\n",
            "   â³ Progress: 99.9%\n",
            "   âœ… Complete! (226.75 MB)\n",
            "\n",
            "======================================================================\n",
            "ğŸ“Š DOWNLOAD SUMMARY\n",
            "======================================================================\n",
            "\n",
            "âœ… Successfully downloaded: 3 episode(s)\n",
            "\n",
            "ğŸ’¾ Total size: 721.89 MB\n",
            "ğŸ“ Location: downloads/Unknown\n",
            "\n",
            "======================================================================\n",
            "ğŸ‰ DOWNLOAD COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ğŸ¬ AnimeKai Episode Downloader & Merger\n",
        "# Enhanced with chunk downloads, yt-dlp support, and better error handling\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ”§ STEP 1: INSTALL DEPENDENCIES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"ğŸ“¦ Installing required packages...\")\n",
        "!pip install -q requests beautifulsoup4 cloudscraper m3u8 pycryptodome tqdm yt-dlp\n",
        "!apt-get -qq install -y ffmpeg aria2 > /dev/null 2>&1\n",
        "print(\"âœ… All dependencies installed!\\n\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# âš™ï¸ STEP 2: CONFIGURATION\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# ğŸ”— Anime URL\n",
        "ANIME_URL = \"https://animekai.to/watch/jujutsu-kaisen-4gm6\" # @param {type:\"string\"}\n",
        "\n",
        "# ğŸ“º Episode Selection\n",
        "DOWNLOAD_MODE = \"Episode Range\"  # @param [\"All Episodes\", \"Episode Range\", \"Single Episode\"]\n",
        "SINGLE_EPISODE = 1 # @param {type:\"integer\"}\n",
        "START_EPISODE = 1 # @param {type:\"integer\"}\n",
        "END_EPISODE = 3 # @param {type:\"integer\"}\n",
        "\n",
        "# ğŸ¥ Quality & Audio Settings\n",
        "VIDEO_QUALITY = \"1080p\"         # @param [\"1080p\", \"720p\", \"480p\", \"360p\"]\n",
        "PREFER_TYPE = \"Soft Sub\"        # @param [\"Hard Sub\", \"Soft Sub\", \"Dub & S-Sub\"]\n",
        "PREFER_SERVER = \"Server 1\"      # @param [\"Server 1\", \"Server 2\"]\n",
        "\n",
        "# ğŸ“¥ Download Settings\n",
        "DOWNLOAD_METHOD = \"yt-dlp\"      # @param [\"yt-dlp\", \"aria2\", \"chunks\", \"ffmpeg\"]\n",
        "CHUNK_SIZE_MB = 5               # @param {type:\"integer\"} # Chunk size in MB for chunked downloads\n",
        "MAX_WORKERS = 8                 # @param {type:\"integer\"} # Parallel download threads\n",
        "MAX_RETRIES = 5                 # @param {type:\"integer\"} # Retry attempts\n",
        "TIMEOUT = 300                   # @param {type:\"integer\"} # Timeout in seconds (5 minutes)\n",
        "MERGE_EPISODES = False          # @param {type:\"boolean\"} # Merge all episodes into one file\n",
        "\n",
        "# ğŸ“¤ Upload Settings\n",
        "UPLOAD_TO = \"None (Keep Local)\" # @param [\"Google Drive Only\", \"GoFile.io Only\", \"Both\", \"None (Keep Local)\"]\n",
        "\n",
        "print(\"âœ… Configuration loaded!\")\n",
        "print(f\"ğŸ“¥ Download method: {DOWNLOAD_METHOD}\")\n",
        "print(f\"âš™ï¸ Max workers: {MAX_WORKERS}\")\n",
        "print(f\"ğŸ”„ Max retries: {MAX_RETRIES}\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸŒ STEP 3: CORE FUNCTIONS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "import threading\n",
        "from bs4 import BeautifulSoup\n",
        "import cloudscraper\n",
        "from urllib.parse import urljoin, urlparse, quote, unquote\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import signal\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import hashlib\n",
        "\n",
        "# Create cloudscraper session\n",
        "scraper = cloudscraper.create_scraper(\n",
        "    browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}\n",
        ")\n",
        "\n",
        "BASE_URL = \"https://animekai.to\"\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "    'Referer': BASE_URL,\n",
        "    'Accept': '*/*',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "    'Connection': 'keep-alive'\n",
        "}\n",
        "\n",
        "def enc_dec_request(endpoint, text):\n",
        "    \"\"\"Make request to enc-dec API\"\"\"\n",
        "    try:\n",
        "        url = f\"https://enc-dec.app/api/{endpoint}?text={text}\"\n",
        "        response = scraper.get(url, headers=headers, timeout=30)\n",
        "        data = response.json()\n",
        "        return data.get('result', '')\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Enc-dec error: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_anime_details(url):\n",
        "    \"\"\"Get anime ID and title\"\"\"\n",
        "    try:\n",
        "        response = scraper.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        anime_div = soup.select_one('div[data-id]')\n",
        "        anime_id = anime_div.get('data-id') if anime_div else None\n",
        "\n",
        "        title_elem = soup.select_one('div.title-wrapper h1.title span')\n",
        "        title = title_elem.get('title', '') if title_elem else \"Unknown\"\n",
        "        title = re.sub(r'[<>:\"/\\\\|?*]', '', title)\n",
        "\n",
        "        return anime_id, title\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error getting anime details: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def get_episode_list(anime_id):\n",
        "    \"\"\"Get list of all episodes\"\"\"\n",
        "    try:\n",
        "        enc = enc_dec_request('enc-kai', anime_id)\n",
        "        if not enc:\n",
        "            return []\n",
        "\n",
        "        ep_url = f\"{BASE_URL}/ajax/episodes/list?ani_id={anime_id}&_={enc}\"\n",
        "        response = scraper.get(ep_url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        if 'result' not in data:\n",
        "            return []\n",
        "\n",
        "        html = data['result']\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        episodes = []\n",
        "        for ep in soup.select('div.eplist a'):\n",
        "            token = ep.get('token', '')\n",
        "            ep_num = ep.get('num', '0')\n",
        "            langs = ep.get('langs', '0')\n",
        "\n",
        "            langs_int = int(langs) if langs.isdigit() else 0\n",
        "            if langs_int == 1:\n",
        "                subdub = \"Sub\"\n",
        "            elif langs_int == 3:\n",
        "                subdub = \"Dub & Sub\"\n",
        "            else:\n",
        "                subdub = \"\"\n",
        "\n",
        "            episodes.append({\n",
        "                'number': float(ep_num),\n",
        "                'token': token,\n",
        "                'subdub': subdub,\n",
        "                'title': f\"Episode {ep_num}\"\n",
        "            })\n",
        "\n",
        "        return sorted(episodes, key=lambda x: x['number'])\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error getting episodes: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_video_servers(token):\n",
        "    \"\"\"Get available video servers\"\"\"\n",
        "    try:\n",
        "        enc = enc_dec_request('enc-kai', token)\n",
        "        if not enc:\n",
        "            return []\n",
        "\n",
        "        url = f\"{BASE_URL}/ajax/links/list?token={token}&_={enc}\"\n",
        "        response = scraper.get(url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        if 'result' not in data:\n",
        "            return []\n",
        "\n",
        "        html = data['result']\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "        servers = []\n",
        "        for type_div in soup.select('div.server-items[data-id]'):\n",
        "            type_id = type_div.get('data-id', '')\n",
        "\n",
        "            for server in type_div.select('span.server[data-lid]'):\n",
        "                server_id = server.get('data-lid', '')\n",
        "                server_name = server.text.strip()\n",
        "\n",
        "                servers.append({\n",
        "                    'type': type_id,\n",
        "                    'server_id': server_id,\n",
        "                    'server_name': server_name\n",
        "                })\n",
        "\n",
        "        return servers\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error getting servers: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_video_url(server_id, server_name):\n",
        "    \"\"\"Get direct video URL\"\"\"\n",
        "    try:\n",
        "        enc = enc_dec_request('enc-kai', server_id)\n",
        "        if not enc:\n",
        "            return None\n",
        "\n",
        "        url = f\"{BASE_URL}/ajax/links/view?id={server_id}&_={enc}\"\n",
        "        response = scraper.get(url, headers=headers)\n",
        "        data = response.json()\n",
        "\n",
        "        encoded_link = data.get('result', '')\n",
        "        if not encoded_link:\n",
        "            return None\n",
        "\n",
        "        dec_body = json.dumps({\"text\": encoded_link})\n",
        "        dec_response = scraper.post(\n",
        "            \"https://enc-dec.app/api/dec-kai\",\n",
        "            data=dec_body,\n",
        "            headers={'Content-Type': 'application/json'}\n",
        "        )\n",
        "        dec_data = dec_response.json()\n",
        "        iframe_url = dec_data.get('result', {}).get('url', '')\n",
        "\n",
        "        if not iframe_url:\n",
        "            return None\n",
        "\n",
        "        return extract_megaup_url(iframe_url)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error getting video URL: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_megaup_url(iframe_url):\n",
        "    \"\"\"Extract video URL from MegaUp\"\"\"\n",
        "    try:\n",
        "        parsed = urlparse(iframe_url)\n",
        "        token = parsed.path.split('/')[-1]\n",
        "\n",
        "        media_url = f\"{parsed.scheme}://{parsed.netloc}/media/{token}\"\n",
        "        response = scraper.get(media_url, headers=headers)\n",
        "        data = response.json()\n",
        "        mega_token = data.get('result', '')\n",
        "\n",
        "        if not mega_token:\n",
        "            return None\n",
        "\n",
        "        dec_body = json.dumps({\"text\": mega_token, \"agent\": headers['User-Agent']})\n",
        "        dec_response = scraper.post(\n",
        "            \"https://enc-dec.app/api/dec-mega\",\n",
        "            data=dec_body,\n",
        "            headers={'Content-Type': 'application/json'}\n",
        "        )\n",
        "\n",
        "        mega_data = dec_response.json()\n",
        "        sources = mega_data.get('result', {}).get('sources', [])\n",
        "\n",
        "        if not sources:\n",
        "            return None\n",
        "\n",
        "        return sources[0].get('file', '')\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ MegaUp extraction error: {e}\")\n",
        "        return None\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“¥ DOWNLOAD METHODS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def download_with_ytdlp(url, output_file, episode_num):\n",
        "    \"\"\"Download using yt-dlp (BEST for m3u8)\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nğŸ“¥ Downloading Episode {episode_num} with yt-dlp...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        cmd = [\n",
        "            'yt-dlp',\n",
        "            url,\n",
        "            '-o', output_file,\n",
        "            '--no-warnings',\n",
        "            '--no-check-certificate',\n",
        "            '--concurrent-fragments', str(MAX_WORKERS),\n",
        "            '--retries', str(MAX_RETRIES),\n",
        "            '--fragment-retries', str(MAX_RETRIES),\n",
        "            '--socket-timeout', str(TIMEOUT),\n",
        "            '--progress',\n",
        "            '--newline',\n",
        "            '--user-agent', headers['User-Agent'],\n",
        "            '--referer', BASE_URL\n",
        "        ]\n",
        "\n",
        "        process = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            universal_newlines=True\n",
        "        )\n",
        "\n",
        "        # Parse progress\n",
        "        for line in process.stdout:\n",
        "            if '[download]' in line and '%' in line:\n",
        "                # Extract percentage\n",
        "                match = re.search(r'(\\d+\\.\\d+)%', line)\n",
        "                if match:\n",
        "                    percent = float(match.group(1))\n",
        "                    print(f\"\\r   â³ Progress: {percent:.1f}%\", end='', flush=True)\n",
        "\n",
        "        process.wait()\n",
        "\n",
        "        if process.returncode == 0 and os.path.exists(output_file):\n",
        "            file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "            print(f\"\\n   âœ… Complete! ({file_size:.2f} MB)\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"\\n   âŒ Download failed (exit code: {process.returncode})\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   âŒ Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_with_aria2(url, output_file, episode_num):\n",
        "    \"\"\"Download using aria2c (FAST multi-connection)\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nğŸ“¥ Downloading Episode {episode_num} with aria2...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        cmd = [\n",
        "            'aria2c',\n",
        "            url,\n",
        "            '-o', os.path.basename(output_file),\n",
        "            '-d', os.path.dirname(output_file),\n",
        "            '-x', str(MAX_WORKERS),  # connections per server\n",
        "            '-s', str(MAX_WORKERS),  # split into N parts\n",
        "            '-j', str(MAX_WORKERS),  # parallel downloads\n",
        "            '--max-tries=5',\n",
        "            '--retry-wait=3',\n",
        "            '--timeout=60',\n",
        "            '--connect-timeout=30',\n",
        "            '--max-connection-per-server=' + str(MAX_WORKERS),\n",
        "            '--min-split-size=1M',\n",
        "            '--continue=true',\n",
        "            '--allow-overwrite=true',\n",
        "            '--auto-file-renaming=false',\n",
        "            '--user-agent=' + headers['User-Agent'],\n",
        "            '--referer=' + BASE_URL,\n",
        "            '--summary-interval=1'\n",
        "        ]\n",
        "\n",
        "        process = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            universal_newlines=True\n",
        "        )\n",
        "\n",
        "        for line in process.stdout:\n",
        "            # Parse aria2 progress\n",
        "            if 'ETA' in line or '%' in line:\n",
        "                print(f\"\\r   {line.strip()}\", end='', flush=True)\n",
        "\n",
        "        process.wait()\n",
        "\n",
        "        if process.returncode == 0 and os.path.exists(output_file):\n",
        "            file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "            print(f\"\\n   âœ… Complete! ({file_size:.2f} MB)\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"\\n   âŒ Download failed\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   âŒ Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_chunk(url, start, end, chunk_file, pbar):\n",
        "    \"\"\"Download a single chunk with range request\"\"\"\n",
        "    try:\n",
        "        chunk_headers = headers.copy()\n",
        "        chunk_headers['Range'] = f'bytes={start}-{end}'\n",
        "\n",
        "        response = scraper.get(url, headers=chunk_headers, stream=True, timeout=30)\n",
        "\n",
        "        if response.status_code not in [200, 206]:\n",
        "            return False\n",
        "\n",
        "        with open(chunk_file, 'wb') as f:\n",
        "            for data in response.iter_content(chunk_size=8192):\n",
        "                if data:\n",
        "                    f.write(data)\n",
        "                    pbar.update(len(data))\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "def download_with_chunks(url, output_file, episode_num):\n",
        "    \"\"\"Download with chunked/parallel downloading\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nğŸ“¥ Downloading Episode {episode_num} with chunks...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        # Get file size\n",
        "        head_response = scraper.head(url, headers=headers, timeout=10)\n",
        "\n",
        "        if 'Content-Length' not in head_response.headers:\n",
        "            print(\"   âš ï¸ Server doesn't support range requests, falling back to direct download\")\n",
        "            return download_direct(url, output_file, episode_num)\n",
        "\n",
        "        total_size = int(head_response.headers['Content-Length'])\n",
        "        chunk_size = CHUNK_SIZE_MB * 1024 * 1024\n",
        "\n",
        "        # Calculate chunks\n",
        "        chunks = []\n",
        "        for i in range(0, total_size, chunk_size):\n",
        "            start = i\n",
        "            end = min(i + chunk_size - 1, total_size - 1)\n",
        "            chunks.append((start, end))\n",
        "\n",
        "        print(f\"   ğŸ“¦ Splitting into {len(chunks)} chunks...\")\n",
        "\n",
        "        # Download chunks in parallel\n",
        "        chunk_dir = f\"{output_file}.chunks\"\n",
        "        os.makedirs(chunk_dir, exist_ok=True)\n",
        "\n",
        "        chunk_files = []\n",
        "        failed = False\n",
        "\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"   â³ Downloading\", ncols=80) as pbar:\n",
        "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "                futures = {}\n",
        "\n",
        "                for idx, (start, end) in enumerate(chunks):\n",
        "                    chunk_file = f\"{chunk_dir}/chunk_{idx:04d}\"\n",
        "                    chunk_files.append(chunk_file)\n",
        "\n",
        "                    future = executor.submit(download_chunk, url, start, end, chunk_file, pbar)\n",
        "                    futures[future] = idx\n",
        "\n",
        "                # Wait for all chunks\n",
        "                for future in as_completed(futures):\n",
        "                    idx = futures[future]\n",
        "                    if not future.result():\n",
        "                        print(f\"\\n   âŒ Chunk {idx} failed\")\n",
        "                        failed = True\n",
        "                        break\n",
        "\n",
        "        if failed:\n",
        "            shutil.rmtree(chunk_dir, ignore_errors=True)\n",
        "            return False\n",
        "\n",
        "        # Merge chunks\n",
        "        print(\"   ğŸ”— Merging chunks...\")\n",
        "        with open(output_file, 'wb') as outfile:\n",
        "            for chunk_file in chunk_files:\n",
        "                with open(chunk_file, 'rb') as infile:\n",
        "                    shutil.copyfileobj(infile, outfile)\n",
        "\n",
        "        # Cleanup\n",
        "        shutil.rmtree(chunk_dir, ignore_errors=True)\n",
        "\n",
        "        file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "        print(f\"   âœ… Complete! ({file_size:.2f} MB)\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   âŒ Error: {e}\")\n",
        "        shutil.rmtree(chunk_dir, ignore_errors=True)\n",
        "        return False\n",
        "\n",
        "def download_direct(url, output_file, episode_num):\n",
        "    \"\"\"Direct download with progress bar\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nğŸ“¥ Downloading Episode {episode_num}...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        response = scraper.get(url, headers=headers, stream=True, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "        with open(output_file, 'wb') as f:\n",
        "            if total_size:\n",
        "                with tqdm(total=total_size, unit='B', unit_scale=True, unit_divisor=1024,\n",
        "                         desc=\"   â³ Progress\", ncols=80) as pbar:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "                            pbar.update(len(chunk))\n",
        "            else:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "\n",
        "        file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "        print(f\"   âœ… Complete! ({file_size:.2f} MB)\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   âŒ Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_with_ffmpeg(url, output_file, episode_num):\n",
        "    \"\"\"Download m3u8 with ffmpeg (with timeout protection)\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nğŸ“¥ Downloading Episode {episode_num} with ffmpeg...\")\n",
        "        print(f\"   File: {os.path.basename(output_file)}\")\n",
        "\n",
        "        cmd = [\n",
        "            'ffmpeg',\n",
        "            '-headers', f'Referer: {BASE_URL}',\n",
        "            '-user_agent', headers['User-Agent'],\n",
        "            '-i', url,\n",
        "            '-c', 'copy',\n",
        "            '-bsf:a', 'aac_adtstoasc',\n",
        "            '-y',\n",
        "            '-progress', 'pipe:1',\n",
        "            '-timeout', str(TIMEOUT * 1000000),  # microseconds\n",
        "            output_file\n",
        "        ]\n",
        "\n",
        "        process = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True\n",
        "        )\n",
        "\n",
        "        # Monitor with timeout\n",
        "        start_time = time.time()\n",
        "        last_progress_time = start_time\n",
        "\n",
        "        try:\n",
        "            for line in process.stdout:\n",
        "                if time.time() - last_progress_time > TIMEOUT:\n",
        "                    print(f\"\\n   âš ï¸ No progress for {TIMEOUT}s, killing process...\")\n",
        "                    process.kill()\n",
        "                    return False\n",
        "\n",
        "                if 'out_time_ms=' in line or 'progress=' in line:\n",
        "                    last_progress_time = time.time()\n",
        "                    if 'progress=continue' in line:\n",
        "                        print(\".\", end='', flush=True)\n",
        "\n",
        "            process.wait(timeout=30)\n",
        "\n",
        "            if process.returncode == 0 and os.path.exists(output_file):\n",
        "                file_size = os.path.getsize(output_file) / (1024*1024)\n",
        "                print(f\"\\n   âœ… Complete! ({file_size:.2f} MB)\")\n",
        "                return True\n",
        "            else:\n",
        "                print(f\"\\n   âŒ Failed (exit code: {process.returncode})\")\n",
        "                return False\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(f\"\\n   âš ï¸ Timeout after {TIMEOUT}s\")\n",
        "            process.kill()\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n   âŒ Error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_episode(url, output_file, episode_num):\n",
        "    \"\"\"Main download function with method selection and retry logic\"\"\"\n",
        "\n",
        "    # Determine if it's m3u8\n",
        "    is_m3u8 = '.m3u8' in url\n",
        "\n",
        "    # Select download method\n",
        "    if DOWNLOAD_METHOD == \"yt-dlp\":\n",
        "        download_func = download_with_ytdlp\n",
        "    elif DOWNLOAD_METHOD == \"aria2\" and not is_m3u8:\n",
        "        download_func = download_with_aria2\n",
        "    elif DOWNLOAD_METHOD == \"chunks\" and not is_m3u8:\n",
        "        download_func = download_with_chunks\n",
        "    elif DOWNLOAD_METHOD == \"ffmpeg\" and is_m3u8:\n",
        "        download_func = download_with_ffmpeg\n",
        "    else:\n",
        "        # Auto-select best method\n",
        "        if is_m3u8:\n",
        "            download_func = download_with_ytdlp  # Best for m3u8\n",
        "        else:\n",
        "            download_func = download_with_chunks  # Best for direct links\n",
        "\n",
        "    # Try download with retries\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        if attempt > 1:\n",
        "            print(f\"\\n   ğŸ”„ Retry {attempt}/{MAX_RETRIES}...\")\n",
        "            time.sleep(3)\n",
        "\n",
        "        # Remove partial file\n",
        "        if os.path.exists(output_file):\n",
        "            os.remove(output_file)\n",
        "\n",
        "        success = download_func(url, output_file, episode_num)\n",
        "\n",
        "        if success:\n",
        "            return True\n",
        "\n",
        "        # Try fallback methods\n",
        "        if attempt == MAX_RETRIES - 1 and is_m3u8:\n",
        "            print(f\"\\n   âš ï¸ Trying fallback method (ffmpeg)...\")\n",
        "            success = download_with_ffmpeg(url, output_file, episode_num)\n",
        "            if success:\n",
        "                return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“º STEP 5: FETCH ANIME INFO\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ¬ ANIMEKAI EPISODE DOWNLOADER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nğŸ” Processing: {ANIME_URL}\")\n",
        "\n",
        "anime_id, anime_title = get_anime_details(ANIME_URL)\n",
        "\n",
        "if not anime_id:\n",
        "    print(\"âŒ Could not extract anime ID\")\n",
        "    raise Exception(\"Invalid anime URL\")\n",
        "\n",
        "print(f\"âœ… Anime ID: {anime_id}\")\n",
        "print(f\"ğŸ“º Title: {anime_title}\")\n",
        "\n",
        "episode_list = get_episode_list(anime_id)\n",
        "\n",
        "if not episode_list:\n",
        "    print(\"âŒ No episodes found!\")\n",
        "    raise Exception(\"No episodes available\")\n",
        "\n",
        "print(f\"ğŸ“‹ Found {len(episode_list)} episode(s)\")\n",
        "\n",
        "# Determine episodes to download\n",
        "if DOWNLOAD_MODE == \"Single Episode\":\n",
        "    episodes_to_download = [ep for ep in episode_list if ep['number'] == SINGLE_EPISODE]\n",
        "elif DOWNLOAD_MODE == \"Episode Range\":\n",
        "    episodes_to_download = [ep for ep in episode_list if START_EPISODE <= ep['number'] <= END_EPISODE]\n",
        "else:\n",
        "    episodes_to_download = episode_list\n",
        "\n",
        "if not episodes_to_download:\n",
        "    print(\"âŒ No episodes match your selection!\")\n",
        "    raise Exception(\"No episodes to download\")\n",
        "\n",
        "print(f\"ğŸ“¥ Will download {len(episodes_to_download)} episode(s)\")\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“¥ STEP 6: DOWNLOAD EPISODES\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "download_dir = f\"downloads/{anime_title}\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "print(f\"\\nğŸ“ Download directory: {download_dir}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Map preference types\n",
        "type_map = {\n",
        "    \"Hard Sub\": \"sub\",\n",
        "    \"Soft Sub\": \"softsub\",\n",
        "    \"Dub & S-Sub\": \"dub\"\n",
        "}\n",
        "prefer_type_id = type_map.get(PREFER_TYPE, \"softsub\")\n",
        "\n",
        "downloaded_files = []\n",
        "failed_episodes = []\n",
        "\n",
        "for idx, episode in enumerate(episodes_to_download, 1):\n",
        "    print(f\"\\n[{idx}/{len(episodes_to_download)}] Episode {episode['number']}\")\n",
        "\n",
        "    try:\n",
        "        servers = get_video_servers(episode['token'])\n",
        "\n",
        "        if not servers:\n",
        "            print(\"   âš ï¸ No servers found\")\n",
        "            failed_episodes.append(episode['number'])\n",
        "            continue\n",
        "\n",
        "        # Filter servers\n",
        "        matching_servers = [\n",
        "            s for s in servers\n",
        "            if s['type'] == prefer_type_id and s['server_name'] == PREFER_SERVER\n",
        "        ]\n",
        "\n",
        "        if not matching_servers:\n",
        "            matching_servers = [s for s in servers if s['server_name'] == PREFER_SERVER]\n",
        "\n",
        "        if not matching_servers:\n",
        "            matching_servers = servers[:1]\n",
        "\n",
        "        server = matching_servers[0]\n",
        "        print(f\"   ğŸ¥ Server: {server['server_name']} ({server['type']})\")\n",
        "\n",
        "        video_url = get_video_url(server['server_id'], server['server_name'])\n",
        "\n",
        "        if not video_url:\n",
        "            print(\"   âŒ Could not extract video URL\")\n",
        "            failed_episodes.append(episode['number'])\n",
        "            continue\n",
        "\n",
        "        # Create filename\n",
        "        ep_num_str = f\"{int(episode['number']):03d}\"\n",
        "        filename = f\"{download_dir}/Episode_{ep_num_str}.mp4\"\n",
        "\n",
        "        # Download\n",
        "        success = download_episode(video_url, filename, episode['number'])\n",
        "\n",
        "        if success:\n",
        "            downloaded_files.append(filename)\n",
        "        else:\n",
        "            print(f\"   âŒ All download attempts failed\")\n",
        "            failed_episodes.append(episode['number'])\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ Error: {e}\")\n",
        "        failed_episodes.append(episode['number'])\n",
        "        continue\n",
        "\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ğŸ“Š STEP 7: DOWNLOAD SUMMARY\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ“Š DOWNLOAD SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nâœ… Successfully downloaded: {len(downloaded_files)} episode(s)\")\n",
        "if failed_episodes:\n",
        "    print(f\"âŒ Failed episodes: {', '.join(map(str, failed_episodes))}\")\n",
        "\n",
        "if downloaded_files:\n",
        "    total_size = sum(os.path.getsize(f) for f in downloaded_files if os.path.exists(f)) / (1024*1024)\n",
        "    print(f\"\\nğŸ’¾ Total size: {total_size:.2f} MB\")\n",
        "    print(f\"ğŸ“ Location: {download_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ‰ DOWNLOAD COMPLETE!\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ğŸ¬ Video Episode Merger & Uploader\n",
        "# Download ZIP file with video episodes, extract, merge them in order, and upload\n",
        "\n",
        "# @title ğŸ”§ **Install Dependencies** { display-mode: \"form\" }\n",
        "print(\"ğŸ“¦ Installing required packages...\")\n",
        "!pip install -q natsort requests\n",
        "!apt-get -qq install -y ffmpeg > /dev/null 2>&1\n",
        "print(\"âœ… All dependencies installed!\\n\")\n",
        "\n",
        "# @title âš™ï¸ **Configuration** { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### ğŸ“¥ Download Settings\n",
        "#@markdown Enter the direct download link (DDL) for your ZIP file:\n",
        "zip_url = \"https://example.com/videos.zip\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Custom User-Agent (leave default if unsure):\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ğŸ¬ Merge Settings\n",
        "#@markdown Custom output name (leave empty to auto-detect from episodes):\n",
        "custom_output_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Video quality for merge:\n",
        "merge_quality = \"Copy Original (Fastest)\" #@param [\"Copy Original (Fastest)\", \"Re-encode High Quality\", \"Re-encode Compressed\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ğŸ“¤ Upload Settings\n",
        "upload_destination = \"Both (GoFile + Google Drive)\" #@param [\"GoFile.io Only\", \"Google Drive Only\", \"Both (GoFile + Google Drive)\", \"None (Keep Local Only)\"]\n",
        "\n",
        "#@markdown Create ZIP of merged video for upload?\n",
        "create_upload_zip = False #@param {type:\"boolean\"}\n",
        "\n",
        "print(\"âœ… Configuration set!\")\n",
        "\n",
        "# @title ğŸ“¥ **Download ZIP File** { display-mode: \"form\" }\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from urllib.parse import unquote, urlparse\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ğŸ”½ Starting download...\")\n",
        "print(f\"ğŸ”— URL: {zip_url[:60]}...\" if len(zip_url) > 60 else f\"ğŸ”— URL: {zip_url}\")\n",
        "\n",
        "headers = {'User-Agent': user_agent}\n",
        "\n",
        "try:\n",
        "    response = requests.get(zip_url, headers=headers, stream=True, allow_redirects=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Smart filename detection\n",
        "    zip_filename = None\n",
        "\n",
        "    # Method 1: Content-Disposition header\n",
        "    if 'Content-Disposition' in response.headers:\n",
        "        cd = response.headers['Content-Disposition']\n",
        "        filenames = re.findall(r'filename\\*?=[\"\\']?(?:UTF-8\\'\\')?([^\"\\';]+)[\"\\']?', cd)\n",
        "        if filenames:\n",
        "            zip_filename = unquote(filenames[0])\n",
        "            print(f\"ğŸ“‹ Filename from header: {zip_filename}\")\n",
        "\n",
        "    # Method 2: Final URL after redirects\n",
        "    if not zip_filename:\n",
        "        final_url = response.url\n",
        "        url_path = urlparse(final_url).path\n",
        "        zip_filename = os.path.basename(url_path)\n",
        "        zip_filename = unquote(zip_filename)\n",
        "        print(f\"ğŸ“‹ Filename from URL: {zip_filename}\")\n",
        "\n",
        "    # Method 3: Extract meaningful name from URL\n",
        "    if not zip_filename or zip_filename in ['', 'download', 'file']:\n",
        "        # Try to extract from full URL path\n",
        "        url_parts = [p for p in urlparse(zip_url).path.split('/') if p and p != 'download']\n",
        "        if url_parts:\n",
        "            zip_filename = url_parts[-1]\n",
        "            zip_filename = unquote(zip_filename)\n",
        "\n",
        "    # Ensure .zip extension\n",
        "    if not zip_filename.lower().endswith('.zip'):\n",
        "        if '.' not in zip_filename:\n",
        "            zip_filename += '.zip'\n",
        "        else:\n",
        "            zip_filename = os.path.splitext(zip_filename)[0] + '.zip'\n",
        "\n",
        "    # Clean filename (remove invalid characters)\n",
        "    zip_filename = re.sub(r'[<>:\"|?*\\\\]', '_', zip_filename)\n",
        "    zip_filename = re.sub(r'[\\x00-\\x1f]', '', zip_filename)  # Remove control characters\n",
        "\n",
        "    print(f\"ğŸ’¾ Saving as: {zip_filename}\")\n",
        "\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    downloaded = 0\n",
        "\n",
        "    with open(zip_filename, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "                downloaded += len(chunk)\n",
        "                if total_size:\n",
        "                    percent = (downloaded / total_size) * 100\n",
        "                    mb_downloaded = downloaded / (1024*1024)\n",
        "                    mb_total = total_size / (1024*1024)\n",
        "                    print(f\"\\râ³ Progress: {percent:.1f}% ({mb_downloaded:.1f}/{mb_total:.1f} MB)\", end='')\n",
        "\n",
        "    print(f\"\\nâœ… Downloaded: {zip_filename} ({downloaded / (1024*1024):.2f} MB)\")\n",
        "\n",
        "    # Extract base name for later use\n",
        "    ZIP_BASE_NAME = os.path.splitext(zip_filename)[0]\n",
        "    ZIP_BASE_NAME = re.sub(r'[_\\-\\s]+', ' ', ZIP_BASE_NAME).strip()\n",
        "\n",
        "    print(f\"ğŸ“¦ Base name extracted: '{ZIP_BASE_NAME}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Download failed: {str(e)}\")\n",
        "    import traceback\n",
        "    print(traceback.format_exc())\n",
        "    raise\n",
        "\n",
        "# @title ğŸ“¦ **Extract ZIP File** { display-mode: \"form\" }\n",
        "import zipfile\n",
        "\n",
        "extract_folder = \"extracted_videos\"\n",
        "os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "print(f\"\\nğŸ“‚ Extracting to: {extract_folder}/\")\n",
        "print(\"â³ Please wait...\")\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        total_files = len(file_list)\n",
        "\n",
        "        print(f\"ğŸ“‹ Found {total_files} file(s) in ZIP\\n\")\n",
        "\n",
        "        # Extract with progress\n",
        "        for idx, file in enumerate(file_list, 1):\n",
        "            zip_ref.extract(file, extract_folder)\n",
        "            if idx % 5 == 0 or idx == total_files:\n",
        "                print(f\"\\râ³ Extracting: {idx}/{total_files} files...\", end='')\n",
        "\n",
        "        print(f\"\\n\\nğŸ“„ Extracted files:\")\n",
        "        video_count = 0\n",
        "        for file in file_list:\n",
        "            file_lower = file.lower()\n",
        "            is_video = any(file_lower.endswith(ext) for ext in ['.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv', '.webm', '.m4v'])\n",
        "            icon = \"ğŸ¬\" if is_video else \"ğŸ“„\"\n",
        "            print(f\"  {icon} {file}\")\n",
        "            if is_video:\n",
        "                video_count += 1\n",
        "\n",
        "        print(f\"\\nâœ… Extraction complete! Found {video_count} video file(s)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Extraction failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# @title ğŸ” **Detect and Sort Episodes** { display-mode: \"form\" }\n",
        "import re\n",
        "from natsort import natsorted\n",
        "\n",
        "def extract_episode_info(filename):\n",
        "    \"\"\"Enhanced episode detection with better pattern matching\"\"\"\n",
        "    name = os.path.basename(filename)\n",
        "\n",
        "    # Combined season and episode patterns (S01E01, S1E1, etc.)\n",
        "    combined_patterns = [\n",
        "        r'[Ss](\\d+)[Ee](\\d+)',  # S01E01, S1E1\n",
        "        r'[Ss]eason[\\s._-]*(\\d+)[\\s._-]*[Ee]pisode[\\s._-]*(\\d+)',  # Season 1 Episode 1\n",
        "        r'[Ss]eason[\\s._-]*(\\d+)[\\s._-]*[Ee][Pp][\\s._-]*(\\d+)',  # Season 1 Ep 1\n",
        "        r'(\\d+)[xX](\\d+)',  # 1x01\n",
        "    ]\n",
        "\n",
        "    # Try combined patterns first\n",
        "    for pattern in combined_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            return int(match.group(1)), int(match.group(2))\n",
        "\n",
        "    # Separate season patterns\n",
        "    season_patterns = [\n",
        "        r'[Ss]eason[\\s._-]*(\\d+)',\n",
        "        r'[Ss](\\d+)(?![Ee])',  # S1 but not followed by E\n",
        "        r'Season[\\s._-]*(\\d+)',\n",
        "    ]\n",
        "\n",
        "    # Episode patterns\n",
        "    episode_patterns = [\n",
        "        r'[Ee]pisode[\\s._-]*(\\d+)',\n",
        "        r'[Ee][Pp][\\s._-]*(\\d+)',\n",
        "        r'[Ee](\\d+)',\n",
        "        r'Episode[\\s._-]*(\\d+)',\n",
        "        r'[\\s._-](\\d{1,3})[\\s._-]',  # Number surrounded by separators\n",
        "        r'^(\\d{1,3})[\\s._-]',  # Number at start\n",
        "        r'[\\s._-](\\d{1,3})\\.',  # Number before extension\n",
        "    ]\n",
        "\n",
        "    season = None\n",
        "    episode = None\n",
        "\n",
        "    # Find season\n",
        "    for pattern in season_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            season = int(match.group(1))\n",
        "            break\n",
        "\n",
        "    # Find episode\n",
        "    for pattern in episode_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            ep_num = int(match.group(1))\n",
        "            # Reasonable episode number (1-999)\n",
        "            if 1 <= ep_num <= 999:\n",
        "                episode = ep_num\n",
        "                break\n",
        "\n",
        "    return season, episode\n",
        "\n",
        "# Find all video files\n",
        "video_extensions = ['.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv', '.webm', '.m4v', '.ts', '.m2ts']\n",
        "video_files = []\n",
        "\n",
        "for root, dirs, files in os.walk(extract_folder):\n",
        "    for file in files:\n",
        "        if any(file.lower().endswith(ext) for ext in video_extensions):\n",
        "            full_path = os.path.join(root, file)\n",
        "            video_files.append(full_path)\n",
        "\n",
        "if not video_files:\n",
        "    print(\"âŒ No video files found in the ZIP!\")\n",
        "    raise Exception(\"No video files detected\")\n",
        "\n",
        "print(f\"ğŸ¬ Found {len(video_files)} video file(s)\\n\")\n",
        "\n",
        "# Extract info and sort\n",
        "video_info = []\n",
        "for vf in video_files:\n",
        "    season, episode = extract_episode_info(vf)\n",
        "    video_info.append({\n",
        "        'path': vf,\n",
        "        'name': os.path.basename(vf),\n",
        "        'season': season if season else 0,\n",
        "        'episode': episode if episode else 0\n",
        "    })\n",
        "\n",
        "# Sort by season, then episode, then natural name\n",
        "video_info.sort(key=lambda x: (x['season'], x['episode'], x['name']))\n",
        "\n",
        "print(\"ğŸ“‹ **Detected Episode Order:**\")\n",
        "print(\"=\" * 70)\n",
        "for idx, info in enumerate(video_info, 1):\n",
        "    s_info = f\"S{info['season']:02d}\" if info['season'] else \"S??\"\n",
        "    e_info = f\"E{info['episode']:02d}\" if info['episode'] else \"E??\"\n",
        "    size_mb = os.path.getsize(info['path']) / (1024*1024)\n",
        "    print(f\"{idx:2d}. [{s_info}{e_info}] {info['name'][:45]:<45} ({size_mb:.1f} MB)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# @title ğŸï¸ **Merge Videos** { display-mode: \"form\" }\n",
        "import subprocess\n",
        "\n",
        "print(\"\\nğŸ¬ Preparing to merge videos...\")\n",
        "\n",
        "# Create file list for ffmpeg\n",
        "list_file = \"filelist.txt\"\n",
        "with open(list_file, 'w', encoding='utf-8') as f:\n",
        "    for info in video_info:\n",
        "        # Escape single quotes for ffmpeg\n",
        "        safe_path = info['path'].replace(\"'\", \"'\\\\''\")\n",
        "        f.write(f\"file '{safe_path}'\\n\")\n",
        "\n",
        "print(f\"âœ… Created merge list with {len(video_info)} video(s)\")\n",
        "\n",
        "# Determine output filename\n",
        "if custom_output_name:\n",
        "    output_name = custom_output_name\n",
        "    if not output_name.lower().endswith('.mp4'):\n",
        "        output_name += '.mp4'\n",
        "else:\n",
        "    # Auto-generate name\n",
        "    seasons = [v['season'] for v in video_info if v['season'] > 0]\n",
        "    episodes = [v['episode'] for v in video_info if v['episode'] > 0]\n",
        "\n",
        "    base_name = ZIP_BASE_NAME\n",
        "\n",
        "    if seasons and episodes:\n",
        "        min_season = min(seasons)\n",
        "        max_season = max(seasons)\n",
        "        min_episode = min(episodes)\n",
        "        max_episode = max(episodes)\n",
        "\n",
        "        if min_season == max_season:\n",
        "            output_name = f\"{base_name} Season {min_season:02d} Episodes {min_episode:02d}-{max_episode:02d}.mp4\"\n",
        "        else:\n",
        "            output_name = f\"{base_name} S{min_season:02d}-S{max_season:02d} Ep{min_episode:02d}-{max_episode:02d}.mp4\"\n",
        "    else:\n",
        "        output_name = f\"{base_name} Merged Complete.mp4\"\n",
        "\n",
        "# Clean output name\n",
        "output_name = re.sub(r'[<>:\"|?*\\\\]', '_', output_name)\n",
        "output_name = re.sub(r'\\s+', ' ', output_name).strip()\n",
        "\n",
        "print(f\"\\nğŸ“ Output filename: {output_name}\")\n",
        "\n",
        "# Build ffmpeg command based on quality setting\n",
        "if merge_quality == \"Copy Original (Fastest)\":\n",
        "    cmd = [\n",
        "        'ffmpeg', '-f', 'concat', '-safe', '0', '-i', list_file,\n",
        "        '-c', 'copy', output_name, '-y'\n",
        "    ]\n",
        "    print(\"âš¡ Mode: Fast merge (copy streams, no re-encoding)\")\n",
        "elif merge_quality == \"Re-encode High Quality\":\n",
        "    cmd = [\n",
        "        'ffmpeg', '-f', 'concat', '-safe', '0', '-i', list_file,\n",
        "        '-c:v', 'libx264', '-crf', '18', '-preset', 'slow',\n",
        "        '-c:a', 'aac', '-b:a', '192k',\n",
        "        output_name, '-y'\n",
        "    ]\n",
        "    print(\"ğŸ¨ Mode: High quality re-encode (slower, best quality)\")\n",
        "else:  # Compressed\n",
        "    cmd = [\n",
        "        'ffmpeg', '-f', 'concat', '-safe', '0', '-i', list_file,\n",
        "        '-c:v', 'libx264', '-crf', '23', '-preset', 'medium',\n",
        "        '-c:a', 'aac', '-b:a', '128k',\n",
        "        output_name, '-y'\n",
        "    ]\n",
        "    print(\"ğŸ“¦ Mode: Compressed re-encode (smaller file size)\")\n",
        "\n",
        "print(\"\\nâ³ Merging videos... This may take a while.\\n\")\n",
        "\n",
        "try:\n",
        "    # Run ffmpeg\n",
        "    process = subprocess.Popen(cmd, stderr=subprocess.PIPE, universal_newlines=True)\n",
        "\n",
        "    # Parse ffmpeg output for progress\n",
        "    duration_pattern = re.compile(r'Duration: (\\d{2}):(\\d{2}):(\\d{2})')\n",
        "    time_pattern = re.compile(r'time=(\\d{2}):(\\d{2}):(\\d{2})')\n",
        "\n",
        "    total_duration = None\n",
        "\n",
        "    for line in process.stderr:\n",
        "        # Get total duration\n",
        "        if total_duration is None:\n",
        "            dur_match = duration_pattern.search(line)\n",
        "            if dur_match:\n",
        "                h, m, s = map(int, dur_match.groups())\n",
        "                total_duration = h * 3600 + m * 60 + s\n",
        "\n",
        "        # Get current time\n",
        "        time_match = time_pattern.search(line)\n",
        "        if time_match and total_duration:\n",
        "            h, m, s = map(int, time_match.groups())\n",
        "            current_time = h * 3600 + m * 60 + s\n",
        "            percent = (current_time / total_duration) * 100\n",
        "            print(f\"\\rğŸ¬ Progress: {percent:.1f}% ({current_time//60}:{current_time%60:02d} / {total_duration//60}:{total_duration%60:02d})\", end='')\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "    if process.returncode == 0:\n",
        "        file_size = os.path.getsize(output_name) / (1024*1024)\n",
        "        print(f\"\\n\\nâœ… **Merge Complete!**\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"ğŸ“ Output: {output_name}\")\n",
        "        print(f\"ğŸ’¾ Size: {file_size:.2f} MB\")\n",
        "        print(f\"ğŸ¬ Episodes: {len(video_info)}\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        MERGED_VIDEO = output_name\n",
        "    else:\n",
        "        print(f\"\\nâŒ Merge failed with exit code {process.returncode}\")\n",
        "        raise Exception(\"FFmpeg merge failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Error during merge: {str(e)}\")\n",
        "    raise\n",
        "finally:\n",
        "    # Cleanup\n",
        "    if os.path.exists(list_file):\n",
        "        os.remove(list_file)\n",
        "\n",
        "# @title ğŸ“¦ **Create ZIP of Merged Video (Optional)** { display-mode: \"form\" }\n",
        "\n",
        "if create_upload_zip:\n",
        "    print(\"\\nğŸ“¦ Creating ZIP file of merged video...\")\n",
        "\n",
        "    zip_output = output_name.replace('.mp4', '.zip')\n",
        "\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(zip_output, 'w', zipfile.ZIP_DEFLATED, compresslevel=0) as zipf:\n",
        "        print(f\"â³ Adding {output_name} to ZIP...\")\n",
        "        zipf.write(output_name, os.path.basename(output_name))\n",
        "\n",
        "    zip_size = os.path.getsize(zip_output) / (1024*1024)\n",
        "    print(f\"âœ… ZIP created: {zip_output} ({zip_size:.2f} MB)\")\n",
        "\n",
        "    UPLOAD_FILE = zip_output\n",
        "else:\n",
        "    UPLOAD_FILE = MERGED_VIDEO\n",
        "    print(\"\\nğŸ“„ Will upload video file directly (no ZIP)\")\n",
        "\n",
        "# @title ğŸ“¤ **Upload Files** { display-mode: \"form\" }\n",
        "\n",
        "def upload_to_gofile(filepath):\n",
        "    \"\"\"Upload file to GoFile.io\"\"\"\n",
        "    try:\n",
        "        print(\"\\nğŸŒ GoFile.io Upload\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Get best server\n",
        "        server_response = requests.get('https://api.gofile.io/servers', timeout=30)\n",
        "        server_response.raise_for_status()\n",
        "        server_data = server_response.json()\n",
        "\n",
        "        if server_data['status'] != 'ok':\n",
        "            print(\"âŒ Failed to get GoFile server\")\n",
        "            return None\n",
        "\n",
        "        server = server_data['data']['servers'][0]['name']\n",
        "        print(f\"ğŸ“¡ Server: {server}\")\n",
        "\n",
        "        # Updated endpoint\n",
        "        upload_url = f'https://{server}.gofile.io/contents/uploadfile'\n",
        "\n",
        "        file_size_mb = os.path.getsize(filepath) / (1024*1024)\n",
        "        print(f\"ğŸ“¦ File: {os.path.basename(filepath)} ({file_size_mb:.2f} MB)\")\n",
        "        print(\"â³ Uploading... (this may take several minutes for large files)\")\n",
        "\n",
        "        with open(filepath, 'rb') as f:\n",
        "            files_data = {'file': (os.path.basename(filepath), f, 'application/octet-stream')}\n",
        "            response = requests.post(upload_url, files=files_data, timeout=7200)\n",
        "\n",
        "        response.raise_for_status()\n",
        "\n",
        "        result = response.json()\n",
        "        if result['status'] == 'ok':\n",
        "            download_page = result['data']['downloadPage']\n",
        "            print(\"âœ… Upload successful!\")\n",
        "            print(f\"ğŸ”— Link: {download_page}\")\n",
        "            return download_page\n",
        "        else:\n",
        "            print(f\"âŒ Upload failed: {result.get('message', 'Unknown error')}\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        print(\"âŒ Upload timed out - file may be too large for GoFile\")\n",
        "        return None\n",
        "    except requests.exceptions.JSONDecodeError:\n",
        "        print(\"âŒ Invalid response from GoFile - service may be down\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ GoFile error: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def upload_to_gdrive(filepath):\n",
        "    \"\"\"Upload file to Google Drive\"\"\"\n",
        "    try:\n",
        "        print(\"\\nâ˜ï¸ Google Drive Upload\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        from google.colab import drive\n",
        "\n",
        "        # Check if already mounted\n",
        "        if not os.path.exists('/content/drive/MyDrive'):\n",
        "            drive.mount('/content/drive', force_remount=False)\n",
        "            print(\"âœ… Google Drive mounted!\")\n",
        "        else:\n",
        "            print(\"âœ… Google Drive already mounted!\")\n",
        "\n",
        "        destination = '/content/drive/MyDrive/Merged_Videos/'\n",
        "        os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "        dest_path = os.path.join(destination, os.path.basename(filepath))\n",
        "\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"âŒ Source file not found: {filepath}\")\n",
        "            return None\n",
        "\n",
        "        file_size_mb = os.path.getsize(filepath) / (1024*1024)\n",
        "        print(f\"ğŸ“¦ File: {os.path.basename(filepath)} ({file_size_mb:.2f} MB)\")\n",
        "        print(f\"â³ Copying to Google Drive...\")\n",
        "\n",
        "        import shutil\n",
        "        shutil.copy2(filepath, dest_path)\n",
        "\n",
        "        print(\"âœ… Upload successful!\")\n",
        "        print(f\"ğŸ“ Location: MyDrive/Merged_Videos/{os.path.basename(filepath)}\")\n",
        "        return dest_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Google Drive error: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        return None\n",
        "\n",
        "# Execute uploads based on user selection\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ“¤ UPLOAD PROCESS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Track upload results\n",
        "gofile_link = None\n",
        "gdrive_path = None\n",
        "\n",
        "if upload_destination == \"GoFile.io Only\":\n",
        "    gofile_link = upload_to_gofile(UPLOAD_FILE)\n",
        "\n",
        "elif upload_destination == \"Google Drive Only\":\n",
        "    gdrive_path = upload_to_gdrive(UPLOAD_FILE)\n",
        "\n",
        "elif upload_destination == \"Both (GoFile + Google Drive)\":\n",
        "    gofile_link = upload_to_gofile(UPLOAD_FILE)\n",
        "    gdrive_path = upload_to_gdrive(UPLOAD_FILE)\n",
        "\n",
        "else:  # None\n",
        "    print(\"\\nğŸ“ Upload skipped - file saved locally\")\n",
        "    print(f\"ğŸ“„ Location: /content/{UPLOAD_FILE}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ğŸ‰ **ALL DONE!**\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nğŸ“Š Summary:\")\n",
        "print(f\"  â€¢ Videos merged: {len(video_info)}\")\n",
        "print(f\"  â€¢ Output file: {output_name}\")\n",
        "print(f\"  â€¢ File size: {os.path.getsize(MERGED_VIDEO) / (1024*1024):.2f} MB\")\n",
        "\n",
        "if gofile_link:\n",
        "    print(f\"\\nğŸ”— GoFile.io Link:\")\n",
        "    print(f\"   {gofile_link}\")\n",
        "\n",
        "if gdrive_path:\n",
        "    print(f\"\\nğŸ“ Google Drive:\")\n",
        "    print(f\"   {gdrive_path}\")\n",
        "\n",
        "if not gofile_link and not gdrive_path and upload_destination != \"None (Keep Local Only)\":\n",
        "    print(f\"\\nâš ï¸ Note: Some uploads may have failed. Check error messages above.\")\n",
        "\n",
        "print(\"\\nâœ¨ Process complete!\")"
      ],
      "metadata": {
        "id": "P7PZkspKUd_a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}