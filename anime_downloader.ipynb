{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cinichi/Ani-Downloader/blob/main/anime_downloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NioTUUOYM4N7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ================================================================\n",
        "# ðŸ“œ DISCLAIMER\n",
        "# Personal / educational use only.\n",
        "# Respect copyright laws and the AnimeKai/host site terms of use.\n",
        "# Do NOT use this notebook for commercial or infringing purposes.\n",
        "# ================================================================\n",
        "\n",
        "# ðŸŽ¬ AnimeKai Episode Downloader & Merger (FIXED VERSION + Pixeldrain)\n",
        "# Fixes: Subtitle embedding for Soft Sub/Dub, Single episode upload, Episode naming\n",
        "# New: Pixeldrain upload support with detailed service limits\n",
        "\n",
        "# @title ðŸ”§ Install Dependencies { display-mode: \"form\" }\n",
        "print(\"ðŸ“¦ Installing required packages...\")\n",
        "!pip install -q requests beautifulsoup4 cloudscraper m3u8 pycryptodome tqdm yt-dlp\n",
        "!apt-get -qq install -y ffmpeg aria2 > /dev/null 2>&1\n",
        "print(\"âœ… All dependencies installed!\\n\")\n",
        "\n",
        "# @title âš™ï¸ Configuration { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### ðŸ”— Anime URL\n",
        "anime_url = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ### ðŸ“º Episode Selection\n",
        "download_mode = \"Episode Range\"  # @param [\"All Episodes\", \"Episode Range\", \"Single Episode\"]\n",
        "\n",
        "#@markdown Single episode (accepts things like \"12.5\", \"SP\"):\n",
        "single_episode = \"1\"  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown Episode range (interpreted numerically when possible):\n",
        "start_episode = \"1\"  # @param {type:\"string\"}\n",
        "end_episode   = \"2\"  # @param {type:\"string\"}\n",
        "\n",
        "#@markdown ### ðŸŽ¥ Quality & Audio Settings\n",
        "video_quality = \"720p\"  # @param [\"1080p\", \"720p\", \"480p\", \"360p\"]\n",
        "\n",
        "# NOTE: \"Dub (with subs)\" is AnimeKai's dub type; not true dual-audio.\n",
        "prefer_type = \"Soft Sub\"  # @param [\"Hard Sub\", \"Soft Sub\", \"Dub (with subs)\"]\n",
        "prefer_server = \"Server 1\"  # @param [\"Server 1\", \"Server 2\"]\n",
        "\n",
        "#@markdown ### ðŸ“¥ Download Settings\n",
        "download_method = \"yt-dlp\"  # @param [\"yt-dlp\", \"aria2\", \"chunks\", \"ffmpeg\"]\n",
        "\n",
        "chunk_size_mb      = 15    # @param {type:\"slider\", min:1, max:20, step:1}\n",
        "max_workers        = 13    # @param {type:\"slider\", min:1, max:16, step:1}\n",
        "max_retries        = 6    # @param {type:\"slider\", min:1, max:10, step:1}\n",
        "connection_timeout = 600  # @param {type:\"slider\", min:60, max:600, step:30}\n",
        "\n",
        "#@markdown ### ðŸ”— Merge Settings\n",
        "merge_episodes = True  # @param {type:\"boolean\"}\n",
        "season_number  = 0     # @param {type:\"integer\"}  # 0 = auto-detect\n",
        "keep_individual_files = False  # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### ðŸ“¤ Upload Settings\n",
        "#@markdown **Service Limits:**\n",
        "#@markdown - **GoFile.io**: Free, unlimited size, no account needed, links expire after inactivity\n",
        "#@markdown - **Pixeldrain**: Free up to 20GB/file (10GB for anonymous), no expiration with free account\n",
        "#@markdown - **Google Drive**: 15GB free storage (shared with Gmail), requires authentication\n",
        "\n",
        "upload_destination = \"GoFile + Pixeldrain\"  # @param [\"GoFile.io Only\", \"Pixeldrain Only\", \"Google Drive Only\", \"GoFile + Pixeldrain\", \"All Services\", \"None (Keep Local)\"]\n",
        "upload_merged_only = True    # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ### ðŸ”‘ Pixeldrain API Key (Optional)\n",
        "#@markdown Get free API key at https://pixeldrain.com/user/api_keys (increases limit to 20GB/file)\n",
        "pixeldrain_api_key = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "print(\"âœ… Configuration set!\")\n",
        "print(f\"ðŸ“¥ Download method: {download_method}\")\n",
        "print(f\"âš™ï¸ Workers: {max_workers} | Chunk size: {chunk_size_mb}MB\")\n",
        "print(f\"ðŸ”„ Max retries: {max_retries} | Timeout: {connection_timeout}s\")\n",
        "if merge_episodes:\n",
        "    print(f\"ðŸ”— Merge enabled | Keep files: {keep_individual_files}\")\n",
        "print(f\"\\nðŸ“¤ Upload destination: {upload_destination}\")\n",
        "if upload_destination in [\"Pixeldrain Only\", \"GoFile + Pixeldrain\", \"All Services\"]:\n",
        "    if pixeldrain_api_key:\n",
        "        print(f\"ðŸ”‘ Pixeldrain API key: Configured (20GB limit)\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Pixeldrain API key: Not set (10GB anonymous limit)\")\n",
        "\n",
        "# ================================================================\n",
        "# Imports & globals\n",
        "# ================================================================\n",
        "\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "from typing import List, Optional, Tuple, Dict, Any\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import cloudscraper\n",
        "from urllib.parse import urlparse\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def log(level: str, msg: str) -> None:\n",
        "    print(f\"[{level}] {msg}\")\n",
        "\n",
        "BASE_URL = \"https://animekai.to\"\n",
        "scraper = cloudscraper.create_scraper(\n",
        "    browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"desktop\": True}\n",
        ")\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
        "    \"Referer\": BASE_URL,\n",
        "    \"Accept\": \"*/*\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "}\n",
        "\n",
        "RETRY_CONFIG = {\n",
        "    \"max_retries\": max_retries,\n",
        "    \"sleep_between\": 3,\n",
        "    \"timeout\": connection_timeout,\n",
        "}\n",
        "\n",
        "# ================================================================\n",
        "# enc-dec helpers\n",
        "# ================================================================\n",
        "\n",
        "def call_enc_dec_api(endpoint: str, payload: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "    base = \"https://enc-dec.app/api\"\n",
        "    url = f\"{base}/{endpoint}\"\n",
        "    try:\n",
        "        if endpoint.startswith(\"enc-\"):\n",
        "            text = payload.get(\"text\", \"\")\n",
        "            resp = scraper.get(f\"{url}?text={text}\", headers=HEADERS, timeout=15)\n",
        "        else:\n",
        "            resp = scraper.post(\n",
        "                url,\n",
        "                headers={\"Content-Type\": \"application/json\"},\n",
        "                data=json.dumps(payload),\n",
        "                timeout=30,\n",
        "            )\n",
        "        resp.raise_for_status()\n",
        "        return resp.json()\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"enc-dec API '{endpoint}' failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def enc_kai(text: str) -> Optional[str]:\n",
        "    data = call_enc_dec_api(\"enc-kai\", {\"text\": text})\n",
        "    if not data or \"result\" not in data:\n",
        "        log(\"ERROR\", \"Failed to get enc-kai result.\")\n",
        "        return None\n",
        "    return data[\"result\"]\n",
        "\n",
        "def dec_kai(text: str) -> Optional[Dict[str, Any]]:\n",
        "    data = call_enc_dec_api(\"dec-kai\", {\"text\": text})\n",
        "    if not data or \"result\" not in data:\n",
        "        log(\"ERROR\", \"Failed to decode dec-kai payload.\")\n",
        "        return None\n",
        "    return data[\"result\"]\n",
        "\n",
        "def dec_mega(text: str, agent: str) -> Optional[Dict[str, Any]]:\n",
        "    data = call_enc_dec_api(\"dec-mega\", {\"text\": text, \"agent\": agent})\n",
        "    if not data or \"result\" not in data:\n",
        "        log(\"ERROR\", \"Failed to decode dec-mega payload.\")\n",
        "        return None\n",
        "    return data[\"result\"]\n",
        "\n",
        "# ================================================================\n",
        "# Anime info & episodes\n",
        "# ================================================================\n",
        "\n",
        "def get_anime_details(url: str) -> Tuple[Optional[str], str]:\n",
        "    try:\n",
        "        r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        anime_div = soup.select_one(\"div[data-id]\")\n",
        "        anime_id = anime_div.get(\"data-id\") if anime_div else None\n",
        "\n",
        "        title_elem = (\n",
        "            soup.select_one(\"div.title-wrapper h1.title span\")\n",
        "            or soup.select_one(\"h1.title\")\n",
        "            or soup.select_one(\".anime-title\")\n",
        "        )\n",
        "        title = title_elem.get(\"title\") if title_elem and title_elem.get(\"title\") else (\n",
        "            title_elem.text.strip() if title_elem else \"Unknown\"\n",
        "        )\n",
        "        title = re.sub(r'[<>:\"/\\\\|?*]', \"\", title)\n",
        "        return anime_id, title\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Error getting anime details: {e}\")\n",
        "        return None, \"Unknown\"\n",
        "\n",
        "def detect_season_from_title(title: str) -> int:\n",
        "    patterns = [\n",
        "        r\"[Ss]eason\\s+(\\d+)\",\n",
        "        r\"[Ss](\\d+)\",\n",
        "        r\"(\\d+)(?:st|nd|rd|th)\\s+[Ss]eason\",\n",
        "        r\"\\s+(\\d+)$\",\n",
        "        r\"Part\\s+(\\d+)\",\n",
        "        r\"Cour\\s+(\\d+)\",\n",
        "    ]\n",
        "    for p in patterns:\n",
        "        m = re.search(p, title)\n",
        "        if m:\n",
        "            return int(m.group(1))\n",
        "    return 1\n",
        "\n",
        "def safe_episode_key(ep_id: str) -> Tuple[int, float]:\n",
        "    m = re.match(r\"(\\d+)(?:\\.(\\d+))?\", ep_id)\n",
        "    if m:\n",
        "        main = int(m.group(1))\n",
        "        frac = float(f\"0.{m.group(2)}\") if m.group(2) else 0.0\n",
        "        return main, frac\n",
        "    return (10**9, 0.0)\n",
        "\n",
        "def get_episode_list(anime_id: str) -> List[Dict[str, Any]]:\n",
        "    try:\n",
        "        enc = enc_kai(anime_id)\n",
        "        if not enc:\n",
        "            return []\n",
        "        url = f\"{BASE_URL}/ajax/episodes/list?ani_id={anime_id}&_={enc}\"\n",
        "        r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        html = data.get(\"result\", \"\")\n",
        "        if not html:\n",
        "            return []\n",
        "\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        episodes: List[Dict[str, Any]] = []\n",
        "        for ep in soup.select(\"div.eplist a\"):\n",
        "            token = ep.get(\"token\", \"\")\n",
        "            ep_id = ep.get(\"num\", \"\").strip()\n",
        "            langs = ep.get(\"langs\", \"0\")\n",
        "            try:\n",
        "                langs_int = int(langs)\n",
        "            except ValueError:\n",
        "                langs_int = 0\n",
        "            if langs_int == 1:\n",
        "                subdub = \"Sub\"\n",
        "            elif langs_int == 3:\n",
        "                subdub = \"Dub & Sub\"\n",
        "            else:\n",
        "                subdub = \"\"\n",
        "\n",
        "            episodes.append(\n",
        "                {\n",
        "                    \"id\": ep_id,\n",
        "                    \"sort_key\": safe_episode_key(ep_id),\n",
        "                    \"token\": token,\n",
        "                    \"subdub\": subdub,\n",
        "                    \"title\": f\"Episode {ep_id}\",\n",
        "                }\n",
        "            )\n",
        "        episodes.sort(key=lambda e: e[\"sort_key\"])\n",
        "        return episodes\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Error getting episodes: {e}\")\n",
        "        return []\n",
        "\n",
        "# ================================================================\n",
        "# Server selection\n",
        "# ================================================================\n",
        "\n",
        "def normalize(s: str) -> str:\n",
        "    return s.strip().lower()\n",
        "\n",
        "def server_matches_pref(server_name: str, preferred: str) -> bool:\n",
        "    s = normalize(server_name)\n",
        "    p = normalize(preferred)\n",
        "    return p in s or s in p\n",
        "\n",
        "def get_video_servers(token: str) -> List[Dict[str, str]]:\n",
        "    try:\n",
        "        enc = enc_kai(token)\n",
        "        if not enc:\n",
        "            return []\n",
        "        url = f\"{BASE_URL}/ajax/links/list?token={token}&_={enc}\"\n",
        "        r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        html = data.get(\"result\", \"\")\n",
        "        if not html:\n",
        "            return []\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "        servers: List[Dict[str, str]] = []\n",
        "        for type_div in soup.select(\"div.server-items[data-id]\"):\n",
        "            type_id = type_div.get(\"data-id\", \"\")\n",
        "            for server in type_div.select(\"span.server[data-lid]\"):\n",
        "                server_id = server.get(\"data-lid\", \"\")\n",
        "                server_name = server.text.strip()\n",
        "                servers.append(\n",
        "                    {\"type\": type_id, \"server_id\": server_id, \"server_name\": server_name}\n",
        "                )\n",
        "        return servers\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Error getting servers: {e}\")\n",
        "        return []\n",
        "\n",
        "def choose_server(\n",
        "    servers: List[Dict[str, str]],\n",
        "    prefer_type_label: str,\n",
        "    prefer_server_name: str,\n",
        ") -> Optional[Dict[str, str]]:\n",
        "    type_map = {\n",
        "        \"Hard Sub\":        \"sub\",\n",
        "        \"Soft Sub\":        \"softsub\",\n",
        "        \"Dub (with subs)\": \"dub\",\n",
        "    }\n",
        "    prefer_type_id = type_map.get(prefer_type_label, \"softsub\")\n",
        "\n",
        "    if not servers:\n",
        "        return None\n",
        "\n",
        "    cand = [\n",
        "        s for s in servers\n",
        "        if s[\"type\"] == prefer_type_id and server_matches_pref(s[\"server_name\"], prefer_server_name)\n",
        "    ]\n",
        "    if cand:\n",
        "        return cand[0]\n",
        "\n",
        "    cand = [s for s in servers if server_matches_pref(s[\"server_name\"], prefer_server_name)]\n",
        "    if cand:\n",
        "        return cand[0]\n",
        "\n",
        "    cand = [s for s in servers if s[\"type\"] == prefer_type_id]\n",
        "    if cand:\n",
        "        return cand[0]\n",
        "\n",
        "    return servers[0]\n",
        "\n",
        "# ================================================================\n",
        "# Resolve video URL with subtitle tracks\n",
        "# ================================================================\n",
        "\n",
        "def get_video_data(server_id: str) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Returns dict with:\n",
        "      - video_url: main video URL\n",
        "      - subtitles: list of subtitle tracks [{'url': ..., 'lang': ...}]\n",
        "    \"\"\"\n",
        "    try:\n",
        "        enc = enc_kai(server_id)\n",
        "        if not enc:\n",
        "            return None\n",
        "        url = f\"{BASE_URL}/ajax/links/view?id={server_id}&_={enc}\"\n",
        "        r = scraper.get(url, headers=HEADERS, timeout=30)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        encoded_link = data.get(\"result\", \"\")\n",
        "        if not encoded_link:\n",
        "            return None\n",
        "\n",
        "        dec = dec_kai(encoded_link)\n",
        "        if not dec:\n",
        "            return None\n",
        "        iframe_url = dec.get(\"url\", \"\")\n",
        "        if not iframe_url:\n",
        "            return None\n",
        "\n",
        "        parsed = urlparse(iframe_url)\n",
        "        token = parsed.path.split(\"/\")[-1]\n",
        "        media_url = f\"{parsed.scheme}://{parsed.netloc}/media/{token}\"\n",
        "        r2 = scraper.get(media_url, headers=HEADERS, timeout=30)\n",
        "        r2.raise_for_status()\n",
        "        j2 = r2.json()\n",
        "        mega_token = j2.get(\"result\", \"\")\n",
        "        if not mega_token:\n",
        "            return None\n",
        "\n",
        "        mega = dec_mega(mega_token, HEADERS[\"User-Agent\"])\n",
        "        if not mega:\n",
        "            return None\n",
        "\n",
        "        sources = mega.get(\"sources\", [])\n",
        "        if not sources:\n",
        "            return None\n",
        "\n",
        "        video_url = sources[0].get(\"file\", \"\")\n",
        "\n",
        "        # Extract subtitle tracks (VTT only)\n",
        "        subtitle_tracks = []\n",
        "        tracks = mega.get(\"tracks\", [])\n",
        "        for track in tracks:\n",
        "            if track.get(\"kind\") == \"captions\" and track.get(\"file\", \"\").endswith(\".vtt\"):\n",
        "                subtitle_tracks.append({\n",
        "                    \"url\": track[\"file\"],\n",
        "                    \"lang\": track.get(\"label\", \"Unknown\")\n",
        "                })\n",
        "\n",
        "        return {\n",
        "            \"video_url\": video_url,\n",
        "            \"subtitles\": subtitle_tracks\n",
        "        }\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Error getting video data: {e}\")\n",
        "        return None\n",
        "\n",
        "# ================================================================\n",
        "# Download with subtitle embedding\n",
        "# ================================================================\n",
        "\n",
        "def download_with_ytdlp(url: str, output_file: str, episode_label: str, subtitles: List[Dict] = None) -> bool:\n",
        "    \"\"\"Download with yt-dlp, embedding subtitles if available.\"\"\"\n",
        "    try:\n",
        "        log(\"INFO\", f\"Using yt-dlp -> {os.path.basename(output_file)}\")\n",
        "\n",
        "        # Base command\n",
        "        cmd = [\n",
        "            \"yt-dlp\",\n",
        "            url,\n",
        "            \"-o\",\n",
        "            output_file,\n",
        "            \"--no-warnings\",\n",
        "            \"--no-check-certificate\",\n",
        "            \"--concurrent-fragments\",\n",
        "            str(max_workers),\n",
        "            \"--retries\",\n",
        "            str(RETRY_CONFIG[\"max_retries\"]),\n",
        "            \"--fragment-retries\",\n",
        "            str(RETRY_CONFIG[\"max_retries\"]),\n",
        "            \"--socket-timeout\",\n",
        "            str(RETRY_CONFIG[\"timeout\"]),\n",
        "            \"--user-agent\",\n",
        "            HEADERS[\"User-Agent\"],\n",
        "            \"--referer\",\n",
        "            BASE_URL,\n",
        "            \"--newline\",\n",
        "        ]\n",
        "\n",
        "        # Add subtitle handling if available\n",
        "        if subtitles:\n",
        "            log(\"INFO\", f\"Found {len(subtitles)} subtitle track(s)\")\n",
        "            # Download video and subs separately, then merge\n",
        "            temp_video = output_file.replace(\".mp4\", \"_temp.mp4\")\n",
        "            cmd_copy = cmd.copy()\n",
        "            cmd_copy[cmd_copy.index(\"-o\") + 1] = temp_video\n",
        "\n",
        "            # Download video first\n",
        "            proc = subprocess.Popen(\n",
        "                cmd_copy,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.STDOUT,\n",
        "                universal_newlines=True,\n",
        "                bufsize=1,\n",
        "            )\n",
        "            with tqdm(\n",
        "                total=100,\n",
        "                unit=\"%\",\n",
        "                desc=f\"Ep {episode_label} (video)\",\n",
        "                bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {elapsed}\",\n",
        "            ) as pbar:\n",
        "                last = 0.0\n",
        "                for line in proc.stdout:\n",
        "                    m = re.search(r\"\\[download\\]\\s+(\\d+\\.?\\d*)%\", line)\n",
        "                    if m:\n",
        "                        cur = float(m.group(1))\n",
        "                        delta = cur - last\n",
        "                        if delta > 0:\n",
        "                            pbar.update(delta)\n",
        "                            last = cur\n",
        "            proc.wait()\n",
        "\n",
        "            if proc.returncode != 0 or not os.path.exists(temp_video):\n",
        "                log(\"ERROR\", f\"Video download failed\")\n",
        "                return False\n",
        "\n",
        "            # Download subtitles\n",
        "            sub_files = []\n",
        "            for idx, sub in enumerate(subtitles):\n",
        "                sub_path = output_file.replace(\".mp4\", f\"_sub{idx}.vtt\")\n",
        "                try:\n",
        "                    log(\"INFO\", f\"Downloading subtitle: {sub['lang']}\")\n",
        "                    r = scraper.get(sub['url'], headers=HEADERS, timeout=30)\n",
        "                    r.raise_for_status()\n",
        "                    with open(sub_path, 'wb') as f:\n",
        "                        f.write(r.content)\n",
        "                    sub_files.append((sub_path, sub['lang']))\n",
        "                except Exception as e:\n",
        "                    log(\"WARN\", f\"Failed to download subtitle {sub['lang']}: {e}\")\n",
        "\n",
        "            # Merge video + subtitles with ffmpeg\n",
        "            if sub_files:\n",
        "                log(\"INFO\", f\"Embedding {len(sub_files)} subtitle(s)...\")\n",
        "                ffmpeg_cmd = [\"ffmpeg\", \"-i\", temp_video]\n",
        "\n",
        "                # Add subtitle inputs\n",
        "                for sub_file, _ in sub_files:\n",
        "                    ffmpeg_cmd.extend([\"-i\", sub_file])\n",
        "\n",
        "                # Map video and audio\n",
        "                ffmpeg_cmd.extend([\"-map\", \"0:v\", \"-map\", \"0:a\"])\n",
        "\n",
        "                # Map and set metadata for each subtitle\n",
        "                for idx, (_, lang) in enumerate(sub_files, 1):\n",
        "                    ffmpeg_cmd.extend([\n",
        "                        \"-map\", f\"{idx}:0\",\n",
        "                        f\"-metadata:s:s:{idx-1}\", f\"language={lang[:3].lower()}\",\n",
        "                        f\"-metadata:s:s:{idx-1}\", f\"title={lang}\"\n",
        "                    ])\n",
        "\n",
        "                # Output settings\n",
        "                ffmpeg_cmd.extend([\n",
        "                    \"-c:v\", \"copy\",\n",
        "                    \"-c:a\", \"copy\",\n",
        "                    \"-c:s\", \"mov_text\",  # MP4-compatible subtitle codec\n",
        "                    \"-y\",\n",
        "                    output_file\n",
        "                ])\n",
        "\n",
        "                result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n",
        "\n",
        "                # Cleanup\n",
        "                try:\n",
        "                    os.remove(temp_video)\n",
        "                    for sub_file, _ in sub_files:\n",
        "                        os.remove(sub_file)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                if result.returncode == 0 and os.path.exists(output_file):\n",
        "                    log(\"INFO\", f\"âœ… Download complete with subtitles: {os.path.basename(output_file)}\")\n",
        "                    return True\n",
        "                else:\n",
        "                    log(\"WARN\", \"Subtitle embedding failed, keeping video only\")\n",
        "                    if os.path.exists(temp_video):\n",
        "                        shutil.move(temp_video, output_file)\n",
        "                    return os.path.exists(output_file)\n",
        "            else:\n",
        "                # No subtitles downloaded, just rename temp file\n",
        "                shutil.move(temp_video, output_file)\n",
        "                return True\n",
        "\n",
        "        else:\n",
        "            # No subtitles, standard download\n",
        "            proc = subprocess.Popen(\n",
        "                cmd,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.STDOUT,\n",
        "                universal_newlines=True,\n",
        "                bufsize=1,\n",
        "            )\n",
        "            with tqdm(\n",
        "                total=100,\n",
        "                unit=\"%\",\n",
        "                desc=f\"Ep {episode_label}\",\n",
        "                bar_format=\"{desc}: {percentage:3.0f}%|{bar}| {elapsed}\",\n",
        "            ) as pbar:\n",
        "                last = 0.0\n",
        "                for line in proc.stdout:\n",
        "                    m = re.search(r\"\\[download\\]\\s+(\\d+\\.?\\d*)%\", line)\n",
        "                    if m:\n",
        "                        cur = float(m.group(1))\n",
        "                        delta = cur - last\n",
        "                        if delta > 0:\n",
        "                            pbar.update(delta)\n",
        "                            last = cur\n",
        "            proc.wait()\n",
        "            if proc.returncode == 0 and os.path.exists(output_file):\n",
        "                log(\"INFO\", f\"âœ… Download complete: {os.path.basename(output_file)}\")\n",
        "                return True\n",
        "            log(\"ERROR\", f\"yt-dlp failed with code {proc.returncode}\")\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"yt-dlp error: {e}\")\n",
        "        return False\n",
        "\n",
        "def download_direct(url: str, output_file: str, episode_label: str) -> bool:\n",
        "    log(\"INFO\", f\"Using direct HTTP -> {os.path.basename(output_file)}\")\n",
        "    try:\n",
        "        with scraper.get(url, headers=HEADERS, stream=True, timeout=RETRY_CONFIG[\"timeout\"]) as r:\n",
        "            r.raise_for_status()\n",
        "            total = int(r.headers.get(\"Content-Length\", 0))\n",
        "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "            with open(output_file, \"wb\") as f:\n",
        "                if total > 0:\n",
        "                    with tqdm(total=total, unit=\"B\", unit_scale=True, desc=f\"Ep {episode_label}\") as pbar:\n",
        "                        for chunk in r.iter_content(chunk_size=8192):\n",
        "                            if not chunk:\n",
        "                                continue\n",
        "                            f.write(chunk)\n",
        "                            pbar.update(len(chunk))\n",
        "                else:\n",
        "                    for chunk in r.iter_content(chunk_size=8192):\n",
        "                        if chunk:\n",
        "                            f.write(chunk)\n",
        "        log(\"INFO\", f\"âœ… Download complete: {os.path.basename(output_file)}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Direct download error: {e}\")\n",
        "        return False\n",
        "\n",
        "def is_m3u8_url(url: str) -> bool:\n",
        "    if \".m3u8\" in url.split(\"?\")[0]:\n",
        "        return True\n",
        "    try:\n",
        "        r = scraper.head(url, headers=HEADERS, timeout=10)\n",
        "        ctype = r.headers.get(\"Content-Type\", \"\")\n",
        "        return \"application/vnd.apple.mpegurl\" in ctype or \"application/x-mpegURL\" in ctype\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def download_episode(video_data: Dict[str, Any], output_file: str, episode_label: str) -> bool:\n",
        "    \"\"\"Download episode with subtitle support.\"\"\"\n",
        "    url = video_data[\"video_url\"]\n",
        "    subtitles = video_data.get(\"subtitles\", [])\n",
        "\n",
        "    use_m3u8 = is_m3u8_url(url)\n",
        "    if download_method == \"yt-dlp\":\n",
        "        primary = lambda u, o: download_with_ytdlp(u, o, episode_label, subtitles)\n",
        "        fallback = lambda u, o: download_direct(u, o, episode_label)\n",
        "    else:\n",
        "        primary = lambda u, o: download_direct(u, o, episode_label)\n",
        "        fallback = lambda u, o: download_direct(u, o, episode_label)\n",
        "\n",
        "    for attempt in range(1, RETRY_CONFIG[\"max_retries\"] + 1):\n",
        "        if os.path.exists(output_file):\n",
        "            os.remove(output_file)\n",
        "        if attempt > 1:\n",
        "            log(\"INFO\", f\"Retry {attempt}/{RETRY_CONFIG['max_retries']} for {os.path.basename(output_file)}\")\n",
        "        if primary(url, output_file):\n",
        "            return True\n",
        "        time.sleep(RETRY_CONFIG[\"sleep_between\"])\n",
        "        if attempt == RETRY_CONFIG[\"max_retries\"] - 1:\n",
        "            log(\"WARN\", \"Primary method failing; trying fallback.\")\n",
        "            if fallback(url, output_file):\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# ================================================================\n",
        "# Merge\n",
        "# ================================================================\n",
        "\n",
        "def merge_videos(\n",
        "    file_list: List[str],\n",
        "    anime_title: str,\n",
        "    season_num: int,\n",
        "    first_ep_id: str,\n",
        "    last_ep_id: str,\n",
        ") -> Optional[str]:\n",
        "    if not file_list:\n",
        "        log(\"ERROR\", \"No files to merge.\")\n",
        "        return None\n",
        "\n",
        "    valid_files = [f for f in file_list if os.path.exists(f)]\n",
        "    if len(valid_files) != len(file_list):\n",
        "        log(\"ERROR\", \"Some input files for merging are missing.\")\n",
        "        return None\n",
        "\n",
        "    merged_filename = f\"{anime_title} Season {season_num:02d} Episodes {first_ep_id}-{last_ep_id}.mp4\"\n",
        "    merged_filename = re.sub(r'[<>:\"/\\\\|?*]', \"\", merged_filename)\n",
        "    merged_path = os.path.join(os.path.dirname(file_list[0]), merged_filename)\n",
        "\n",
        "    log(\"INFO\", f\"Merging {len(valid_files)} files into {merged_filename}\")\n",
        "\n",
        "    list_file = os.path.join(os.path.dirname(file_list[0]), \"filelist_merge.txt\")\n",
        "    try:\n",
        "        with open(list_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            for vf in valid_files:\n",
        "                f.write(f\"file '{os.path.abspath(vf)}'\\n\")\n",
        "\n",
        "        cmd = [\n",
        "            \"ffmpeg\",\n",
        "            \"-f\", \"concat\",\n",
        "            \"-safe\", \"0\",\n",
        "            \"-i\", list_file,\n",
        "            \"-c:v\", \"copy\",\n",
        "            \"-c:a\", \"copy\",\n",
        "            \"-c:s\", \"copy\",\n",
        "            \"-y\",\n",
        "            \"-loglevel\", \"info\",\n",
        "            merged_path,\n",
        "        ]\n",
        "        proc = subprocess.Popen(\n",
        "            cmd,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            universal_newlines=True,\n",
        "        )\n",
        "\n",
        "        print(\"Merging...\", end=\"\", flush=True)\n",
        "        ffmpeg_err = []\n",
        "        for line in proc.stderr:\n",
        "            ffmpeg_err.append(line.rstrip())\n",
        "            if \"time=\" in line:\n",
        "                print(\".\", end=\"\", flush=True)\n",
        "        proc.wait()\n",
        "        print()\n",
        "\n",
        "        if proc.returncode != 0 or not os.path.exists(merged_path):\n",
        "            log(\"ERROR\", \"ffmpeg merge failed. Last lines:\")\n",
        "            for l in ffmpeg_err[-10:]:\n",
        "                print(l)\n",
        "            return None\n",
        "\n",
        "        log(\"INFO\", f\"âœ… Merged: {merged_filename}\")\n",
        "        return merged_path\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Merge error: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        try:\n",
        "            if os.path.exists(list_file):\n",
        "                os.remove(list_file)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# ================================================================\n",
        "# Upload helpers\n",
        "# ================================================================\n",
        "\n",
        "def upload_to_gofile(filepath: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Upload file to GoFile.io\n",
        "    Limits: Unlimited file size, no account needed, links expire after inactivity\n",
        "    \"\"\"\n",
        "    try:\n",
        "        filename = os.path.basename(filepath)\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        log(\"INFO\", f\"Uploading to GoFile: {filename} ({size_mb:.2f} MB)\")\n",
        "\n",
        "        server = None\n",
        "        for attempt in range(1, 6):\n",
        "            try:\n",
        "                r = requests.get(\"https://api.gofile.io/servers\", timeout=15)\n",
        "                data = r.json()\n",
        "\n",
        "                if data['status'] == 'ok' and data['data']['servers']:\n",
        "                    server = data['data']['servers'][0]['name']\n",
        "                    break\n",
        "                else:\n",
        "                    log(\"WARN\", f\"GoFile busy (Attempt {attempt}/5)... Waiting 3s.\")\n",
        "            except Exception:\n",
        "                pass\n",
        "            time.sleep(3)\n",
        "\n",
        "        if not server:\n",
        "            log(\"ERROR\", \"âŒ GoFile upload failed: No servers available currently.\")\n",
        "            return None\n",
        "\n",
        "        upload_url = f\"https://{server}.gofile.io/contents/uploadfile\"\n",
        "        with open(filepath, \"rb\") as f:\n",
        "            with tqdm(total=size_mb, unit=\"MB\", desc=\"GoFile Upload\") as pbar:\n",
        "\n",
        "                class ProgressFile:\n",
        "                    def __init__(self, file_obj, pbar):\n",
        "                        self.f = file_obj\n",
        "                        self.pbar = pbar\n",
        "\n",
        "                    def read(self, size=-1):\n",
        "                        data = self.f.read(size)\n",
        "                        if not data: return data\n",
        "                        self.pbar.update(len(data) / (1024 * 1024))\n",
        "                        return data\n",
        "\n",
        "                    def __getattr__(self, name):\n",
        "                        return getattr(self.f, name)\n",
        "\n",
        "                pf = ProgressFile(f, pbar)\n",
        "                resp = requests.post(upload_url, files={\"file\": (filename, pf)}, timeout=7200)\n",
        "\n",
        "        j = resp.json()\n",
        "        if j.get(\"status\") == \"ok\":\n",
        "            link = j[\"data\"][\"downloadPage\"]\n",
        "            log(\"INFO\", f\"âœ… GoFile link: {link}\")\n",
        "            return link\n",
        "        else:\n",
        "            log(\"ERROR\", f\"GoFile upload failed: {j}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"GoFile upload error: {e}\")\n",
        "        return None\n",
        "\n",
        "def upload_to_pixeldrain(filepath: str, api_key: str = \"\") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Upload file to Pixeldrain\n",
        "    Limits:\n",
        "    - Anonymous: 10GB per file\n",
        "    - With free account: 20GB per file\n",
        "    - Files don't expire with account\n",
        "    \"\"\"\n",
        "    try:\n",
        "        filename = os.path.basename(filepath)\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        size_gb = size_mb / 1024\n",
        "\n",
        "        # Check file size limit\n",
        "        limit_gb = 20 if api_key else 10\n",
        "        if size_gb > limit_gb:\n",
        "            log(\"ERROR\", f\"âŒ File too large for Pixeldrain ({size_gb:.2f}GB > {limit_gb}GB limit)\")\n",
        "            if not api_key:\n",
        "                log(\"INFO\", \"ðŸ’¡ Tip: Add Pixeldrain API key to increase limit to 20GB\")\n",
        "            return None\n",
        "\n",
        "        log(\"INFO\", f\"Uploading to Pixeldrain: {filename} ({size_mb:.2f} MB)\")\n",
        "        if api_key:\n",
        "            log(\"INFO\", \"ðŸ”‘ Using API key (20GB limit)\")\n",
        "        else:\n",
        "            log(\"INFO\", \"âš ï¸  Anonymous upload (10GB limit)\")\n",
        "\n",
        "        upload_url = \"https://pixeldrain.com/api/file\"\n",
        "\n",
        "        # Prepare headers with API key if provided\n",
        "        headers = {}\n",
        "        if api_key:\n",
        "            headers[\"Authorization\"] = f\"Basic {api_key}\"\n",
        "\n",
        "        with open(filepath, \"rb\") as f:\n",
        "            with tqdm(total=size_mb, unit=\"MB\", desc=\"Pixeldrain Upload\") as pbar:\n",
        "\n",
        "                class ProgressFile:\n",
        "                    def __init__(self, file_obj, pbar):\n",
        "                        self.f = file_obj\n",
        "                        self.pbar = pbar\n",
        "                        self.bytes_read = 0\n",
        "\n",
        "                    def read(self, size=-1):\n",
        "                        data = self.f.read(size)\n",
        "                        if data:\n",
        "                            self.bytes_read += len(data)\n",
        "                            self.pbar.update(len(data) / (1024 * 1024))\n",
        "                        return data\n",
        "\n",
        "                    def __len__(self):\n",
        "                        return os.path.getsize(filepath)\n",
        "\n",
        "                    def __getattr__(self, name):\n",
        "                        return getattr(self.f, name)\n",
        "\n",
        "                pf = ProgressFile(f, pbar)\n",
        "\n",
        "                # Upload with or without API key\n",
        "                if api_key:\n",
        "                    resp = requests.post(\n",
        "                        upload_url,\n",
        "                        files={\"file\": (filename, pf)},\n",
        "                        headers=headers,\n",
        "                        timeout=7200\n",
        "                    )\n",
        "                else:\n",
        "                    resp = requests.post(\n",
        "                        upload_url,\n",
        "                        files={\"file\": (filename, pf)},\n",
        "                        timeout=7200\n",
        "                    )\n",
        "\n",
        "        resp.raise_for_status()\n",
        "        j = resp.json()\n",
        "\n",
        "        if j.get(\"success\"):\n",
        "            file_id = j[\"id\"]\n",
        "            link = f\"https://pixeldrain.com/u/{file_id}\"\n",
        "            log(\"INFO\", f\"âœ… Pixeldrain link: {link}\")\n",
        "            return link\n",
        "        else:\n",
        "            log(\"ERROR\", f\"Pixeldrain upload failed: {j}\")\n",
        "            return None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        log(\"ERROR\", f\"Pixeldrain upload error: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"Pixeldrain upload error: {e}\")\n",
        "        return None\n",
        "\n",
        "def upload_to_gdrive(filepath: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Upload file to Google Drive\n",
        "    Limits: 15GB free storage (shared with Gmail and Photos)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "\n",
        "        if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "            log(\"INFO\", \"Mounting Google Drive...\")\n",
        "            drive.mount(\"/content/drive\")\n",
        "\n",
        "        dest_dir = \"/content/drive/MyDrive/AnimeKai_Downloads/\"\n",
        "        os.makedirs(dest_dir, exist_ok=True)\n",
        "        filename = os.path.basename(filepath)\n",
        "        dest_path = os.path.join(dest_dir, filename)\n",
        "\n",
        "        total = os.path.getsize(filepath)\n",
        "        log(\"INFO\", f\"Copying to GDrive: {filename}\")\n",
        "        with open(filepath, \"rb\") as src, open(dest_path, \"wb\") as dst:\n",
        "            with tqdm(total=total, unit=\"B\", unit_scale=True, desc=\"Uploading to GDrive\") as pbar:\n",
        "                while True:\n",
        "                    chunk = src.read(8192)\n",
        "                    if not chunk:\n",
        "                        break\n",
        "                    dst.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "\n",
        "        log(\"INFO\", f\"âœ… Uploaded to {dest_path}\")\n",
        "        return dest_path\n",
        "    except Exception as e:\n",
        "        log(\"ERROR\", f\"GDrive upload error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ================================================================\n",
        "# Filename generator\n",
        "# ================================================================\n",
        "\n",
        "def generate_episode_filename(anime_title: str, season_num: int, ep_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate proper episode filename: Anime Name Season xx Episode xx.mp4\n",
        "    Example: My Dress Up Darling Season 02 Episode 01.mp4\n",
        "    \"\"\"\n",
        "    # Pad episode number if it's a simple integer\n",
        "    try:\n",
        "        ep_num = float(ep_id)\n",
        "        if ep_num == int(ep_num):\n",
        "            # Simple integer episode (1, 2, 3...)\n",
        "            ep_formatted = f\"{int(ep_num):02d}\"\n",
        "        else:\n",
        "            # Decimal episode (12.5, etc.)\n",
        "            ep_formatted = ep_id\n",
        "    except ValueError:\n",
        "        # Special episodes (SP, OVA, etc.)\n",
        "        ep_formatted = ep_id\n",
        "\n",
        "    filename = f\"{anime_title} Season {season_num:02d} Episode {ep_formatted}.mp4\"\n",
        "    # Remove invalid characters\n",
        "    filename = re.sub(r'[<>:\"/\\\\|?*]', \"\", filename)\n",
        "    return filename\n",
        "\n",
        "# ================================================================\n",
        "# main()\n",
        "# ================================================================\n",
        "\n",
        "def parse_episode_id(ep_id: str) -> Tuple[int, float]:\n",
        "    return safe_episode_key(ep_id)\n",
        "\n",
        "def in_episode_range(ep_id: str, start_id: str, end_id: str) -> bool:\n",
        "    s_key = parse_episode_id(start_id)\n",
        "    e_key = parse_episode_id(end_id)\n",
        "    k = parse_episode_id(ep_id)\n",
        "    return s_key <= k <= e_key\n",
        "\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ðŸŽ¬ ANIMEKAI EPISODE DOWNLOADER & MERGER\")\n",
        "    print(\"   (Enhanced with Pixeldrain Support)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    log(\"INFO\", f\"Processing: {anime_url}\")\n",
        "\n",
        "    anime_id, anime_title = get_anime_details(anime_url)\n",
        "    if not anime_id:\n",
        "        raise RuntimeError(\"Could not extract anime ID â€“ check URL or site changes.\")\n",
        "\n",
        "    log(\"INFO\", f\"Anime ID: {anime_id}\")\n",
        "    log(\"INFO\", f\"Title: {anime_title}\")\n",
        "\n",
        "    detected_season = detect_season_from_title(anime_title)\n",
        "    final_season = season_number if season_number > 0 else detected_season\n",
        "    log(\"INFO\", f\"Season: {final_season} ({'auto' if season_number == 0 else 'manual'})\")\n",
        "\n",
        "    episodes = get_episode_list(anime_id)\n",
        "    if not episodes:\n",
        "        raise RuntimeError(\"No episodes found.\")\n",
        "\n",
        "    log(\"INFO\", f\"Found {len(episodes)} episodes\")\n",
        "\n",
        "    if download_mode == \"Single Episode\":\n",
        "        target = single_episode.strip()\n",
        "        selected = [ep for ep in episodes if ep[\"id\"] == target]\n",
        "    elif download_mode == \"Episode Range\":\n",
        "        if parse_episode_id(start_episode) > parse_episode_id(end_episode):\n",
        "            raise ValueError(f\"Invalid episode range: {start_episode} > {end_episode}\")\n",
        "        selected = [ep for ep in episodes if in_episode_range(ep[\"id\"], start_episode, end_episode)]\n",
        "    else:\n",
        "        selected = episodes\n",
        "\n",
        "    if not selected:\n",
        "        raise RuntimeError(\"No episodes match your selection.\")\n",
        "\n",
        "    log(\"INFO\", f\"Will download {len(selected)} episode(s)\")\n",
        "\n",
        "    download_dir = os.path.join(\"downloads\", anime_title)\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    log(\"INFO\", f\"Download directory: {download_dir}\")\n",
        "\n",
        "    downloaded_files: List[str] = []\n",
        "    failed_episodes: List[str] = []\n",
        "\n",
        "    for idx, ep in enumerate(selected, 1):\n",
        "        ep_id = ep[\"id\"]\n",
        "        print(\"\\n\" + \"-\" * 50)\n",
        "        log(\"INFO\", f\"[{idx}/{len(selected)}] Episode {ep_id}\")\n",
        "\n",
        "        servers = get_video_servers(ep[\"token\"])\n",
        "        if not servers:\n",
        "            log(\"ERROR\", \"No servers available for this episode.\")\n",
        "            failed_episodes.append(ep_id)\n",
        "            continue\n",
        "\n",
        "        server = choose_server(servers, prefer_type, prefer_server)\n",
        "        if not server:\n",
        "            log(\"ERROR\", \"Could not choose any server.\")\n",
        "            failed_episodes.append(ep_id)\n",
        "            continue\n",
        "\n",
        "        log(\"INFO\", f\"Using server: {server['server_name']} (type={server['type']})\")\n",
        "\n",
        "        # Get video data with subtitles\n",
        "        video_data = get_video_data(server[\"server_id\"])\n",
        "        if not video_data:\n",
        "            log(\"ERROR\", \"Could not resolve video data.\")\n",
        "            failed_episodes.append(ep_id)\n",
        "            continue\n",
        "\n",
        "        # Generate proper filename\n",
        "        filename = generate_episode_filename(anime_title, final_season, ep_id)\n",
        "        filepath = os.path.join(download_dir, filename)\n",
        "\n",
        "        log(\"INFO\", f\"Output filename: {filename}\")\n",
        "\n",
        "        if download_episode(video_data, filepath, ep_id):\n",
        "            downloaded_files.append(filepath)\n",
        "        else:\n",
        "            log(\"ERROR\", \"All download attempts failed for this episode.\")\n",
        "            failed_episodes.append(ep_id)\n",
        "        time.sleep(1)\n",
        "\n",
        "    merged_video = None\n",
        "    # Only merge if multiple episodes AND merge is enabled\n",
        "    if merge_episodes and len(downloaded_files) > 1:\n",
        "        # Create mapping with proper filenames\n",
        "        file_by_ep = {}\n",
        "        for ep in selected:\n",
        "            filename = generate_episode_filename(anime_title, final_season, ep[\"id\"])\n",
        "            filepath = os.path.join(download_dir, filename)\n",
        "            file_by_ep[ep[\"id\"]] = filepath\n",
        "\n",
        "        ordered_files = [file_by_ep[ep[\"id\"]] for ep in selected if os.path.exists(file_by_ep[ep[\"id\"]])]\n",
        "\n",
        "        first_ep_id = selected[0][\"id\"]\n",
        "        last_ep_id = selected[-1][\"id\"]\n",
        "        merged_video = merge_videos(ordered_files, anime_title, final_season, first_ep_id, last_ep_id)\n",
        "\n",
        "        if merged_video and not keep_individual_files:\n",
        "            log(\"INFO\", \"Removing individual episode files after merge.\")\n",
        "            for fpath in ordered_files:\n",
        "                try:\n",
        "                    os.remove(fpath)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ðŸ“Š DOWNLOAD SUMMARY\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nâœ… Successfully downloaded: {len(downloaded_files)} episode(s)\")\n",
        "    if failed_episodes:\n",
        "        print(f\"âŒ Failed episodes: {', '.join(failed_episodes)}\")\n",
        "\n",
        "    if merged_video:\n",
        "        size_mb = os.path.getsize(merged_video) / (1024 * 1024)\n",
        "        print(f\"\\nðŸ”— Merged file: {os.path.basename(merged_video)} ({size_mb:.2f} MB)\")\n",
        "        if not keep_individual_files:\n",
        "            print(\"   Individual episode files were deleted.\")\n",
        "    elif downloaded_files:\n",
        "        total = sum(os.path.getsize(f) for f in downloaded_files if os.path.exists(f)) / (1024 * 1024)\n",
        "        print(f\"\\nðŸ’¾ Total size of downloaded episodes: {total:.2f} MB\")\n",
        "\n",
        "    print(f\"\\nðŸ“ Local files location: {download_dir}\")\n",
        "\n",
        "    # Show downloaded filenames\n",
        "    if downloaded_files:\n",
        "        print(\"\\nðŸ“ Downloaded files:\")\n",
        "        for fpath in downloaded_files:\n",
        "            if os.path.exists(fpath):\n",
        "                print(f\"   â€¢ {os.path.basename(fpath)}\")\n",
        "\n",
        "    # Upload logic\n",
        "    files_to_upload: List[str] = []\n",
        "\n",
        "    # Determine which files to upload\n",
        "    if len(downloaded_files) == 1:\n",
        "        # Single episode - always upload it\n",
        "        files_to_upload = downloaded_files\n",
        "        log(\"INFO\", \"Single episode detected - will upload individual file\")\n",
        "    elif merge_episodes and len(downloaded_files) > 1:\n",
        "        # Multiple episodes with merge enabled\n",
        "        if upload_merged_only and merged_video:\n",
        "            # Only upload merged file\n",
        "            files_to_upload = [merged_video]\n",
        "            log(\"INFO\", \"Upload merged only - will upload merged file\")\n",
        "        elif merged_video:\n",
        "            # Upload merged + individuals if kept\n",
        "            files_to_upload = [merged_video]\n",
        "            if keep_individual_files:\n",
        "                files_to_upload.extend([f for f in downloaded_files if os.path.exists(f)])\n",
        "            log(\"INFO\", f\"Will upload merged file + {len(files_to_upload)-1} individual files\")\n",
        "        else:\n",
        "            # Merge failed, upload individuals\n",
        "            files_to_upload = [f for f in downloaded_files if os.path.exists(f)]\n",
        "            log(\"INFO\", \"Merge failed - will upload individual files\")\n",
        "    else:\n",
        "        # Multiple episodes, merge disabled - upload all\n",
        "        files_to_upload = [f for f in downloaded_files if os.path.exists(f)]\n",
        "        log(\"INFO\", f\"Merge disabled - will upload {len(files_to_upload)} individual files\")\n",
        "\n",
        "    gofile_links = []\n",
        "    pixeldrain_links = []\n",
        "    gdrive_paths = []\n",
        "\n",
        "    if files_to_upload and upload_destination != \"None (Keep Local)\":\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"ðŸ“¤ UPLOADING FILES\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"\\nFiles to upload: {len(files_to_upload)}\")\n",
        "\n",
        "        # Display upload service info\n",
        "        print(\"\\nðŸ“‹ Upload Service Information:\")\n",
        "        if upload_destination in [\"GoFile.io Only\", \"GoFile + Pixeldrain\", \"All Services\"]:\n",
        "            print(\"   â€¢ GoFile.io: Unlimited size, no account, links expire after inactivity\")\n",
        "        if upload_destination in [\"Pixeldrain Only\", \"GoFile + Pixeldrain\", \"All Services\"]:\n",
        "            if pixeldrain_api_key:\n",
        "                print(\"   â€¢ Pixeldrain: 20GB/file limit (authenticated), no expiration\")\n",
        "            else:\n",
        "                print(\"   â€¢ Pixeldrain: 10GB/file limit (anonymous), no expiration\")\n",
        "        if upload_destination in [\"Google Drive Only\", \"All Services\"]:\n",
        "            print(\"   â€¢ Google Drive: 15GB total storage (shared with Gmail)\")\n",
        "\n",
        "        for path in files_to_upload:\n",
        "            if not os.path.exists(path):\n",
        "                log(\"WARN\", f\"File not found, skipping: {os.path.basename(path)}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nðŸ“¤ Processing: {os.path.basename(path)}\")\n",
        "\n",
        "            if upload_destination in [\"GoFile.io Only\", \"GoFile + Pixeldrain\", \"All Services\"]:\n",
        "                link = upload_to_gofile(path)\n",
        "                if link:\n",
        "                    gofile_links.append((os.path.basename(path), link))\n",
        "\n",
        "            if upload_destination in [\"Pixeldrain Only\", \"GoFile + Pixeldrain\", \"All Services\"]:\n",
        "                link = upload_to_pixeldrain(path, pixeldrain_api_key)\n",
        "                if link:\n",
        "                    pixeldrain_links.append((os.path.basename(path), link))\n",
        "\n",
        "            if upload_destination in [\"Google Drive Only\", \"All Services\"]:\n",
        "                dpath = upload_to_gdrive(path)\n",
        "                if dpath:\n",
        "                    gdrive_paths.append(os.path.basename(dpath))\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"âœ… UPLOAD COMPLETE\")\n",
        "        print(\"=\" * 70)\n",
        "        if gofile_links:\n",
        "            print(\"\\nðŸ”— GoFile Links:\")\n",
        "            for name, link in gofile_links:\n",
        "                print(f\"   â€¢ {name}: {link}\")\n",
        "        if pixeldrain_links:\n",
        "            print(\"\\nðŸ”— Pixeldrain Links:\")\n",
        "            for name, link in pixeldrain_links:\n",
        "                print(f\"   â€¢ {name}: {link}\")\n",
        "        if gdrive_paths:\n",
        "            print(\"\\nðŸ“ Google Drive files (MyDrive/AnimeKai_Downloads/):\")\n",
        "            for name in gdrive_paths:\n",
        "                print(f\"   â€¢ {name}\")\n",
        "    elif upload_destination == \"None (Keep Local)\":\n",
        "        print(\"\\nðŸ“ Upload disabled - files kept locally only\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"ðŸŽ‰ ALL DONE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nðŸ“º Anime: {anime_title}\")\n",
        "    print(f\"ðŸ“Š Season: {final_season}\")\n",
        "    print(f\"ðŸ“¥ Downloaded: {len(downloaded_files)} episode(s)\")\n",
        "    if failed_episodes:\n",
        "        print(f\"âŒ Failed: {', '.join(failed_episodes)}\")\n",
        "    if merged_video:\n",
        "        print(f\"ðŸ”— Merged file: {os.path.basename(merged_video)}\")\n",
        "    print(f\"\\nðŸ“ Local files: {download_dir}\")\n",
        "\n",
        "    # Show subtitle info if Soft Sub or Dub was selected\n",
        "    if prefer_type in [\"Soft Sub\", \"Dub (with subs)\"]:\n",
        "        print(f\"\\nðŸ’¬ Subtitles: Embedded in video file(s) when available\")\n",
        "        print(\"   (Enable subtitles in your video player's subtitle menu)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "# Run main\n",
        "try:\n",
        "    main()\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"âŒ ERROR\")\n",
        "    print(\"=\" * 70)\n",
        "    log(\"ERROR\", f\"Fatal error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\n\" + \"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "P7PZkspKUd_a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ðŸŽ¬ Video Episode Merger & Uploader - SMART MERGE VERSION\n",
        "# Download ZIP file with video episodes, extract, merge them in order, and upload\n",
        "# âœ… SMART AUDIO LOGIC: Auto-detects when audio copy is safe vs. rebuild required\n",
        "\n",
        "# @title ðŸ”§ **Install Dependencies** { display-mode: \"form\" }\n",
        "print(\"ðŸ“¦ Installing required packages...\")\n",
        "!pip install -q natsort requests tqdm > /dev/null 2>&1\n",
        "!apt-get -qq install -y ffmpeg > /dev/null 2>&1\n",
        "print(\"âœ… All dependencies installed!\\n\")\n",
        "\n",
        "# @title âš™ï¸ **Configuration** { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### ðŸ“¥ Download Settings\n",
        "#@markdown Enter the direct download link (DDL) for your ZIP file:\n",
        "zip_url = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Custom User-Agent (leave default if unsure):\n",
        "user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Chunk size for downloads (MB):\n",
        "chunk_size_mb = 45 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ðŸŽ¬ Merge Settings\n",
        "#@markdown Custom output name (leave empty to auto-detect from episodes):\n",
        "custom_output_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Output container format:\n",
        "output_format = \"MKV\" #@param [\"MP4\", \"MKV\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ðŸ“ Subtitle Settings\n",
        "#@markdown Subtitle handling mode:\n",
        "subtitle_mode = \"Preserve All Subtitles\" #@param [\"Preserve All Subtitles\", \"Ignore All Subtitles\"]\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ðŸ§¹ Other Settings\n",
        "#@markdown Cleanup temporary files after process?\n",
        "cleanup_after = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ðŸ“¤ Upload Settings\n",
        "upload_destination = \"GoFile + Google Drive\" #@param [\"GoFile.io Only\", \"Google Drive Only\", \"GoFile + Google Drive\", \"None (Keep Local Only)\"]\n",
        "\n",
        "print(\"âœ… Configuration set!\")\n",
        "print(f\"ðŸ“ Subtitle mode: {subtitle_mode}\")\n",
        "print(f\"ðŸ“¦ Output format: {output_format}\")\n",
        "\n",
        "# @title ðŸ“¥ **Download ZIP File with Chunked Download** { display-mode: \"form\" }\n",
        "import requests\n",
        "import os\n",
        "import re\n",
        "from urllib.parse import unquote, urlparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"ðŸ”½ Starting chunked download...\")\n",
        "print(f\"ðŸ”— URL: {zip_url[:60]}...\" if len(zip_url) > 60 else f\"ðŸ”— URL: {zip_url}\")\n",
        "\n",
        "headers = {'User-Agent': user_agent}\n",
        "chunk_size = chunk_size_mb * 1024 * 1024\n",
        "\n",
        "try:\n",
        "    response = requests.get(zip_url, headers=headers, stream=True, allow_redirects=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    zip_filename = None\n",
        "    if 'Content-Disposition' in response.headers:\n",
        "        cd = response.headers['Content-Disposition']\n",
        "        filenames = re.findall(r'filename\\*?=[\"\\']?(?:UTF-8\\'\\')?([^\"\\';]+)[\"\\']?', cd)\n",
        "        if filenames:\n",
        "            zip_filename = unquote(filenames[0])\n",
        "            print(f\"ðŸ“‹ Filename from header: {zip_filename}\")\n",
        "\n",
        "    if not zip_filename:\n",
        "        final_url = response.url\n",
        "        url_path = urlparse(final_url).path\n",
        "        zip_filename = os.path.basename(url_path)\n",
        "        zip_filename = unquote(zip_filename)\n",
        "        print(f\"ðŸ“‹ Filename from URL: {zip_filename}\")\n",
        "\n",
        "    if not zip_filename or zip_filename in ['', 'download', 'file']:\n",
        "        url_parts = [p for p in urlparse(zip_url).path.split('/') if p and p != 'download']\n",
        "        if url_parts:\n",
        "            zip_filename = url_parts[-1]\n",
        "            zip_filename = unquote(zip_filename)\n",
        "\n",
        "    if not zip_filename.lower().endswith('.zip'):\n",
        "        if '.' not in zip_filename:\n",
        "            zip_filename += '.zip'\n",
        "        else:\n",
        "            zip_filename = os.path.splitext(zip_filename)[0] + '.zip'\n",
        "\n",
        "    zip_filename = re.sub(r'[<>:\"|?*\\\\]', '_', zip_filename)\n",
        "    zip_filename = re.sub(r'[\\x00-\\x1f]', '', zip_filename)\n",
        "\n",
        "    print(f\"ðŸ’¾ Saving as: {zip_filename}\")\n",
        "\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "    if total_size:\n",
        "        print(f\"ðŸ“Š Total size: {total_size / (1024*1024):.2f} MB\")\n",
        "        print(f\"ðŸ”„ Using {chunk_size_mb}MB chunks for download\")\n",
        "\n",
        "    with open(zip_filename, 'wb') as f:\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True,\n",
        "                  desc=zip_filename[:30], ncols=100) as pbar:\n",
        "            for chunk in response.iter_content(chunk_size=chunk_size):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "\n",
        "    downloaded_size = os.path.getsize(zip_filename)\n",
        "    print(f\"\\nâœ… Downloaded: {zip_filename} ({downloaded_size / (1024*1024):.2f} MB)\")\n",
        "\n",
        "    ZIP_BASE_NAME = os.path.splitext(zip_filename)[0]\n",
        "    ZIP_BASE_NAME = re.sub(r'[_\\-\\s]+', ' ', ZIP_BASE_NAME).strip()\n",
        "\n",
        "    print(f\"ðŸ“¦ Base name extracted: '{ZIP_BASE_NAME}'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Download failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# @title ðŸ“¦ **Safe Extract ZIP File** { display-mode: \"form\" }\n",
        "import zipfile\n",
        "\n",
        "extract_folder = \"extracted_videos\"\n",
        "os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "print(f\"\\nðŸ“‚ Extracting to: {extract_folder}/\")\n",
        "\n",
        "def is_safe_path(base_path, target_path):\n",
        "    base_real = os.path.realpath(base_path)\n",
        "    target_real = os.path.realpath(target_path)\n",
        "    return target_real.startswith(base_real)\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        file_list = zip_ref.namelist()\n",
        "        total_files = len(file_list)\n",
        "\n",
        "        print(f\"ðŸ“‹ Found {total_files} file(s) in ZIP\\n\")\n",
        "\n",
        "        extracted_count = 0\n",
        "        blocked_count = 0\n",
        "\n",
        "        with tqdm(total=total_files, desc=\"Extracting\", unit=\"file\") as pbar:\n",
        "            for file in file_list:\n",
        "                extract_path = os.path.join(extract_folder, file)\n",
        "\n",
        "                if not is_safe_path(extract_folder, extract_path):\n",
        "                    blocked_count += 1\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "\n",
        "                zip_ref.extract(file, extract_folder)\n",
        "                extracted_count += 1\n",
        "                pbar.update(1)\n",
        "\n",
        "        print(f\"\\nâœ… Extraction complete: {extracted_count} files\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Extraction failed: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# @title ðŸ” **Detect and Sort Episodes** { display-mode: \"form\" }\n",
        "\n",
        "def extract_episode_info(filename):\n",
        "    name = os.path.basename(filename)\n",
        "\n",
        "    combined_patterns = [\n",
        "        r'[Ss](\\d+)[Ee](\\d+)',\n",
        "        r'[Ss]eason[\\s._-]*(\\d+)[\\s._-]*[Ee]pisode[\\s._-]*(\\d+)',\n",
        "        r'(\\d+)[xX](\\d+)',\n",
        "    ]\n",
        "\n",
        "    for pattern in combined_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            season, episode = int(match.group(1)), int(match.group(2))\n",
        "            if 1 <= episode <= 100:\n",
        "                return season, episode\n",
        "\n",
        "    episode_patterns = [\n",
        "        r'[Ee]pisode[\\s._-]*(\\d+)',\n",
        "        r'[Ee][Pp][\\s._-]*(\\d+)',\n",
        "        r'[Ee](\\d{2,3})(?!\\d)',\n",
        "    ]\n",
        "\n",
        "    season = 1\n",
        "    episode = 0\n",
        "\n",
        "    for pattern in episode_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            ep_num = int(match.group(1))\n",
        "            if ep_num not in [720, 1080, 480, 360, 240, 2160, 1440, 265, 264]:\n",
        "                if 1 <= ep_num <= 100:\n",
        "                    episode = ep_num\n",
        "                    break\n",
        "\n",
        "    return season, episode\n",
        "\n",
        "video_extensions = ['.mp4', '.mkv', '.avi', '.mov', '.flv', '.wmv', '.webm', '.m4v']\n",
        "video_files = []\n",
        "\n",
        "for root, dirs, files in os.walk(extract_folder):\n",
        "    for file in files:\n",
        "        if any(file.lower().endswith(ext) for ext in video_extensions):\n",
        "            video_files.append(os.path.join(root, file))\n",
        "\n",
        "if not video_files:\n",
        "    raise Exception(\"No video files detected\")\n",
        "\n",
        "print(f\"ðŸŽ¬ Found {len(video_files)} video file(s)\\n\")\n",
        "\n",
        "video_info = []\n",
        "for vf in video_files:\n",
        "    season, episode = extract_episode_info(vf)\n",
        "    video_info.append({\n",
        "        'path': vf,\n",
        "        'name': os.path.basename(vf),\n",
        "        'season': season,\n",
        "        'episode': episode\n",
        "    })\n",
        "\n",
        "video_info.sort(key=lambda x: (x['season'], x['episode'], x['name']))\n",
        "\n",
        "print(\"ðŸ“‹ **Detected Episode Order:**\")\n",
        "print(\"=\" * 70)\n",
        "for idx, info in enumerate(video_info, 1):\n",
        "    s_info = f\"S{info['season']:02d}\" if info['season'] > 0 else \"S??\"\n",
        "    e_info = f\"E{info['episode']:02d}\" if info['episode'] > 0 else \"E??\"\n",
        "    size_mb = os.path.getsize(info['path']) / (1024*1024)\n",
        "    print(f\"{idx:2d}. [{s_info}{e_info}] {info['name'][:45]:<45} ({size_mb:.1f} MB)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# @title ðŸ§  **SMART AUDIO MERGE LOGIC** { display-mode: \"form\" }\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "def get_audio_layout(video_path):\n",
        "    \"\"\"\n",
        "    Returns a list of audio stream signatures for a video.\n",
        "    Each signature = (codec, channels, language)\n",
        "    \"\"\"\n",
        "    cmd = [\n",
        "        \"ffprobe\", \"-v\", \"error\",\n",
        "        \"-select_streams\", \"a\",\n",
        "        \"-show_entries\", \"stream=codec_name,channels:stream_tags=language\",\n",
        "        \"-of\", \"json\",\n",
        "        video_path\n",
        "    ]\n",
        "    try:\n",
        "        out = subprocess.check_output(cmd, stderr=subprocess.DEVNULL).decode(\"utf-8\")\n",
        "        data = json.loads(out)\n",
        "\n",
        "        layout = []\n",
        "        for s in data.get(\"streams\", []):\n",
        "            codec = s.get(\"codec_name\", \"unknown\")\n",
        "            channels = s.get(\"channels\", 0)\n",
        "            lang = s.get(\"tags\", {}).get(\"language\", \"und\")\n",
        "            layout.append((codec, channels, lang))\n",
        "\n",
        "        return layout\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "def audio_copy_is_safe(video_paths):\n",
        "    \"\"\"\n",
        "    Returns True only if ALL videos have identical\n",
        "    audio stream layouts (count + order + codec + channels + language)\n",
        "    \"\"\"\n",
        "    reference = None\n",
        "\n",
        "    for v in video_paths:\n",
        "        layout = get_audio_layout(v)\n",
        "        if reference is None:\n",
        "            reference = layout\n",
        "        elif layout != reference:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def build_smart_merge_cmd(list_file, output_name, video_paths):\n",
        "    \"\"\"\n",
        "    Builds the safest ffmpeg command automatically.\n",
        "    - Copy video always\n",
        "    - Copy audio ONLY if safe\n",
        "    - Otherwise re-encode audio and keep all tracks\n",
        "    \"\"\"\n",
        "\n",
        "    safe_audio = audio_copy_is_safe(video_paths)\n",
        "\n",
        "    print(\"\\nðŸ” Smart audio analysis:\")\n",
        "\n",
        "    # Show audio layout details\n",
        "    if video_paths:\n",
        "        sample_layout = get_audio_layout(video_paths[0])\n",
        "        if sample_layout:\n",
        "            print(f\"   â†’ Audio tracks detected: {len(sample_layout)}\")\n",
        "            for idx, (codec, ch, lang) in enumerate(sample_layout, 1):\n",
        "                print(f\"      Track {idx}: {codec.upper()} | {ch}ch | lang={lang}\")\n",
        "        else:\n",
        "            print(\"   â†’ No audio tracks detected\")\n",
        "\n",
        "    print(f\"   â†’ Audio copy safe? {'YES âœ…' if safe_audio else 'NO âš ï¸ (rebuild required)'}\")\n",
        "\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\n",
        "        \"-f\", \"concat\",\n",
        "        \"-safe\", \"0\",\n",
        "        \"-i\", list_file,\n",
        "        \"-map\", \"0:v\",\n",
        "        \"-map\", \"0:a\",\n",
        "        \"-c:v\", \"copy\",\n",
        "    ]\n",
        "\n",
        "    if safe_audio:\n",
        "        print(\"âš¡ Using FAST MODE: copy video + copy audio\")\n",
        "        cmd.extend([\"-c:a\", \"copy\"])\n",
        "    else:\n",
        "        print(\"ðŸ›¡ï¸ Using SAFE MODE: copy video + re-encode audio\")\n",
        "        cmd.extend([\"-c:a\", \"aac\", \"-b:a\", \"192k\"])\n",
        "\n",
        "    # Subtitle handling\n",
        "    if subtitle_mode == \"Preserve All Subtitles\":\n",
        "        if output_format == \"MKV\":\n",
        "            cmd.extend([\"-c:s\", \"copy\"])\n",
        "        else:\n",
        "            cmd.extend([\"-c:s\", \"mov_text\"])\n",
        "    else:\n",
        "        cmd.extend([\"-sn\"])\n",
        "\n",
        "    cmd.extend([\"-map_metadata\", \"0\", \"-y\", output_name])\n",
        "\n",
        "    return cmd\n",
        "\n",
        "# @title ðŸŽžï¸ **Merge Videos with Smart Audio Logic** { display-mode: \"form\" }\n",
        "\n",
        "print(\"\\nðŸŽ¬ Preparing to merge videos...\")\n",
        "\n",
        "if output_format == \"MKV\":\n",
        "    output_ext = \".mkv\"\n",
        "else:\n",
        "    output_ext = \".mp4\"\n",
        "\n",
        "list_file = \"filelist.txt\"\n",
        "with open(list_file, 'w', encoding='utf-8') as f:\n",
        "    for info in video_info:\n",
        "        safe_path = info['path'].replace(\"'\", \"'\\\\''\")\n",
        "        f.write(f\"file '{safe_path}'\\n\")\n",
        "\n",
        "print(f\"âœ… Created merge list with {len(video_info)} video(s)\")\n",
        "\n",
        "if custom_output_name:\n",
        "    output_name = custom_output_name\n",
        "    if not output_name.lower().endswith(output_ext):\n",
        "        output_name = os.path.splitext(output_name)[0] + output_ext\n",
        "else:\n",
        "    seasons = [v['season'] for v in video_info if v['season'] > 0]\n",
        "    episodes = [v['episode'] for v in video_info if v['episode'] > 0]\n",
        "\n",
        "    base_name = ZIP_BASE_NAME\n",
        "\n",
        "    if seasons and episodes:\n",
        "        min_season = min(seasons)\n",
        "        max_season = max(seasons)\n",
        "        min_episode = min(episodes)\n",
        "        max_episode = max(episodes)\n",
        "\n",
        "        if min_season == max_season:\n",
        "            output_name = f\"{base_name} Season {min_season:02d} Episodes {min_episode:02d}-{max_episode:02d}{output_ext}\"\n",
        "        else:\n",
        "            output_name = f\"{base_name} S{min_season:02d}-S{max_season:02d} Ep{min_episode:02d}-{max_episode:02d}{output_ext}\"\n",
        "    else:\n",
        "        output_name = f\"{base_name} Merged Complete{output_ext}\"\n",
        "\n",
        "output_name = re.sub(r'[<>:\"|?*\\\\]', '_', output_name)\n",
        "output_name = re.sub(r'\\s+', ' ', output_name).strip()\n",
        "\n",
        "print(f\"\\nðŸ“ Output filename: {output_name}\")\n",
        "\n",
        "# Build smart merge command\n",
        "video_paths = [v[\"path\"] for v in video_info]\n",
        "\n",
        "base_cmd = build_smart_merge_cmd(\n",
        "    list_file=list_file,\n",
        "    output_name=output_name,\n",
        "    video_paths=video_paths\n",
        ")\n",
        "\n",
        "print(\"\\nðŸ§¾ FFmpeg command:\")\n",
        "print(\" \".join(base_cmd))\n",
        "\n",
        "# Execute merge\n",
        "print(\"\\nâ³ Merging videos... This may take a while.\\n\")\n",
        "\n",
        "try:\n",
        "    process = subprocess.Popen(base_cmd, stderr=subprocess.PIPE, universal_newlines=True)\n",
        "\n",
        "    time_pattern = re.compile(r'time=(\\d{2}):(\\d{2}):(\\d{2})')\n",
        "\n",
        "    for line in process.stderr:\n",
        "        time_match = time_pattern.search(line)\n",
        "        if time_match:\n",
        "            h, m, s = map(int, time_match.groups())\n",
        "            current_time = h * 3600 + m * 60 + s\n",
        "            print(f\"\\rðŸŽ¬ Processingâ€¦ {current_time//60}:{current_time%60:02d}\", end='')\n",
        "\n",
        "    process.wait()\n",
        "\n",
        "    if process.returncode == 0 and os.path.exists(output_name):\n",
        "        file_size = os.path.getsize(output_name) / (1024*1024)\n",
        "        print(f\"\\n\\nâœ… **Merge Complete!**\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"ðŸ“ Output: {output_name}\")\n",
        "        print(f\"ðŸ’¾ Size: {file_size:.2f} MB\")\n",
        "        print(f\"ðŸŽ¬ Episodes: {len(video_info)}\")\n",
        "\n",
        "        # Show final audio info\n",
        "        final_layout = get_audio_layout(output_name)\n",
        "        if final_layout:\n",
        "            print(f\"ðŸŽµ Audio tracks in merged file: {len(final_layout)}\")\n",
        "            for idx, (codec, ch, lang) in enumerate(final_layout, 1):\n",
        "                print(f\"   Track {idx}: {codec.upper()} | {ch}ch | lang={lang}\")\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        MERGED_VIDEO = output_name\n",
        "    else:\n",
        "        raise Exception(\"Merge failed\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Error during merge: {str(e)}\")\n",
        "    raise\n",
        "finally:\n",
        "    if os.path.exists(list_file):\n",
        "        os.remove(list_file)\n",
        "\n",
        "# @title ðŸ“¤ **Upload Functions** { display-mode: \"form\" }\n",
        "\n",
        "def upload_to_gofile(filepath):\n",
        "    try:\n",
        "        print(\"\\nðŸŒ GoFile.io Upload\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        import time\n",
        "        server = None\n",
        "        for attempt in range(1, 6):\n",
        "            try:\n",
        "                r = requests.get(\"https://api.gofile.io/servers\", timeout=15)\n",
        "                data = r.json()\n",
        "\n",
        "                if data['status'] == 'ok' and data['data']['servers']:\n",
        "                    server = data['data']['servers'][0]['name']\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "            time.sleep(3)\n",
        "\n",
        "        if not server:\n",
        "            print(\"âŒ No servers available\")\n",
        "            return None\n",
        "\n",
        "        upload_url = f'https://{server}.gofile.io/contents/uploadfile'\n",
        "        file_size_mb = os.path.getsize(filepath) / (1024*1024)\n",
        "        print(f\"ðŸ“¦ File: {os.path.basename(filepath)} ({file_size_mb:.2f} MB)\")\n",
        "        print(\"â³ Uploading...\")\n",
        "\n",
        "        with open(filepath, 'rb') as f:\n",
        "            response = requests.post(upload_url, files={'file': f}, timeout=7200)\n",
        "\n",
        "        result = response.json()\n",
        "        if result['status'] == 'ok':\n",
        "            link = result['data']['downloadPage']\n",
        "            print(f\"âœ… GoFile link: {link}\")\n",
        "            return link\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ GoFile error: {e}\")\n",
        "        return None\n",
        "\n",
        "def upload_to_gdrive(filepath):\n",
        "    try:\n",
        "        print(\"\\nâ˜ï¸ Google Drive Upload\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        from google.colab import drive\n",
        "\n",
        "        if not os.path.exists('/content/drive/MyDrive'):\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "        destination = '/content/drive/MyDrive/Merged_Videos/'\n",
        "        os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "        dest_path = os.path.join(destination, os.path.basename(filepath))\n",
        "        print(f\"â³ Copying to Google Drive...\")\n",
        "\n",
        "        import shutil\n",
        "        shutil.copy2(filepath, dest_path)\n",
        "\n",
        "        print(f\"âœ… Uploaded: MyDrive/Merged_Videos/{os.path.basename(filepath)}\")\n",
        "        return dest_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Google Drive error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Execute uploads\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸ“¤ UPLOAD PROCESS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "gofile_link = None\n",
        "gdrive_path = None\n",
        "\n",
        "if upload_destination == \"GoFile.io Only\":\n",
        "    gofile_link = upload_to_gofile(MERGED_VIDEO)\n",
        "elif upload_destination == \"Google Drive Only\":\n",
        "    gdrive_path = upload_to_gdrive(MERGED_VIDEO)\n",
        "elif upload_destination == \"GoFile + Google Drive\":\n",
        "    gofile_link = upload_to_gofile(MERGED_VIDEO)\n",
        "    gdrive_path = upload_to_gdrive(MERGED_VIDEO)\n",
        "else:\n",
        "    print(\"\\nðŸ“ Upload skipped - file saved locally\")\n",
        "\n",
        "# Final Summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ðŸŽ‰ **ALL DONE!**\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nðŸ“Š Summary:\")\n",
        "print(f\"  â€¢ Videos merged: {len(video_info)}\")\n",
        "print(f\"  â€¢ Output file: {output_name}\")\n",
        "print(f\"  â€¢ File size: {os.path.getsize(MERGED_VIDEO) / (1024*1024):.2f} MB\")\n",
        "\n",
        "if gofile_link:\n",
        "    print(f\"\\nðŸ”— GoFile.io: {gofile_link}\")\n",
        "\n",
        "if gdrive_path:\n",
        "    print(f\"\\nðŸ“ Google Drive: {gdrive_path}\")\n",
        "\n",
        "if cleanup_after:\n",
        "    print(\"\\nðŸ§¹ Cleaning up...\")\n",
        "    if os.path.exists(zip_filename):\n",
        "        os.remove(zip_filename)\n",
        "    if os.path.exists(extract_folder):\n",
        "        import shutil\n",
        "        shutil.rmtree(extract_folder)\n",
        "\n",
        "print(\"\\nâœ¨ Process complete!\")\n",
        "print(\"\\nðŸ’¡ SMART MERGE guarantees:\")\n",
        "print(\"   âœ… All audio tracks preserved (Hindi + English)\")\n",
        "print(\"   âœ… No silent track drops\")\n",
        "print(\"   âœ… Auto-detects safe vs rebuild scenarios\")\n",
        "print(\"   âœ… Video quality: Lossless (always copied)\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT3Wipll1YWOE55ZfJiXHK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}